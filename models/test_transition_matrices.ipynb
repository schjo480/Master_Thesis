{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    \n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        def cosine_beta_schedule(timesteps, s=0.008):\n",
    "            \"\"\"\n",
    "            Cosine schedule as described in https://arxiv.org/abs/2102.09672.\n",
    "\n",
    "            Parameters:\n",
    "            - timesteps: int, the number of timesteps for the schedule.\n",
    "            - s: float, small constant to prevent numerical issues.\n",
    "\n",
    "            Returns:\n",
    "            - betas: torch.Tensor, beta values for each timestep.\n",
    "            - alphas: torch.Tensor, alpha values for each timestep.\n",
    "            - alpha_bars: torch.Tensor, cumulative product of alphas for each timestep.\n",
    "            \"\"\"\n",
    "            steps = timesteps + 1\n",
    "            x = torch.linspace(0, timesteps, steps)\n",
    "            alphas_cumprod = torch.cos((x / timesteps + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "            betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "            alphas = 1 - betas\n",
    "            alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "            return betas, alphas, alpha_bars\n",
    "        betas, alphas, alpha_bars = cosine_beta_schedule(spec['num_timesteps'])\n",
    "        return betas.to(device)\n",
    "    \n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])\n",
    "    \n",
    "def _get_custom_transition_mat(t, betas):\n",
    "    \"\"\"\n",
    "    Generates a 2x2 transition matrix for a discrete diffusion process at timestep t.\n",
    "\n",
    "    Parameters:\n",
    "    - t: int, current timestep.\n",
    "    - device: torch.device, the device to place the tensor on.\n",
    "\n",
    "    Returns:\n",
    "    - transition_matrix: torch.Tensor, a 2x2 transition matrix.\n",
    "    \"\"\"\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "    alpha_bar_t = alpha_bars[t]\n",
    "    \n",
    "    # Compute the transition probabilities\n",
    "    transition_matrix = torch.tensor([[0.5*alphas[t] + 0.5, 0.5 - 0.5*alphas[t]],\n",
    "                                    [0.5 - 0.5*alphas[t], 0.5*alphas[t] + 0.5]])\n",
    "    \n",
    "    return transition_matrix\n",
    "\n",
    "betas = get_diffusion_betas({'type': 'cosine', 'num_timesteps': 100}, 'cpu')\n",
    "#print(betas)\n",
    "alphas = 1 - betas\n",
    "#print(alphas)\n",
    "q_onestep_mats = [_get_custom_transition_mat(t, betas)\n",
    "                               for t in range(0, 100)]\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, 100):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "print(q_mats[0])\n",
    "print(q_mats[50])\n",
    "print(q_mats[99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_mats[0])\n",
    "print(q_mats[50])\n",
    "print(q_mats[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "T=50\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/1006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        steps = torch.linspace(0, 1, spec['num_timesteps'] + 1, dtype=torch.float64)\n",
    "        alpha_bar = torch.square(torch.cos(((steps + 0.008) / 1.008) * torch.pi / 2))\n",
    "        betas = torch.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], torch.tensor(0.999))\n",
    "        return betas.to(device)\n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])\n",
    "    \n",
    "    \n",
    "def _get_gaussian_transition_mat(t):\n",
    "    r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "    This method constructs a transition matrix Q with\n",
    "    decaying entries as a function of how far off diagonal the entry is.\n",
    "    Normalization option 1:\n",
    "    Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                0                          else.\n",
    "\n",
    "    Normalization option 2:\n",
    "    tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                        0                        else.\n",
    "\n",
    "    Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "    Args:\n",
    "        t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "    Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    num_classes = 2\n",
    "    device = 'cpu'\n",
    "    transition_bands = 1\n",
    "    spec = {'type': 'linear', 'start': 0.4, 'stop': 1.0, 'num_timesteps': T}\n",
    "    betas = get_diffusion_betas(spec, device=device)\n",
    "    transition_bands = transition_bands if transition_bands else num_classes - 1\n",
    "\n",
    "    beta_t = betas[t]\n",
    "\n",
    "    mat = torch.zeros((num_classes, num_classes),\n",
    "                    dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "    # Make the values correspond to a similar type of gaussian as in the\n",
    "    # gaussian diffusion case for continuous state spaces.\n",
    "    values = torch.linspace(torch.tensor(0.), torch.tensor(num_classes-1), num_classes, dtype=torch.float64).to(device, non_blocking=True)\n",
    "    #print(values)   # [0, 1]\n",
    "    values = values * 2./ (num_classes - 1.)\n",
    "    #print(values)   # [0, 2]\n",
    "    values = values[:transition_bands+1]\n",
    "    #print(values)   # [0, 2]\n",
    "    values = -values * values / beta_t\n",
    "    #print(values)   # [0, -6300]\n",
    "    \n",
    "    # To reverse the tensor 'values' starting from the second element\n",
    "    reversed_values = values[1:].flip(dims=[0])\n",
    "    #print(reversed_values)  # [-6300]\n",
    "    # Concatenating the reversed values with the original values\n",
    "    values = torch.cat([reversed_values, values], dim=0)\n",
    "    #print(values)   # [-6300, 0, -6300]\n",
    "    values = F.softmax(values, dim=0)\n",
    "    #print(values)   # [0, 1, 0]\n",
    "    values = values[transition_bands:]\n",
    "    #print(values)   # [1, 0]\n",
    "    \n",
    "    for k in range(1, transition_bands + 1):\n",
    "        off_diag = torch.full((num_classes - k,), values[k], dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "        mat += torch.diag(off_diag, k)\n",
    "        mat += torch.diag(off_diag, -k)\n",
    "\n",
    "    # Add diagonal values such that rows and columns sum to one.\n",
    "    # Technically only the ROWS need to sum to one\n",
    "    # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "    # which is necessary if we want to have a uniform stationary distribution.\n",
    "    diag = 1. - mat.sum(dim=1)\n",
    "    mat += torch.diag_embed(diag)\n",
    "\n",
    "    return mat.to(device, non_blocking=True)\n",
    "\n",
    "def _get_prior_distribution_transition_mat(t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "        Use cosine schedule for these transition matrices.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        device = 'cpu'\n",
    "        spec = {'type': 'cosine', 'start': 0.4, 'stop': 0.8, 'num_timesteps': T}\n",
    "        betas = get_diffusion_betas(spec, device=device)\n",
    "        beta_t = betas[t]\n",
    "        num_classes = 2\n",
    "        class_probs = torch.tensor([1 - 5/11000, 5/11000], device=device)\n",
    "        mat = torch.zeros((num_classes, num_classes), dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "        '''for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if i != j:\n",
    "                    mat[i, j] = beta_t * class_probs[j]\n",
    "                else:\n",
    "                    mat[i, j] = 1 - beta_t + beta_t * class_probs[j]'''\n",
    "        mat = beta_t * class_probs + (1 - beta_t) * torch.eye(2, device=device)\n",
    "        \n",
    "        return mat\n",
    "\n",
    "\n",
    "q_one_step_mats = [_get_gaussian_transition_mat(t)\n",
    "                            for t in range(0, T)]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (T,\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, T):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "for t in range(0, T, 9):\n",
    "    print(q_mats[t])\n",
    "    # print(_get_gaussian_transition_mat(t))\n",
    "    # print(_get_prior_distribution_transition_mat(t))\n",
    "\n",
    "'''import matplotlib.pyplot as plt\n",
    "\n",
    "spec = {'type': 'linear', 'start': 1.0, 'stop': 100.0, 'num_timesteps': 100}\n",
    "betas = get_diffusion_betas(spec, device='cpu')\n",
    "plt.plot(betas)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_marginals = torch.tensor([0.9, 0.1], dtype=torch.float64)\n",
    "print(e_marginals.unsqueeze(0).expand(2, -1).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "def get_diffusion_betas(spec):\n",
    "  \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "  if spec['type'] == 'linear':\n",
    "    # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "    # To be used with Gaussian diffusion models in continuous and discrete\n",
    "    # state spaces.\n",
    "    # To be used with transition_mat_type = 'gaussian'\n",
    "    return onp.linspace(spec['start'], spec['stop'], spec['num_timesteps'])\n",
    "  elif spec['type'] == 'cosine':\n",
    "    # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "    # To be used with transition_mat_type = 'uniform'.\n",
    "    steps = (\n",
    "        onp.arange(spec['num_timesteps'] + 1, dtype=onp.float64) /\n",
    "        spec['num_timesteps'])\n",
    "    alpha_bar = onp.cos((steps + 0.008) / 1.008 * onp.pi / 2)\n",
    "    betas = onp.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], 0.999)\n",
    "    return betas\n",
    "  elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "    # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "    # To be used with absorbing state models.\n",
    "    # ensures that the probability of decaying to the absorbing state\n",
    "    # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "    # To be used with transition_mat_type = 'absorbing'\n",
    "    return 1. / onp.linspace(spec['num_timesteps'], 1., spec['num_timesteps'])\n",
    "  else:\n",
    "    raise NotImplementedError(spec['type'])\n",
    "  \n",
    "spec = {'type': 'linear', 'start': 0.4, 'stop': 0.8, 'num_timesteps': 100}\n",
    "betas = get_diffusion_betas(spec)\n",
    "alphas = 1 - betas\n",
    "alpha_bar = onp.cumprod(alphas)\n",
    "plt.plot(betas)\n",
    "plt.plot(alpha_bar)\n",
    "plt.legend(['betas', 'alpha_bar'])\n",
    "plt.show()\n",
    "\n",
    "import scipy\n",
    "def _get_gaussian_transition_mat(t):\n",
    "    r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "    This method constructs a transition matrix Q with\n",
    "    decaying entries as a function of how far off diagonal the entry is.\n",
    "    Normalization option 1:\n",
    "    Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "             1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "             0                          else.\n",
    "\n",
    "    Normalization option 2:\n",
    "    tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                     0                        else.\n",
    "\n",
    "    Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "    Args:\n",
    "      t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "    Returns:\n",
    "      Q_t: transition matrix. shape = (num_pixel_vals, num_pixel_vals).\n",
    "    \"\"\"\n",
    "    transition_bands = 2 - 1\n",
    "    betas = get_diffusion_betas(spec)\n",
    "    beta_t = betas[t]\n",
    "\n",
    "    mat = onp.zeros((2, 2),\n",
    "                    dtype=onp.float64)\n",
    "\n",
    "    # Make the values correspond to a similar type of gaussian as in the\n",
    "    # gaussian diffusion case for continuous state spaces.\n",
    "    values = onp.linspace(start=0., stop=1., num=2,\n",
    "                          endpoint=True, dtype=onp.float64)\n",
    "    print(values)\n",
    "    values = values * 2./ (2 - 1.)\n",
    "    print(values)\n",
    "    values = values[:transition_bands+1]\n",
    "    print(values)\n",
    "    values = -values * values / beta_t\n",
    "    print(values)\n",
    "\n",
    "    values = onp.concatenate([values[:0:-1], values], axis=0)\n",
    "    print(values)\n",
    "    values = scipy.special.softmax(values, axis=0)\n",
    "    print(values)\n",
    "    values = values[transition_bands:]\n",
    "    print(values)\n",
    "    for k in range(1, transition_bands + 1):\n",
    "      off_diag = onp.full(shape=(2 - k,),\n",
    "                          fill_value=values[k],\n",
    "                          dtype=onp.float64)\n",
    "\n",
    "      mat += onp.diag(off_diag, k=k)\n",
    "      mat += onp.diag(off_diag, k=-k)\n",
    "\n",
    "    # Add diagonal values such that rows and columns sum to one.\n",
    "    # Technically only the ROWS need to sum to one\n",
    "    # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "    # which is necessary if we want to have a uniform stationary distribution.\n",
    "    diag = 1. - mat.sum(1)\n",
    "    mat += onp.diag(diag, k=0)\n",
    "\n",
    "    return mat\n",
    "  \n",
    "_get_gaussian_transition_mat(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_beta_schedule(T, s=0.008):\n",
    "    \"\"\"\n",
    "    Generate a cosine schedule for diffusion betas.\n",
    "    \n",
    "    Args:\n",
    "    - T (int): The total number of diffusion timesteps.\n",
    "    - s (float): A small constant to ensure the betas do not reach 0, which could cause numerical stability issues.\n",
    "    \n",
    "    Returns:\n",
    "    - np.array: An array of length T with the beta values for each timestep.\n",
    "    \"\"\"\n",
    "    steps = np.arange(T, dtype=np.float64)\n",
    "    x = s + (1 - s) * np.cos(0.5 * np.pi * steps / T)\n",
    "    betas = 1 - x / x.max()\n",
    "    return betas\n",
    "\n",
    "# Example usage\n",
    "T = 1000  # Total number of diffusion steps\n",
    "betas = cosine_beta_schedule(T)\n",
    "alphas = 1 - betas\n",
    "alpha_bar = np.cumprod(alphas)\n",
    "plt.plot(betas)\n",
    "#plt.plot(alphas)\n",
    "plt.plot(alpha_bar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Cosine schedule as described in https://arxiv.org/abs/2102.09672.\n",
    "\n",
    "    Parameters:\n",
    "    - timesteps: int, the number of timesteps for the schedule.\n",
    "    - s: float, small constant to prevent numerical issues.\n",
    "\n",
    "    Returns:\n",
    "    - betas: torch.Tensor, beta values for each timestep.\n",
    "    - alphas: torch.Tensor, alpha values for each timestep.\n",
    "    - alpha_bars: torch.Tensor, cumulative product of alphas for each timestep.\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos((x / timesteps + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "    betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    return betas, alphas, alpha_bars\n",
    "\n",
    "# Example usage\n",
    "timesteps = 100  # Number of timesteps\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "betas, alphas, alpha_bars = cosine_beta_schedule(timesteps)\n",
    "betas = betas.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(betas)\n",
    "plt.plot(alpha_bars)\n",
    "plt.legend(['betas', 'alpha_bar'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "T = 100\n",
    "\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/1006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        steps = torch.linspace(0, 1, spec['num_timesteps'] + 1, dtype=torch.float64)\n",
    "        betas, alphas, alpha_bars = cosine_beta_schedule(spec['num_timesteps'])\n",
    "        return betas.to(device)\n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])\n",
    "    \n",
    "    \n",
    "def _get_gaussian_transition_mat(t):\n",
    "    r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "    This method constructs a transition matrix Q with\n",
    "    decaying entries as a function of how far off diagonal the entry is.\n",
    "    Normalization option 1:\n",
    "    Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                0                          else.\n",
    "\n",
    "    Normalization option 2:\n",
    "    tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                        0                        else.\n",
    "\n",
    "    Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "    Args:\n",
    "        t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "    Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    num_classes = 2\n",
    "    device = 'cpu'\n",
    "    transition_bands = 1\n",
    "    spec = {'type': 'cosine', 'start': 0.4, 'stop': 1.0, 'num_timesteps': T}\n",
    "    betas = get_diffusion_betas(spec, device=device)\n",
    "    # transition_bands = transition_bands if transition_bands else num_classes - 1\n",
    "\n",
    "    beta_t = betas[t]\n",
    "\n",
    "    mat = torch.zeros((num_classes, num_classes),\n",
    "                    dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "    # Make the values correspond to a similar type of gaussian as in the\n",
    "    # gaussian diffusion case for continuous state spaces.\n",
    "    values = torch.linspace(torch.tensor(0.), torch.tensor(num_classes-1), num_classes, dtype=torch.float64).to(device, non_blocking=True)\n",
    "    #print(values)   # [0, 1]\n",
    "    values = values * 2./ (num_classes - 1.)\n",
    "    #print(values)   # [0, 2]\n",
    "    values = values[:transition_bands+1]\n",
    "    #print(values)   # [0, 2]\n",
    "    values = -values * values / beta_t\n",
    "    #print(values)   # [0, -6300]\n",
    "    \n",
    "    # To reverse the tensor 'values' starting from the second element\n",
    "    reversed_values = values[1:].flip(dims=[0])\n",
    "    #print(reversed_values)  # [-6300]\n",
    "    # Concatenating the reversed values with the original values\n",
    "    values = torch.cat([reversed_values, values], dim=0)\n",
    "    #print(values)   # [-6300, 0, -6300]\n",
    "    values = F.softmax(values, dim=0)\n",
    "    #print(values)   # [0, 1, 0]\n",
    "    values = values[transition_bands:]\n",
    "    #print(values)   # [1, 0]\n",
    "    \n",
    "    for k in range(1, transition_bands + 1):\n",
    "        off_diag = torch.full((num_classes - k,), values[k], dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "        mat += torch.diag(off_diag, k)\n",
    "        mat += torch.diag(off_diag, -k)\n",
    "\n",
    "    # Add diagonal values such that rows and columns sum to one.\n",
    "    # Technically only the ROWS need to sum to one\n",
    "    # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "    # which is necessary if we want to have a uniform stationary distribution.\n",
    "    diag = 1. - mat.sum(dim=1)\n",
    "    mat += torch.diag_embed(diag)\n",
    "\n",
    "    return mat.to(device, non_blocking=True)\n",
    "\n",
    "def _get_prior_distribution_transition_mat(t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "        Use cosine schedule for these transition matrices.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        device = 'cpu'\n",
    "        spec = {'type': 'cosine', 'start': 0.4, 'stop': 0.8, 'num_timesteps': T}\n",
    "        betas = get_diffusion_betas(spec, device=device)\n",
    "        beta_t = betas[t]\n",
    "        num_classes = 2\n",
    "        class_probs = torch.tensor([1 - 5/11000, 5/11000], device=device)\n",
    "        mat = torch.zeros((num_classes, num_classes), dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "        '''for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                if i != j:\n",
    "                    mat[i, j] = beta_t * class_probs[j]\n",
    "                else:\n",
    "                    mat[i, j] = 1 - beta_t + beta_t * class_probs[j]'''\n",
    "        mat = beta_t * class_probs + (1 - beta_t) * torch.eye(2, device=device)\n",
    "        \n",
    "        return mat\n",
    "\n",
    "\n",
    "q_one_step_mats = [_get_gaussian_transition_mat(t)\n",
    "                            for t in range(0, T)]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (T,\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, T):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "for t in range(0, T, 9):\n",
    "    print(q_mats[t])\n",
    "    # print(_get_gaussian_transition_mat(t))\n",
    "    # print(_get_prior_distribution_transition_mat(t))\n",
    "\n",
    "'''import matplotlib.pyplot as plt\n",
    "\n",
    "spec = {'type': 'linear', 'start': 1.0, 'stop': 100.0, 'num_timesteps': 100}\n",
    "betas = get_diffusion_betas(spec, device='cpu')\n",
    "plt.plot(betas)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "def get_transition_matrix(t, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates a 2x2 transition matrix for a discrete diffusion process at timestep t.\n",
    "\n",
    "    Parameters:\n",
    "    - t: int, current timestep.\n",
    "    - device: torch.device, the device to place the tensor on.\n",
    "\n",
    "    Returns:\n",
    "    - transition_matrix: torch.Tensor, a 2x2 transition matrix.\n",
    "    \"\"\"\n",
    "    betas, _, alpha_bars = cosine_beta_schedule(T)\n",
    "    spec = {'type': 'linear', 'start': 0.00001, 'stop': 0.09, 'num_timesteps': T}\n",
    "    betas = get_diffusion_betas(spec, device=device)\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "    alpha_bar_t = alpha_bars[t].to(device)\n",
    "    \n",
    "    # Compute the transition probabilities\n",
    "    transition_matrix = torch.tensor([[alphas[t], 1 - alphas[t]],\n",
    "                                      [1 - alphas[t], alphas[t]]], device=device)\n",
    "    \n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one_step_mats = [get_transition_matrix(t)\n",
    "                            for t in range(0, T)]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (T,\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, T):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "zero_to_one = []\n",
    "one_to_zero = []\n",
    "for t in range(0, T, 1):\n",
    "    zero_to_one.append(q_mats[t][0][1].item())\n",
    "    one_to_zero.append(q_mats[t][1][0].item())\n",
    "    print(q_mats[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(zero_to_one)\n",
    "plt.plot(one_to_zero, linestyle='dashed')\n",
    "plt.legend(['0 -> 1', '1 -> 0'])\n",
    "plt.title('Transition probabilities with linear noise schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "def _get_prior_distribution_transition_mat(t):\n",
    "    \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "    Use cosine schedule for these transition matrices.\n",
    "\n",
    "    Args:\n",
    "    t: timestep. integer scalar.\n",
    "\n",
    "    Returns:\n",
    "    Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    device = 'cpu'\n",
    "    spec = {'type': 'cosine', 'start': 0.01, 'stop': 0.46, 'num_timesteps': T}\n",
    "    betas = get_diffusion_betas(spec, device=device)\n",
    "    beta_t = betas[t]\n",
    "    num_classes = 2\n",
    "    class_probs = torch.tensor([1 - 5/16000, 5/16000], device=device)\n",
    "    mat = torch.zeros((num_classes, num_classes), dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "    '''for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                mat[i, j] = beta_t * class_probs[j]\n",
    "            else:\n",
    "                mat[i, j] = 1 - beta_t + beta_t * class_probs[j]'''\n",
    "    mat = beta_t * class_probs + (1 - beta_t) * torch.eye(2, device=device)\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one_step_mats = [_get_prior_distribution_transition_mat(t)\n",
    "                            for t in range(0, T)]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (T,\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, T):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "zero_to_one = []\n",
    "one_to_zero = []\n",
    "for t in range(0, T, 1):\n",
    "    zero_to_one.append(q_mats[t][0][1].item())\n",
    "    one_to_zero.append(q_mats[t][1][0].item())\n",
    "    print(q_mats[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0].plot(zero_to_one, label='0 -> 1')\n",
    "axs[0].legend()  # Add a legend\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1].plot(one_to_zero, label='1 -> 0')\n",
    "axs[1].legend()  # Add a legend\n",
    "\n",
    "# Show the plot\n",
    "fig.suptitle('Transition probabilities with cosine noise schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5/16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_transition_mat(t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition\n",
    "        matrix Q with\n",
    "        Q_{ij} = beta_t / num_classes       if |i-j| <= transition_bands\n",
    "                1 - \\sum_{l \\neq i} Q_{il} if i==j.\n",
    "                0                          else.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        num_classes = 2\n",
    "        device = 'cpu'\n",
    "        spec = {'type': 'linear', 'start': 0.01, 'stop': 0.9, 'num_timesteps': T}\n",
    "        betas = get_diffusion_betas(spec, device=device)\n",
    "        beta_t = betas[t]\n",
    "        \n",
    "        mat = torch.zeros((num_classes, num_classes),\n",
    "                        dtype=torch.float64)\n",
    "        off_diag = torch.full((num_classes - 1,), fill_value=beta_t / float(num_classes), dtype=torch.float64)\n",
    "\n",
    "        for k in range(1, 1 + 1):\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "            off_diag = off_diag[:-1]\n",
    "\n",
    "        # Add diagonal values such that rows sum to one\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag(diag)\n",
    "        \n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one_step_mats = [_get_transition_mat(t)\n",
    "                            for t in range(0, T)]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (T,\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, T):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "zero_to_one = []\n",
    "one_to_zero = []\n",
    "for t in range(0, T, 1):\n",
    "    zero_to_one.append(q_mats[t][0][1].item())\n",
    "    one_to_zero.append(q_mats[t][1][0].item())\n",
    "    print(q_mats[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0].plot(zero_to_one, label='0 -> 1')\n",
    "axs[0].legend()  # Add a legend\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1].plot(one_to_zero, label='1 -> 0')\n",
    "axs[1].legend()  # Add a legend\n",
    "\n",
    "# Show the plot\n",
    "fig.suptitle('Transition probabilities with cosine noise schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "q_mats = torch.stack([torch.tensor([[0.4, 0.6], [0.6, 0.4]]), \n",
    "                      torch.tensor([[0.7, 0.3], [0.1, 0.8]]), \n",
    "                      torch.tensor([[0.2, 0.8],[0.8, 0.2]])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_mats[1, torch.tensor([0, 0, 0, 1, 0, 1, 0, 0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "def calculate_fut_ratio(sample_list, ground_truth_fut):\n",
    "    \"\"\"\n",
    "    Calculates the ratio of samples in `sample_list` that have at least n edges in common with the ground truth future trajectory for each n up to future_len.\n",
    "\n",
    "    Args:\n",
    "        sample_list (list): A list of samples.\n",
    "        ground_truth_fut (list): A list of ground truth future trajectories.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are the minimum number of common edges (from 1 to future_len) and values are the ratios of samples meeting that criterion.\n",
    "    \"\"\"\n",
    "    # Initialize counts for each number of common edges from 1 up to future_len\n",
    "    future_len = 2\n",
    "    counts = {i: 0 for i in range(1, future_len + 1)}            \n",
    "\n",
    "    total = 0\n",
    "    for (sample_sublist, ground_truth_sublist) in zip(sample_list, ground_truth_fut):\n",
    "        total += len(sample_sublist)\n",
    "        for i in range(len(sample_sublist)):\n",
    "            # Convert tensors to lists if they are indeed tensors\n",
    "            sample = sample_sublist[i].tolist()\n",
    "            ground_truth = ground_truth_sublist[i].flatten().tolist()\n",
    "\n",
    "            edges_count = sum(1 for edge in ground_truth if edge in sample)\n",
    "            for n in range(1, min(edges_count, future_len) + 1):\n",
    "                counts[n] += 1\n",
    "        \n",
    "    if total != 0:\n",
    "        ratios = {n: counts[n] / total for n in counts}\n",
    "    else:\n",
    "        ratios = {n: 0 for n in counts}\n",
    "    return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [[tensor([1, 2]), tensor([3, 4])], [tensor([0, 2]), tensor([4, 6])]]\n",
    "ground_truth_fut = [[tensor([[2, 1]]), tensor([[1, 4]])], [tensor([[4, 2]]), tensor([[4, 6]])]]\n",
    "print(calculate_fut_ratio(sample_list, ground_truth_fut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    \n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        def cosine_beta_schedule(timesteps, s=0.008):\n",
    "            \"\"\"\n",
    "            Cosine schedule as described in https://arxiv.org/abs/2102.09672.\n",
    "\n",
    "            Parameters:\n",
    "            - timesteps: int, the number of timesteps for the schedule.\n",
    "            - s: float, small constant to prevent numerical issues.\n",
    "\n",
    "            Returns:\n",
    "            - betas: torch.Tensor, beta values for each timestep.\n",
    "            - alphas: torch.Tensor, alpha values for each timestep.\n",
    "            - alpha_bars: torch.Tensor, cumulative product of alphas for each timestep.\n",
    "            \"\"\"\n",
    "            steps = timesteps + 1\n",
    "            x = torch.linspace(0, timesteps, steps)\n",
    "            alphas_cumprod = torch.cos((x / timesteps + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "            betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "            alphas = 1 - betas\n",
    "            alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "            return betas, alphas, alpha_bars\n",
    "        betas, alphas, alpha_bars = cosine_beta_schedule(spec['num_timesteps'])\n",
    "        return betas.to(device)\n",
    "    \n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prior_distribution_transition_mat(spec, t):\n",
    "    betas = get_diffusion_betas(spec, 'cpu')\n",
    "    device = 'cpu'\n",
    "    beta_t = betas[t]\n",
    "    num_classes = 2\n",
    "    class_probs = torch.tensor([1 - 5/11000, 5/11000])\n",
    "    mat = torch.zeros((num_classes, num_classes), dtype=torch.float64).to(device, non_blocking=True)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                mat[i, j] = beta_t * class_probs[j]\n",
    "            else:\n",
    "                mat[i, j] = 1 - beta_t + beta_t * class_probs[j]\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def _get_custom_transition_mat(spec, t):\n",
    "    device = 'cpu'\n",
    "    betas = get_diffusion_betas(spec, 'cpu')\n",
    "    alphas = 1 - betas\n",
    "    \n",
    "    # Compute the transition probabilities\n",
    "    transition_matrix = torch.tensor([[alphas[t] + 0.5*betas[t], (1 - alphas[t])/2],\n",
    "                                    [(1 - alphas[t])/2, alphas[t] + 0.5*betas[t]]], device=device)\n",
    "    \n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = {'type': 'cosine', 'start': 1e-6, 'stop': 0.04, 'num_timesteps': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one_step_mats = [_get_custom_transition_mat(spec, t)\n",
    "                            for t in range(0, spec['num_timesteps'])]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (spec['num_timesteps'],\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, spec['num_timesteps']):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "zero_to_one = []\n",
    "one_to_zero = []\n",
    "for t in range(0, spec['num_timesteps'], 1):\n",
    "    zero_to_one.append(q_mats[t][0][1].item())\n",
    "    one_to_zero.append(q_mats[t][1][0].item())\n",
    "    print(q_mats[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0].plot(zero_to_one, label='0 -> 1')\n",
    "axs[0].legend()  # Add a legend\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1].plot(one_to_zero, label='1 -> 0')\n",
    "axs[1].legend()  # Add a legend\n",
    "\n",
    "# Show the plot\n",
    "fig.suptitle('Cumulative transition probabilities with cosine noise schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one_step_mats = [_get_custom_transition_mat(spec, t)\n",
    "                            for t in range(0, spec['num_timesteps'])]\n",
    "q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to('cpu', non_blocking=True)\n",
    "assert q_onestep_mats.shape == (spec['num_timesteps'],\n",
    "                                    2,\n",
    "                                    2)\n",
    "\n",
    "# Construct transition matrices for q(x_t|x_start)\n",
    "q_mat_t = q_onestep_mats[0]\n",
    "q_mats = [q_mat_t]\n",
    "for t in range(1, spec['num_timesteps']):\n",
    "    # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "    q_mat_t = torch.tensordot(q_mat_t, q_onestep_mats[t],\n",
    "                            dims=[[1], [0]])\n",
    "    q_mats.append(q_mat_t)\n",
    "q_mats = torch.stack(q_mats, axis=0)\n",
    "\n",
    "zero_to_one = []\n",
    "one_to_zero = []\n",
    "for t in range(0, spec['num_timesteps'], 1):\n",
    "    zero_to_one.append(q_mats[t][0][1].item())\n",
    "    one_to_zero.append(q_mats[t][1][0].item())\n",
    "    print(q_mats[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0].plot(zero_to_one, label='0 -> 1')\n",
    "axs[0].legend()  # Add a legend\n",
    "\n",
    "# Plot on the second subplot\n",
    "axs[1].plot(one_to_zero, label='1 -> 0')\n",
    "axs[1].legend()  # Add a legend\n",
    "\n",
    "# Show the plot\n",
    "fig.suptitle('Cumulative transition probabilities with cosine noise schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    \n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        def cosine_beta_schedule(timesteps, s=0.008):\n",
    "            \"\"\"\n",
    "            Cosine schedule as described in https://arxiv.org/abs/2102.09672.\n",
    "\n",
    "            Parameters:\n",
    "            - timesteps: int, the number of timesteps for the schedule.\n",
    "            - s: float, small constant to prevent numerical issues.\n",
    "\n",
    "            Returns:\n",
    "            - betas: torch.Tensor, beta values for each timestep.\n",
    "            - alphas: torch.Tensor, alpha values for each timestep.\n",
    "            - alpha_bars: torch.Tensor, cumulative product of alphas for each timestep.\n",
    "            \"\"\"\n",
    "            steps = timesteps + 1\n",
    "            x = torch.linspace(0, timesteps, steps)\n",
    "            alphas_cumprod = torch.cos((x / timesteps + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "\n",
    "            betas = 1 - alphas_cumprod[1:] / alphas_cumprod[:-1]\n",
    "            alphas = 1 - betas\n",
    "            alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "            return betas, alphas, alpha_bars\n",
    "        betas, alphas, alpha_bars = cosine_beta_schedule(spec['num_timesteps'])\n",
    "        return betas.to(device)\n",
    "    \n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n",
    "plt.style.use('seaborn-v0_8-paper')  # or 'ggplot', 'fivethirtyeight', etc.\n",
    "plt.rcParams.update({\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'figure.figsize': (8, 6),\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = {'type': 'cosine', 'start': 0.01, 'stop': 0.99, 'num_timesteps': 1000}\n",
    "betas = get_diffusion_betas(spec, 'cpu')\n",
    "alphas = 1 - betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "t_over_T = torch.linspace(0, 1, spec['num_timesteps'])\n",
    "spec_linear = {'type': 'linear', 'start': 0.01, 'stop': 0.01, 'num_timesteps': 1000}\n",
    "betas_linear = get_diffusion_betas(spec_linear, 'cpu')\n",
    "alphas_linear = 1 - betas_linear\n",
    "alpha_bars_linear = torch.cumprod(alphas_linear, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(betas[0], betas[-1])\n",
    "print(betas_linear[0], betas_linear[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_over_T, alpha_bars_linear, label='Linear')\n",
    "plt.plot(t_over_T, alpha_bars, label='Cosine')\n",
    "plt.xlabel(r'Diffusion Step ($t/T$)')\n",
    "plt.ylabel(r'$\\bar{\\alpha}_t$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3pm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
