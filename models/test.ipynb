{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "nodes = [(0, {'pos': (0.1, 0.65)}),\n",
    "         (1, {'pos': (0.05, 0.05)}), \n",
    "         (2, {'pos': (0.2, 0.15)}), \n",
    "         (3, {'pos': (0.55, 0.05)}),\n",
    "         (4, {'pos': (0.8, 0.05)}),\n",
    "         (5, {'pos': (0.9, 0.1)}),\n",
    "         (6, {'pos': (0.75, 0.15)}),\n",
    "         (7, {'pos': (0.5, 0.2)}),\n",
    "         (8, {'pos': (0.3, 0.3)}),\n",
    "         (9, {'pos': (0.2, 0.3)}),\n",
    "         (10, {'pos': (0.3, 0.4)}),\n",
    "         (11, {'pos': (0.65, 0.35)}),\n",
    "         (12, {'pos': (0.8, 0.5)}),\n",
    "         (13, {'pos': (0.5, 0.5)}),\n",
    "         (14, {'pos': (0.4, 0.65)}),\n",
    "         (15, {'pos': (0.15, 0.6)}),\n",
    "         (16, {'pos': (0.3, 0.7)}),\n",
    "         (17, {'pos': (0.5, 0.7)}),\n",
    "         (18, {'pos': (0.8, 0.8)}),\n",
    "         (19, {'pos': (0.4, 0.8)}),\n",
    "         (20, {'pos': (0.25, 0.85)}),\n",
    "         (21, {'pos': (0.1, 0.9)}),\n",
    "         (22, {'pos': (0.2, 0.95)}),\n",
    "         (23, {'pos': (0.45, 0.9)}),\n",
    "         (24, {'pos': (0.95, 0.95)}),\n",
    "         (25, {'pos': (0.9, 0.4)}),\n",
    "         (26, {'pos': (0.95, 0.05)})]\n",
    "edges = [(0, 21), (0, 1), (0, 15), (21, 22), (22, 20), (20, 23), (23, 24), (24, 18), (19, 14), (14, 15), (15, 16), (16, 20), (19, 20), (19, 17), (14, 17), (14, 16), (17, 18), (12, 18), (12, 13), (13, 14), (10, 14), (1, 15), (9, 15), (1, 9), (1, 2), (11, 12), (9, 10), (3, 7), (2, 3), (7, 8), (8, 9), (8, 10), (10, 11), (8, 11), (6, 11), (3, 4), (4, 5), (4, 6), (5, 6), (24, 25), (12, 25), (5, 25), (11, 25), (5, 26)]\n",
    "\n",
    "def visualize_predictions(samples, ground_truth_hist, ground_truth_fut, num_samples=5):\n",
    "        \"\"\"\n",
    "        Visualize the predictions of the model along with ground truth data.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        :param num_samples: Number of samples to visualize.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import networkx as nx\n",
    "        '''save_dir = f'{os.path.join(model_dir, f'{exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)'''\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(nodes)\n",
    "        all_edges = {tuple(edges[idx]) for idx in range(len(edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "        \n",
    "        pos = nx.get_node_attributes(G, 'pos')  # Retrieve node positions stored in node attributes\n",
    "\n",
    "        for i in range(min(num_samples, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "                subgraph_edges = {tuple(edges[idx]) for idx in edge_indices if idx < len(edges)}\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'# if plot_num == 0 else 'lightgreen' if plot_num == 1 else 'orange'\n",
    "                nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [[12, 13, 16], [4, 3], [21, 10], [37, 34]]\n",
    "ground_truth_hist = [[1, 0, 3, 4, 5], [32, 25, 17, 7, 6], [11, 15, 20, 26, 23], [22, 30, 29, 27, 35]] \n",
    "ground_truth_fut = [[6, 7], [5, 4], [24, 28], [36, 43]]\n",
    "\n",
    "visualize_predictions(samples, ground_truth_hist, ground_truth_fut, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm import tqdm\n",
    "TDRIVE_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive.h5'\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        \n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "                if 'edge_orientation' in path:\n",
    "                    path['edge_orientations'] = path.pop('edge_orientation')\n",
    "                paths.append(path)\n",
    "    nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "    \n",
    "    return paths, nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges = load_new_format(TDRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "lr = 0.01\n",
    "num_epochs = 50000\n",
    "learning_rate_warmup_steps = 2500\n",
    "lr_decay_parameter = 0.9998\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < learning_rate_warmup_steps:\n",
    "        return 1.0\n",
    "    else:\n",
    "        decay_lr = lr_decay_parameter ** (epoch - learning_rate_warmup_steps)\n",
    "        return max(decay_lr, 2e-5 / lr)\n",
    "\n",
    "\n",
    "# Calculate learning rates for each epoch\n",
    "learning_rates = [lr * lr_lambda(epoch) for epoch in range(num_epochs)]\n",
    "\n",
    "# Plot the learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, label='Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')#\n",
    "plt.yscale('log')\n",
    "plt.title('Learning Rate Schedule Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [(0, {'pos': (0.1, 0.65)}),\n",
    "         (1, {'pos': (0.05, 0.05)}), \n",
    "         (2, {'pos': (0.2, 0.15)}), \n",
    "         (3, {'pos': (0.55, 0.05)}),\n",
    "         (4, {'pos': (0.8, 0.05)}),\n",
    "         (5, {'pos': (0.9, 0.1)}),\n",
    "         (6, {'pos': (0.75, 0.15)}),\n",
    "         (7, {'pos': (0.5, 0.2)}),\n",
    "         (8, {'pos': (0.3, 0.3)}),\n",
    "         (9, {'pos': (0.2, 0.3)}),\n",
    "         (10, {'pos': (0.3, 0.4)}),\n",
    "         (11, {'pos': (0.65, 0.35)}),\n",
    "         (12, {'pos': (0.8, 0.5)}),\n",
    "         (13, {'pos': (0.5, 0.5)}),\n",
    "         (14, {'pos': (0.4, 0.65)}),\n",
    "         (15, {'pos': (0.15, 0.6)}),\n",
    "         (16, {'pos': (0.3, 0.7)}),\n",
    "         (17, {'pos': (0.5, 0.7)}),\n",
    "         (18, {'pos': (0.8, 0.8)}),\n",
    "         (19, {'pos': (0.4, 0.8)}),\n",
    "         (20, {'pos': (0.25, 0.85)}),\n",
    "         (21, {'pos': (0.1, 0.9)}),\n",
    "         (22, {'pos': (0.2, 0.95)}),\n",
    "         (23, {'pos': (0.45, 0.9)}),\n",
    "         (24, {'pos': (0.95, 0.95)}),\n",
    "         (25, {'pos': (0.9, 0.4)}),\n",
    "         (26, {'pos': (0.95, 0.05)}),\n",
    "         (27, {'pos': (0.75, 1.0)})]\n",
    "edges = [(0, 21), (0, 1), (0, 15), (21, 22), (22, 20), (20, 23), (23, 24), (24, 18), (19, 14), (14, 15), (15, 16), (16, 20), (19, 20), (19, 17), (14, 17), (14, 16), (17, 18), (12, 18), (12, 13), (13, 14), (10, 14), (1, 15), (9, 15), (1, 9), (1, 2), (11, 12), (9, 10), (3, 7), (2, 3), (7, 8), (8, 9), (8, 10), (10, 11), (8, 11), (6, 11), (3, 4), (4, 5), (4, 6), (5, 6), (24, 25), (12, 25), (5, 25), (11, 25), (5, 26), (23, 27), (24, 27)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "res = {'sample_list': [tensor([ 0,  2,  7, 11, 15, 20, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2,  7, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 30, 34, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37]), tensor([ 0,  2, 23, 24, 30]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37, 45]), tensor([ 0,  2, 23, 24, 30, 34, 36, 37]), tensor([ 4, 12, 17, 18, 21, 24, 40, 41]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2,  7, 11, 15, 20, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2,  7, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 30, 34, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37])], 'ground_truth_hist': [tensor([[43, 36, 37, 34, 32]]), tensor([[23, 26, 20, 14, 16]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[39, 41, 36, 35, 27]]), tensor([[44,  5, 11, 10, 22]]), tensor([[24, 28, 35, 36, 41]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[35, 27, 29, 30, 22]]), tensor([[39, 42, 33, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[38, 34, 25, 18, 19]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[10, 15, 14, 16, 17]]), tensor([[26, 32, 34, 38, 41]]), tensor([[45,  7, 17, 25, 33]]), tensor([[37, 34, 32, 26, 22]]), tensor([[ 2, 22, 26, 32, 34]])], 'ground_truth_fut': [tensor([[20, 15]]), tensor([[ 7, 45]]), tensor([[12,  4]]), tensor([[34, 37]]), tensor([[29, 30]]), tensor([[23, 24]]), tensor([[40, 17]]), tensor([[36, 35]]), tensor([[2, 0]]), tensor([[21, 24]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[15, 11]]), tensor([[26, 23]]), tensor([[18, 40]]), tensor([[40, 41]]), tensor([[39, 45]]), tensor([[30, 23]]), tensor([[2, 0]]), tensor([[37, 36]])]}\n",
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import F1Score\n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [[tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40])], [tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26])], [tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37])], [tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40])], [tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34])], [tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40])], [tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18])], [tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45])]], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [res['sample_list'][i][0] for i in range(len(res['sample_list']))]\n",
    "print(samples)\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1_tot = 0\n",
    "for i in range(len(res['sample_list'])):\n",
    "    f1 = metric(one_hot_samples[i], one_hot_futures[i])\n",
    "    f1_tot += f1\n",
    "f1 = f1_tot / len(res['sample_list'])\n",
    "print(one_hot_samples[0])\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_10 = [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])]\n",
    "sample_1 = [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])]\n",
    "for j in range(10):\n",
    "        sample_10 = [res['sample_list'][i][j] for i in range(len(res['sample_list']))]\n",
    "        equal = 0\n",
    "        for i in range(len(sample_10)):\n",
    "                if sample_10[i].equal(sample_1[i]):\n",
    "                        equal += 1\n",
    "                \n",
    "        print(f\"Ratio of equal samples for sample {j}:\", equal/len(sample_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([11, 12, 18, 19]), tensor([18, 19]), tensor([18, 19]), tensor([35, 36]), tensor([11, 12]), tensor([11, 12]), tensor([18, 19]), tensor([41]), tensor([24, 35, 36]), tensor([24, 35, 36]), tensor([11, 12, 19]), tensor([20, 21, 40, 41]), tensor([20, 21, 40, 41]), tensor([20, 21, 41]), tensor([35, 36]), tensor([ 0,  2, 40, 41]), tensor([11, 12]), tensor([35, 36]), tensor([11, 12]), tensor([36])],\n",
    "\n",
    "\n",
    "'ground_truth_hist': [tensor([[45,  7, 17, 25, 33]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[43, 36, 37, 34, 32]]), tensor([[40, 18, 19, 14, 13]]), tensor([[41, 40, 18, 19,  8]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[37, 34, 32, 26, 22]]), tensor([[35, 27, 29, 30, 22]]), tensor([[39, 42, 33, 30, 22]]), tensor([[24, 28, 35, 36, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[39, 41, 36, 35, 27]]), tensor([[26, 32, 34, 38, 41]]), tensor([[38, 34, 25, 18, 19]]), tensor([[44,  5, 11, 10, 22]])],\n",
    "\n",
    "\n",
    "'ground_truth_fut': [tensor([[30, 23]]), tensor([[26, 23]]), tensor([[19, 18]]), tensor([[20, 15]]), tensor([[12,  4]]), tensor([[12,  4]]), tensor([[40, 41]]), tensor([[34, 37]]), tensor([[36, 35]]), tensor([[ 7, 45]]), tensor([[18, 40]]), tensor([[2, 0]]), tensor([[2, 0]]), tensor([[21, 24]]), tensor([[40, 17]]), tensor([[37, 36]]), tensor([[29, 30]]), tensor([[39, 45]]), tensor([[15, 11]]), tensor([[23, 24]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples).reshape(20, 46), torch.cat(one_hot_futures).reshape(20, 46))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cat(one_hot_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(torch.cat(one_hot_samples).reshape(20, 46)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Future = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "Predicted_Future = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1 = metric(Predicted_Future, True_Future)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([], dtype=torch.int64), tensor([ 7, 22, 23]), tensor([], dtype=torch.int64), tensor([ 4,  5, 34, 36, 37]), tensor([0]), tensor([10, 11, 15, 18, 19]), tensor([0]), tensor([0]), tensor([21, 23, 39, 45]), tensor([ 0, 29, 30, 36]), tensor([0, 4]), tensor([0]), tensor([0]), tensor([17, 35, 40]), tensor([], dtype=torch.int64), tensor([0]), tensor([0]), tensor([ 4,  5, 34, 36, 37]), tensor([21, 23, 24, 39, 40, 45]), tensor([21, 30])], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_gaussian_transition_mat(t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition matrix Q with\n",
    "        decaying entries as a function of how far off diagonal the entry is.\n",
    "        Normalization option 1:\n",
    "        Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                    1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                    0                          else.\n",
    "\n",
    "        Normalization option 2:\n",
    "        tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                            0                        else.\n",
    "\n",
    "        Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        \n",
    "        num_classes = 2\n",
    "        transition_bands = num_classes - 1 # 1\n",
    "\n",
    "        betas = torch.linspace(0.9, 1.0, 1000)\n",
    "        beta_t = betas[t]\n",
    "\n",
    "        mat = torch.zeros((num_classes, num_classes),\n",
    "                        dtype=torch.float64)\n",
    "\n",
    "        # Make the values correspond to a similar type of gaussian as in the\n",
    "        # gaussian diffusion case for continuous state spaces.\n",
    "        values = torch.linspace(torch.tensor(0.), torch.tensor(num_classes-1), num_classes, dtype=torch.float64)\n",
    "        values = values * 2./ (num_classes - 1.)\n",
    "        values = values[:transition_bands+1]\n",
    "        values = -values * values / beta_t\n",
    "        \n",
    "        # To reverse the tensor 'values' starting from the second element\n",
    "        reversed_values = values[1:].flip(dims=[0])\n",
    "        # Concatenating the reversed values with the original values\n",
    "        values = torch.cat([reversed_values, values], dim=0)\n",
    "        values = F.softmax(values, dim=0)\n",
    "        values = values[transition_bands:]\n",
    "        \n",
    "        for k in range(1, transition_bands + 1):\n",
    "            off_diag = torch.full((num_classes - k,), values[k], dtype=torch.float64)\n",
    "\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "\n",
    "        # Add diagonal values such that rows and columns sum to one.\n",
    "        # Technically only the ROWS need to sum to one\n",
    "        # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "        # which is necessary if we want to have a uniform stationary distribution.\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag_embed(diag)\n",
    "\n",
    "        return mat\n",
    "\n",
    "_get_gaussian_transition_mat(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prior_distribution_transition_mat(t):\n",
    "    \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "    Args:\n",
    "    t: timestep. integer scalar.\n",
    "\n",
    "    Returns:\n",
    "    Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    betas = torch.linspace(0.001, 0.2, 1000)\n",
    "    beta_t = betas[t]\n",
    "    steps = torch.linspace(0, 1, 1000 + 1, dtype=torch.float64)\n",
    "    alpha_bar = torch.cos((steps + 0.008) / 1.008 * torch.pi / 2)\n",
    "    betas = torch.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], torch.tensor(0.999))\n",
    "    beta_t = betas[t]\n",
    "    print(beta_t)\n",
    "    num_classes = 2\n",
    "    class_weights = [0.9, 0.1]\n",
    "    mat = torch.zeros((num_classes, num_classes), dtype=torch.float64)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                mat[i, j] = beta_t * class_weights[j]\n",
    "            else:\n",
    "                mat[i, j] = 1 - beta_t + beta_t * class_weights[j]\n",
    "    \n",
    "    return mat\n",
    "\n",
    "_get_prior_distribution_transition_mat(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def cosine_beta_schedule_discrete(timesteps, s=0.008):\n",
    "    \"\"\" Cosine schedule as proposed in https://openreview.net/forum?id=-NEXDKk8gZ. \"\"\"\n",
    "    steps = timesteps + 2\n",
    "    x = np.linspace(0, steps, steps)\n",
    "\n",
    "    alphas_cumprod = np.cos(0.5 * np.pi * ((x / steps) + s) / (1 + s)) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    alphas = (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = 1 - alphas\n",
    "    return torch.tensor(betas, dtype=torch.float64)\n",
    "\n",
    "class PlaceHolder:\n",
    "    def __init__(self, X, E, y):\n",
    "        self.X = X\n",
    "        self.E = E\n",
    "        self.y = y\n",
    "\n",
    "    def type_as(self, x: torch.Tensor):\n",
    "        \"\"\" Changes the device and dtype of X, E, y. \"\"\"\n",
    "        self.X = self.X.type_as(x)\n",
    "        self.E = self.E.type_as(x)\n",
    "        self.y = self.y.type_as(x)\n",
    "        return self\n",
    "\n",
    "    def mask(self, node_mask, collapse=False):\n",
    "        x_mask = node_mask.unsqueeze(-1)          # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)             # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)             # bs, 1, n, 1\n",
    "\n",
    "        if collapse:\n",
    "            self.X = torch.argmax(self.X, dim=-1)\n",
    "            self.E = torch.argmax(self.E, dim=-1)\n",
    "\n",
    "            self.X[node_mask == 0] = - 1\n",
    "            self.E[(e_mask1 * e_mask2).squeeze(-1) == 0] = - 1\n",
    "        else:\n",
    "            self.X = self.X * x_mask\n",
    "            self.E = self.E * e_mask1 * e_mask2\n",
    "            assert torch.allclose(self.E, torch.transpose(self.E, 1, 2))\n",
    "        return self\n",
    "\n",
    "class DiscreteUniformTransition:\n",
    "    def __init__(self, x_classes: int, e_classes: int, y_classes: int):\n",
    "        self.X_classes = x_classes\n",
    "        self.E_classes = e_classes\n",
    "        self.y_classes = y_classes\n",
    "        self.u_x = torch.ones(1, self.X_classes, self.X_classes)\n",
    "        if self.X_classes > 0:\n",
    "            self.u_x = self.u_x / self.X_classes\n",
    "\n",
    "        self.u_e = torch.ones(1, self.E_classes, self.E_classes)\n",
    "        if self.E_classes > 0:\n",
    "            self.u_e = self.u_e / self.E_classes\n",
    "\n",
    "        self.u_y = torch.ones(1, self.y_classes, self.y_classes)\n",
    "        if self.y_classes > 0:\n",
    "            self.u_y = self.u_y / self.y_classes\n",
    "\n",
    "    def get_Qt(self, beta_t, device):\n",
    "        \"\"\" Returns one-step transition matrices for X and E, from step t - 1 to step t.\n",
    "        Qt = (1 - beta_t) * I + beta_t / K\n",
    "\n",
    "        beta_t: (bs)                         noise level between 0 and 1\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy).\n",
    "        \"\"\"\n",
    "        beta_t = beta_t.unsqueeze(1).unsqueeze(1)\n",
    "        beta_t = beta_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = beta_t * self.u_x + (1 - beta_t) * torch.eye(self.X_classes, device=device).unsqueeze(0)\n",
    "        q_e = beta_t * self.u_e + (1 - beta_t) * torch.eye(self.E_classes, device=device).unsqueeze(0)\n",
    "        q_y = beta_t * self.u_y + (1 - beta_t) * torch.eye(self.y_classes, device=device).unsqueeze(0)\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n",
    "    \n",
    "class MarginalUniformTransition:\n",
    "    def __init__(self, x_marginals, e_marginals, y_classes):\n",
    "        self.X_classes = len(x_marginals)\n",
    "        self.E_classes = len(e_marginals)\n",
    "        self.y_classes = y_classes\n",
    "        self.x_marginals = x_marginals\n",
    "        self.e_marginals = e_marginals\n",
    "\n",
    "        self.u_x = x_marginals.unsqueeze(0).expand(self.X_classes, -1).unsqueeze(0)\n",
    "        self.u_e = e_marginals.unsqueeze(0).expand(self.E_classes, -1).unsqueeze(0)\n",
    "        self.u_y = torch.ones(1, self.y_classes, self.y_classes)\n",
    "        if self.y_classes > 0:\n",
    "            self.u_y = self.u_y / self.y_classes\n",
    "\n",
    "    def get_Qt(self, beta_t, device):\n",
    "        \"\"\" Returns one-step transition matrices for X and E, from step t - 1 to step t.\n",
    "        Qt = (1 - beta_t) * I + beta_t / K\n",
    "\n",
    "        beta_t: (bs)                         noise level between 0 and 1\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy). \"\"\"\n",
    "        beta_t = beta_t.unsqueeze(1).unsqueeze(1)\n",
    "        print(beta_t[100])\n",
    "        beta_t = beta_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = beta_t * self.u_x + (1 - beta_t) * torch.eye(self.X_classes, device=device).unsqueeze(0)\n",
    "        q_e = beta_t * self.u_e + (1 - beta_t) * torch.eye(self.E_classes, device=device).unsqueeze(0)\n",
    "        q_y = beta_t * self.u_y + (1 - beta_t) * torch.eye(self.y_classes, device=device).unsqueeze(0)\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n",
    "\n",
    "# Example usage\n",
    "timesteps = 1000\n",
    "betas = cosine_beta_schedule_discrete(timesteps)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transition = MarginalUniformTransition(x_marginals=torch.tensor([0.5, 0.5]), e_marginals=torch.tensor([0.9, 0.1]), y_classes=2)\n",
    "beta_t = betas.to(device)\n",
    "q_t_matrices = transition.get_Qt(beta_t, device)\n",
    "\n",
    "# Print the transition matrix for edges at the final timestep\n",
    "print(\"Transition matrix for edges (q_e) at the final timestep:\")\n",
    "t = 997\n",
    "print(q_t_matrices.E[t+1])\n",
    "print(_get_prior_distribution_transition_mat(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters\n",
    "N = 1000  # number of trials\n",
    "p = 0.012  # probability of success\n",
    "n_values = np.arange(0, 21)  # number of successes from 0 to 20\n",
    "\n",
    "# Calculate the probabilities\n",
    "probabilities = binom.pmf(n_values, N, p)\n",
    "\n",
    "# Display the probabilities\n",
    "for n, prob in zip(n_values, probabilities):\n",
    "    print(f\"P(X = {n}) = {prob:.10f}\")\n",
    "\n",
    "# Plot the probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.stem(n_values, probabilities, use_line_collection=True)\n",
    "plt.xlabel('Number of successes (n)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Binomial Distribution PMF (N=1000, p=0.012)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        \n",
    "        # Convert edges to a list of tuples\n",
    "        edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "        self.graph.add_edges_from(self.edges)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # trajectory_name = self.keys[idx]\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        # edge_coordinates_data = trajectory.get('coordinates', [])\n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.stack((history_one_hot_edges, history_edge_orientations), dim=1)\n",
    "            future_edge_features = torch.stack((future_one_hot_edges, future_edge_orientations), dim=1)\n",
    "        else:\n",
    "            history_edge_features = history_one_hot_edges\n",
    "            future_edge_features = future_one_hot_edges\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"node_in_history\": node_in_history,\n",
    "        }, self.graph\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrajectoryDataset(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_1001_1200.h5\", 5, 2, edge_features=['one_hot_edges', 'coordinates'])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given tensor\n",
    "original_tensor = torch.tensor([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Current edge tensor\n",
    "current_edge = torch.tensor([39, 17])\n",
    "\n",
    "# Transforming the tensor\n",
    "def transform_tensor(original_tensor, current_edge):\n",
    "    batch_size = original_tensor.size(0)\n",
    "    max_neighbors = (original_tensor == 1).sum(dim=1).max().item()\n",
    "    transformed_tensor = torch.zeros((batch_size, max_neighbors), dtype=torch.long)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Find indices of neighbors\n",
    "        neighbor_indices = (original_tensor[i] == 1).nonzero(as_tuple=False).squeeze()\n",
    "        # Only keep the index where it matches the current edge\n",
    "        valid_indices = (neighbor_indices == current_edge[i]).nonzero(as_tuple=False).squeeze()    \n",
    "        # Set the values in the transformed tensor\n",
    "        transformed_tensor[i, valid_indices] = 1\n",
    "\n",
    "    return transformed_tensor\n",
    "\n",
    "transformed_tensor = transform_tensor(original_tensor, current_edge)\n",
    "print(transformed_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import LineGraph\n",
    "#from dataset.trajctory_dataset import TrajectoryDataset, collate_fn\n",
    "#from .d3pm_diffusion import make_diffusion\n",
    "#from .d3pm_edge_encoder import Edge_Encoder\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "\n",
    "class Benchmark_Models(nn.Module):\n",
    "    def __init__(self, data_config, model_config, train_config, test_config, wandb_config, model):\n",
    "        super(Benchmark_Models, self).__init__()\n",
    "        # Data\n",
    "        self.data_config = data_config\n",
    "        self.train_data_path = self.data_config['train_data_path']\n",
    "        self.test_data_path = self.data_config['test_data_path']\n",
    "        self.history_len = self.data_config['history_len']\n",
    "        self.future_len = self.data_config['future_len']\n",
    "        self.num_classes = self.data_config['num_classes']\n",
    "        self.edge_features = self.data_config['edge_features']\n",
    "        \n",
    "        # Model\n",
    "        self.model_config = model_config\n",
    "        self.model = model # Edge_Encoder\n",
    "        self.hidden_channels = self.model_config['hidden_channels']\n",
    "        self.condition_dim = self.model_config['condition_dim']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        \n",
    "        # Training\n",
    "        self.train_config = train_config\n",
    "        self.lr = self.train_config['lr']\n",
    "        self.lr_decay_parameter = self.train_config['lr_decay']\n",
    "        self.learning_rate_warmup_steps = self.train_config['learning_rate_warmup_steps']\n",
    "        self.num_epochs = self.train_config['num_epochs']\n",
    "        self.gradient_accumulation = self.train_config['gradient_accumulation']\n",
    "        self.gradient_accumulation_steps = self.train_config['gradient_accumulation_steps']\n",
    "        self.batch_size = self.train_config['batch_size'] if not self.gradient_accumulation else self.train_config['batch_size'] * self.gradient_accumulation_steps\n",
    "        \n",
    "        # Testing\n",
    "        self.test_config = test_config\n",
    "        self.test_batch_size = self.test_config['batch_size']\n",
    "        self.model_path = self.test_config['model_path']\n",
    "        self.eval_every_steps = self.test_config['eval_every_steps']\n",
    "        \n",
    "        # WandB\n",
    "        self.wandb_config = wandb_config\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            project=self.wandb_config['project'],\n",
    "            entity=self.wandb_config['entity'],\n",
    "            notes=self.wandb_config['notes'],\n",
    "            job_type=self.wandb_config['job_type'],\n",
    "            config={**self.data_config, **self.model_config, **self.train_config}\n",
    "        )\n",
    "        self.exp_name = self.wandb_config['exp_name']\n",
    "        wandb.run.name = self.exp_name\n",
    "\n",
    "        # Logging\n",
    "        self.dataset = self.data_config['dataset']\n",
    "        self.model_dir = os.path.join(\"experiments\", self.exp_name)\n",
    "        os.makedirs(self.model_dir,exist_ok=True)\n",
    "        log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        log_name = f\"{self.dataset}_{log_name}\"\n",
    "        \n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        log_dir = os.path.join(self.model_dir, log_name)\n",
    "        file_handler = logging.FileHandler(log_dir)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.log.addHandler(file_handler)\n",
    "        \n",
    "        self.log_loss_every_steps = self.train_config['log_loss_every_steps']        \n",
    "        \n",
    "        # Build Components\n",
    "        self._build_train_dataloader()\n",
    "        self._build_test_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def train(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        '''def get_neighbors(line_graph, edge, device):\n",
    "            neighbors = []\n",
    "            for e in edge:\n",
    "                neighbor_set = set()\n",
    "                for i in range(line_graph.edge_index.size(1)):\n",
    "                    if line_graph.edge_index[0, i] == e:\n",
    "                        neighbor_set.add(line_graph.edge_index[1, i].item())\n",
    "                    elif line_graph.edge_index[1, i] == e:\n",
    "                        neighbor_set.add(line_graph.edge_index[0, i].item())\n",
    "                neighbors.append(list(neighbor_set))\n",
    "            # Create a binary tensor for neighbors\n",
    "            neighbors_binary = torch.zeros((len(edge), self.num_edges), dtype=torch.long, device=device)\n",
    "            \n",
    "            for i, n in enumerate(neighbors):\n",
    "                neighbors_binary[i, n] = 1\n",
    "            \n",
    "            return neighbors_binary\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            ground_truth_fut = []\n",
    "            pred_fut = []\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in self.train_data_loader:\n",
    "                history_edge_features = batch[\"history_edge_features\"]\n",
    "                last_history_edge = batch[\"history_indices\"][:, -1]\n",
    "                future_edge_indices = batch[\"future_indices\"]\n",
    "                future_edge_features = batch[\"future_edge_features\"]\n",
    "                future_edge_indices_one_hot = future_edge_features[:, :, 0]\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                visited_edges = [set() for _ in range(history_edge_features.size(0))]   # keep track of visited edges, to avoid cycles\n",
    "                prediction = []\n",
    "                for idx in range(self.future_len):\n",
    "                    future = future_edge_indices_one_hot.clone()\n",
    "                    future.zero_()\n",
    "                    future[:, idx] = 1\n",
    "                    \"\"\"for j in range(future_edge_indices.size(0)):\n",
    "                        future[j, future_edge_indices[j, idx]] = 1\"\"\"\"\"\"\n",
    "                    \n",
    "                    neighbors = get_neighbors(self.line_graph, last_history_edge, device=history_edge_features.device)\n",
    "                    logits, preds = self.model(history_edge_features, neighbors)\n",
    "                    \n",
    "                    # Mask logits for visited edges\n",
    "                    #logits = logits.clone()\n",
    "                    \"\"\"for i in range(len(preds)):\n",
    "                        for visited_edge in visited_edges[i]:\n",
    "                            logits[i, neighbors[i] == visited_edge] = float('-inf')\n",
    "                    \n",
    "                    # Update history and visited edges\n",
    "                    for i in range(len(preds)):\n",
    "                        visited_edges[i].add(preds[i].item())  # Add predicted edge to visited set\n",
    "                        history_edge_features[i] = history_edge_features[i].clone()\n",
    "                        history_edge_features[i, preds[i], 0] = 1\"\"\"  # Add it to history edge features\n",
    "                    last_history_edge = preds   # Update last history edge\n",
    "                    loss = F.binary_cross_entropy_with_logits(logits, future)\n",
    "                    prediction.append(preds)\n",
    "                    loss.backward()\n",
    "\n",
    "                pred_fut.append(prediction)\n",
    "                ground_truth_fut.append(future_edge_indices_one_hot)\n",
    "                # Calculate the loss (cross-entropy)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            #f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach(), torch.flatten(torch.cat(ground_truth_fut)).detach())\n",
    "            # Logging\n",
    "            if (epoch + 1) % self.log_loss_every_steps == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "                wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "                #wandb.log({\"epoch\": epoch + 1, \"average_F1_score\": f1_epoch.item()})\n",
    "                print(pred_fut)\n",
    "                print(future_edge_indices)\n",
    "                self.log.info(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
    "                print(\"Epoch:\", epoch + 1)\n",
    "                print(\"Loss:\", avg_loss)\n",
    "                #print(\"F1:\", f1_epoch.item())'''\n",
    "        def transform_tensor(neighbor_tensor, current_edge):\n",
    "            batch_size = neighbor_tensor.size(0)\n",
    "            max_neighbors = (neighbor_tensor == 1).sum(dim=1).max().item()\n",
    "            transformed_tensor = torch.zeros((batch_size, max_neighbors), dtype=torch.long)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find indices of neighbors\n",
    "                neighbor_indices = (neighbor_tensor[i] == 1).nonzero(as_tuple=False).squeeze()\n",
    "                # Only keep the index where it matches the current edge\n",
    "                valid_indices = (neighbor_indices == current_edge[i]).nonzero(as_tuple=False).squeeze()\n",
    "                # Set the values in the transformed tensor\n",
    "                transformed_tensor[i, valid_indices] = 1\n",
    "\n",
    "            return transformed_tensor\n",
    "        \n",
    "        '''def group_and_pad(tensor, batch_size, pad_value=0):\n",
    "            # Get unique groups\n",
    "            groups = torch.unique(tensor[:, 0])\n",
    "            grouped_sequences = []\n",
    "            \n",
    "            # Find the maximum length of sequences\n",
    "            max_len = 0\n",
    "            for group in groups:\n",
    "                group_elements = tensor[tensor[:, 0] == group][:, 1]\n",
    "                grouped_sequences.append(group_elements)\n",
    "                max_len = max(max_len, len(group_elements))\n",
    "            \n",
    "            # Pad the sequences\n",
    "            padded_sequences = []\n",
    "            for seq in grouped_sequences:\n",
    "                padded_seq = torch.cat([seq, torch.full((max_len - len(seq),), pad_value)])\n",
    "                padded_sequences.append(padded_seq)\n",
    "            \n",
    "            return torch.stack(padded_sequences)'''\n",
    "        \n",
    "        def group_and_pad(tensor, batch_size, pad_value=0, missing_value=-1):\n",
    "            grouped_sequences = []\n",
    "\n",
    "            # Iterate through all possible groups\n",
    "            for group in range(batch_size):\n",
    "                group_elements = tensor[tensor[:, 0] == group][:, 1]\n",
    "                if len(group_elements) == 0:\n",
    "                    # If the group is missing, add the missing value\n",
    "                    group_elements = torch.tensor([missing_value])\n",
    "                grouped_sequences.append(group_elements)\n",
    "\n",
    "            # Find the maximum length of sequences\n",
    "            max_len = max(len(seq) for seq in grouped_sequences)\n",
    "            \n",
    "            # Pad the sequences\n",
    "            padded_sequences = []\n",
    "            for seq in grouped_sequences:\n",
    "                padded_seq = torch.cat([seq, torch.full((max_len - len(seq),), pad_value)])\n",
    "                padded_sequences.append(padded_seq)\n",
    "            \n",
    "            return torch.stack(padded_sequences)\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            pred_fut = []\n",
    "            ground_truth_fut = []\n",
    "            for batch in self.train_data_loader:\n",
    "                hidden = self.model.init_hidden(self.batch_size)\n",
    "                history_edge_features = batch[\"history_edge_features\"]\n",
    "                history_edge_indices = batch[\"history_indices\"]\n",
    "                history_edge_indices_one_hot = history_edge_features[:, :, 0]\n",
    "                initial_edge = batch[\"history_indices\"][:, -1]\n",
    "                future_edge_indices = batch[\"future_indices\"]\n",
    "                future_edge_features = batch[\"future_edge_features\"]\n",
    "                future_edge_indices_one_hot = future_edge_features[:, :, 0]\n",
    "                batch_size_act = history_edge_features.size(0)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = 0\n",
    "                preds = []\n",
    "                preds_binary = torch.zeros_like(future_edge_indices_one_hot)\n",
    "                for t in range(self.future_len):\n",
    "                    if t == 0:\n",
    "                        current_edge = initial_edge\n",
    "                    else:\n",
    "                        current_edge = future_edge_indices[:, t-1]\n",
    "                    \n",
    "                    true_neighbors = self.get_neighbors(self.line_graph, current_edge)\n",
    "                    # TODO: Use history edge features at t == 0, then add true future_edge_features at t-1\n",
    "                    input_features = self.model.get_neighbor_features(history_edge_features, true_neighbors)\n",
    "                    out, hidden = self.model(input_features, hidden)\n",
    "                    logits = out.squeeze(-1)  # Remove the last dimension to match (bs, num_neighbors)\n",
    "                    \n",
    "                    #masked_logits = logits.clone()\n",
    "                    #masked_logits[true_neighbors == 0] = -100    # Mask out true neighbors\n",
    "                    #masked_logits[history_edge_indices_one_hot == 1] = -100   # Mask out history\n",
    "                    \n",
    "                    predicted_edge_indices = torch.argmax(logits, dim=1)\n",
    "                    true_neighbors_padded = group_and_pad(torch.argwhere(true_neighbors == 1), batch_size_act)  # (bs, num_neighbors)\n",
    "                    \n",
    "                    predicted_edges = true_neighbors_padded.gather(1, predicted_edge_indices.unsqueeze(1)).squeeze(1)\n",
    "                    preds.append(predicted_edges)\n",
    "                    ground_truth = transform_tensor(true_neighbors, future_edge_indices[:, t])\n",
    "                    loss += criterion(logits, ground_truth.float())\n",
    "                \n",
    "                for i in range(self.batch_size):\n",
    "                    pred_binary = torch.zeros(self.num_edges, dtype=torch.float)\n",
    "                    pred_binary[torch.stack(preds).t()[i]] = 1\n",
    "                    preds_binary[i] = pred_binary.clone().detach()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred_fut.append(preds_binary)\n",
    "                ground_truth_fut.append(future_edge_indices_one_hot)\n",
    "                \n",
    "            preds = [torch.stack(preds).t()]\n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach(), torch.flatten(torch.cat(ground_truth_fut)).detach())\n",
    "\n",
    "            if (epoch + 1) % self.log_loss_every_steps == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {avg_loss:.4f}, F1 Score: {f1_epoch.item()}\")\n",
    "                print(\"History Edges:\", history_edge_indices)\n",
    "                print(\"Predicted Edges:\", preds)\n",
    "                print(\"Future_edge_indices\", future_edge_indices)\n",
    "        \n",
    "        print(\"> Training Complete!\\n\")\n",
    "        \n",
    "    '''def get_neighbors(self, line_graph, edge):\n",
    "        neighbors = []\n",
    "        for e in edge:\n",
    "            # When e == -1, neighbor_tensor = torch.zeros(self.num_edges, dtype=torch.long)\n",
    "            neighbor_set = set()\n",
    "            for i in range(line_graph.edge_index.size(1)):\n",
    "                if line_graph.edge_index[0, i] == e:\n",
    "                    neighbor_set.add(line_graph.edge_index[1, i].item())\n",
    "                elif line_graph.edge_index[1, i] == e:\n",
    "                    neighbor_set.add(line_graph.edge_index[0, i].item())\n",
    "            neighbor_tensor = torch.zeros(self.num_edges, dtype=torch.long)\n",
    "            neighbor_tensor[list(neighbor_set)] = 1\n",
    "            neighbors.append(neighbor_tensor)\n",
    "        \n",
    "        return torch.stack(neighbors)'''\n",
    "        \n",
    "    def get_neighbors(self, line_graph, edge):\n",
    "        edge_tensor = torch.tensor(edge, dtype=torch.long)\n",
    "\n",
    "        # Loop through each edge in batch and update the corresponding row in neighbor_tensor\n",
    "        for i, e in enumerate(edge_tensor):\n",
    "            # Create a 2D boolean mask for matching edges in edge_index\n",
    "            mask0 = line_graph.edge_index[0].unsqueeze(1) == e.unsqueeze(0)\n",
    "            mask1 = line_graph.edge_index[1].unsqueeze(1) == e.unsqueeze(0)\n",
    "\n",
    "            # Find neighbors: if mask0[i, j] is True, the neighbor is edge_index[1, i], and vice versa for mask1\n",
    "            neighbors_index_0 = torch.where(mask0, line_graph.edge_index[1].unsqueeze(1), torch.full_like(line_graph.edge_index[1].unsqueeze(1), -1))\n",
    "            neighbor_indices = neighbors_index_0[:, 0][neighbors_index_0[:, 0] != -1]\n",
    "            \n",
    "            print(\"Edge\", e)\n",
    "            print(\"neighbor_indices\", neighbor_indices)\n",
    "\n",
    "            # We now need to construct the final neighbor tensor\n",
    "            neighbor_tensor = torch.zeros((edge_tensor.size(0), self.num_edges), dtype=torch.long, device=edge_tensor.device)\n",
    "            # Get unique neighbor indices for the current edge e\n",
    "            for j in range(len(neighbor_indices)):\n",
    "                neighbor_tensor[i, neighbor_indices[j]] = 1\n",
    "\n",
    "        print(neighbor_tensor)\n",
    "        return neighbor_tensor\n",
    "        \n",
    "    def save_model(self):\n",
    "        save_path = os.path.join(self.model_dir, \n",
    "                                 self.exp_name + '_' + self.model_config['name'] + '_' +  self.model_config['transition_mat_type'] + '_'  + \n",
    "                                 f'_hidden_dim_{self.hidden_channels}_condition_dim_{self.condition_dim}_layers_{self.num_layers}.pth')\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        self.log.info(f\"Model saved at {save_path}!\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.log.info(\"Model loaded!\\n\")\n",
    "    \n",
    "    def _build_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.learning_rate_warmup_steps:\n",
    "                return 1.0\n",
    "            else:\n",
    "                decay_lr = self.lr_decay_parameter ** (epoch - self.learning_rate_warmup_steps)\n",
    "                return max(decay_lr, 2e-5 / self.lr)\n",
    "            \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "        print(\"> Optimizer and Scheduler built!\\n\")\n",
    "        \n",
    "    def _build_train_dataloader(self):\n",
    "        print(\"Loading Training Dataset...\")\n",
    "        self.train_dataset = TrajectoryDataset(self.train_data_path, self.history_len, self.future_len, self.edge_features)\n",
    "        self.G = self.train_dataset.graph\n",
    "        self.nodes = self.G.nodes\n",
    "        self.edges = self.G.edges(data=True)\n",
    "        self.indexed_edges = self.train_dataset.edges\n",
    "        self.num_edge_features = self.train_dataset.num_edge_features\n",
    "        \n",
    "        # Build the line graph and corresponding edge index\n",
    "        edge_index = self._build_edge_index()\n",
    "        self.line_graph = Data(edge_index=edge_index)\n",
    "        \n",
    "        self.edge_tensor = self.train_dataset.get_all_edges_tensor()\n",
    "        self.num_edges = self.train_dataset.get_n_edges()\n",
    "        \n",
    "        self.train_data_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "                \n",
    "        print(\"> Training Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.edges], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.edges}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        \n",
    "        neighbor_counts = {edge_to_index[(u1, v1)]: 0 for u1, v1 in edge_list}\n",
    "        \n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "                    neighbor_counts[edge_to_index[(u1, v1)]] += 1\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        # Find the maximum neighbor degree\n",
    "        self.max_degree = max(neighbor_counts.values())\n",
    "        \n",
    "        return edge_index    \n",
    "\n",
    "    def _build_test_dataloader(self):\n",
    "        self.test_dataset = TrajectoryDataset(self.test_data_path, self.history_len, self.future_len, self.edge_features)\n",
    "        self.test_data_loader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        print(\"> Test Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_model(self):\n",
    "        #self.model = self.model(self.model_config, self.history_len, self.future_len, self.num_classes,\n",
    "        #                        nodes=self.nodes, edges=self.edges,\n",
    "        #                        num_edges=self.num_edges, hidden_channels=self.hidden_channels, num_edge_features=self.num_edge_features, max_degree=self.max_degree)\n",
    "        self.model = self.model(self.model_config, self.num_edge_features, self.num_edges)\n",
    "        print(\"> Model built!\\n\")\n",
    "        \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        if 'edge_indices' in new_hf['graph']:\n",
    "            edge_indices = new_hf['graph']['edge_indices'][:]\n",
    "            # Convert edges to a list of tuples\n",
    "            # Sort edges based on their saved indices\n",
    "            indexed_edges = sorted(zip(edges, edge_indices), key=lambda x: x[1])\n",
    "            edges = [edge for edge, _ in indexed_edges]\n",
    "        else:\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "\n",
    "        # Add edges with index to the graph\n",
    "        for (start, end), index in indexed_edges:\n",
    "            self.graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.stack((history_one_hot_edges, history_edge_orientations), dim=1)\n",
    "            future_edge_features = torch.stack((future_one_hot_edges, future_edge_orientations), dim=1)\n",
    "        else:\n",
    "            history_edge_features = history_one_hot_edges\n",
    "            future_edge_features = future_one_hot_edges\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"node_in_history\": node_in_history,\n",
    "        }, self.graph, self.edges\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    edges = [item[2] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "            \"edges\": edges,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeRNN(nn.Module):\n",
    "    def __init__(self, model_config, num_edge_features, num_edges):\n",
    "        super(EdgeRNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        self.input_dim = num_edge_features\n",
    "        self.num_edges = num_edges\n",
    "        self.hidden_dim = self.model_config['hidden_channels']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        self.rnn = nn.RNN(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)  # Output one logit per neighbor\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    def recursive_edge_prediction(self, edge_features, initial_edge, line_graph, future_len):\n",
    "        batch_size, num_edges, num_features = edge_features.size()\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(edge_features.device)\n",
    "        current_edge = initial_edge\n",
    "        predicted_edges = []\n",
    "        logits_list = []\n",
    "\n",
    "        for _ in range(future_len):\n",
    "            neighbors = self.get_neighbors(line_graph, current_edge, edge_features.device)  # shape: [batch_size, num_edges]\n",
    "            neighbor_features = self.get_neighbor_features(edge_features, neighbors)  # shape: [batch_size, num_neighbors, num_features]\n",
    "            max_num_neighbors = neighbors.sum(1).max().item()\n",
    "            logits, hidden = self.forward(neighbor_features, hidden)\n",
    "            logits = logits.squeeze(-1)  # Remove the last dimension to match [batch_size, num_neighbors]\n",
    "            \n",
    "            # Mask logits\n",
    "            mask = torch.arange(max_num_neighbors, device=edge_features.device).expand(len(neighbors), max_num_neighbors) < neighbors.sum(1, keepdim=True)\n",
    "            logits[~mask] = float(-10)\n",
    "            \n",
    "            logits_list.append(logits)\n",
    "            \n",
    "            _, predicted_edge_idx = torch.max(logits, dim=1)\n",
    "            #print(\"predicted_edge_idx\", predicted_edge_idx)\n",
    "            neighbor_indices = torch.nonzero(neighbors, as_tuple=True)\n",
    "            neighbor_indices = torch.split(neighbor_indices[1], neighbors.sum(dim=1).tolist())\n",
    "            max_len = max(len(s) for s in neighbor_indices)\n",
    "            neighbor_indices = torch.stack([F.pad(s, (0, max_len - len(s))) for s in neighbor_indices])\n",
    "            #print(\"Neighbor indices\", neighbor_indices)\n",
    "            predicted_edge = neighbor_indices.gather(1, predicted_edge_idx.unsqueeze(1)).squeeze(1)\n",
    "            #print(\"predicted_edge\", predicted_edge)\n",
    "\n",
    "            predicted_edges.append(predicted_edge)\n",
    "            current_edge = predicted_edge.unsqueeze(1)  # Update current edge for the next iteration\n",
    "        return logits_list, predicted_edges\n",
    "    \n",
    "    def get_neighbor_features(self, edge_features, neighbors):\n",
    "        batch_size, num_edges, num_features = edge_features.size()\n",
    "        neighbor_list = []\n",
    "        for i in range(batch_size):\n",
    "            if neighbors[i].sum() == 0:\n",
    "                neighbor_list.append(torch.zeros(1, num_features, dtype=torch.float))\n",
    "                continue\n",
    "            neighbor_indices = neighbors[i].nonzero(as_tuple=False).squeeze(1)\n",
    "            neighbor_list.append(edge_features[i, neighbor_indices])\n",
    "        \n",
    "        neighbor_features = torch.nn.utils.rnn.pad_sequence(neighbor_list, batch_first=True)\n",
    "        '''\n",
    "        # Filter out the row indicating the edge in the history!\n",
    "        filtered_neighbor_features = neighbor_features[~(neighbor_features[:, :, 0] == 1)]\n",
    "        filtered_neighbor_features = filtered_neighbor_features.view(neighbor_features.size(0), -1, neighbor_features.size(2))\n",
    "        return filtered_neighbor_features\n",
    "        '''\n",
    "        return neighbor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = EdgeRNN\n",
    "\n",
    "    \n",
    "data_config = {\"dataset\": \"synthetic_2_traj\",\n",
    "    \"train_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_2_traj.h5',\n",
    "    \"test_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_2_traj.h5',\n",
    "    \"history_len\": 5,\n",
    "    \"future_len\": 2,\n",
    "    \"num_classes\": 2,\n",
    "    \"edge_features\": ['one_hot_edges', 'coordinates'],\n",
    "    \"one_hot_nodes\": False}\n",
    "\n",
    "model_config = {\"name\": \"mlp_benchmark\",\n",
    "    \"hidden_channels\": 32,\n",
    "    \"time_embedding_dim\": 16,\n",
    "    \"condition_dim\": 16,\n",
    "    \"out_ch\": 1,\n",
    "    \"num_heads\": 1,\n",
    "    \"num_layers\": 1,\n",
    "    \"theta\": 1.0, # controls strength of conv layers in residual model\n",
    "    \"dropout\": 0.1,\n",
    "    \"model_output\": \"logits\",\n",
    "    \"model_prediction\": \"x_start\",  # Options: 'x_start','xprev'\n",
    "    \"transition_mat_type\": 'gaussian',  # Options: 'gaussian','uniform','absorbing', 'marginal_prior'\n",
    "    \"transition_bands\": 1,\n",
    "    \"loss_type\": \"cross_entropy_x_start\",  # Options: kl, cross_entropy_x_start, hybrid\n",
    "    \"hybrid_coeff\": 0.001,  # Only used for hybrid loss type.\n",
    "    \"class_weights\": [0.05, 0.95] # = future_len/num_edges and (num_edges - future_len)/num_edges\n",
    "    }\n",
    "\n",
    "train_config = {\"batch_size\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"gradient_accumulation\": False,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"learning_rate_warmup_steps\": 2000, # previously 10000\n",
    "    \"lr_decay\": 0.9999, # previously 0.9999\n",
    "    \"log_loss_every_steps\": 20,\n",
    "    \"save_model\": False,\n",
    "    \"save_model_every_steps\": 1000}\n",
    "\n",
    "test_config = {\"batch_size\": 1, # currently only 1 works\n",
    "    \"model_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/synthetic_d3pm_residual_fixed/synthetic_d3pm_residual_fixed_hidden_dim_32_time_dim_16_condition_dim_16_layers_2_weights_0.1.pth',\n",
    "    \"number_samples\": 1,\n",
    "    \"eval_every_steps\": 1000\n",
    "  }\n",
    "\n",
    "wandb_config = {\"exp_name\": \"tdrive_benchmark_test\",\n",
    "    \"project\": \"trajectory_prediction_using_denoising_diffusion_models\",\n",
    "    \"entity\": \"joeschmit99\",\n",
    "    \"job_type\": \"test\",\n",
    "    \"notes\": \"Benchmark test\",\n",
    "    \"tags\": [\"synthetic\", \"MLP_Benchmark\"]} \n",
    "\n",
    "model = Benchmark_Models(data_config, model_config, train_config, test_config, wandb_config, encoder_model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        if 'edge_indices' in new_hf['graph']:\n",
    "            edge_indices = new_hf['graph']['edge_indices'][:]\n",
    "            # Convert edges to a list of tuples\n",
    "            # Sort edges based on their saved indices\n",
    "            indexed_edges = sorted(zip(edges, edge_indices), key=lambda x: x[1])\n",
    "            edges = [edge for edge, _ in indexed_edges]\n",
    "        else:\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "\n",
    "        # Add edges with index to the graph\n",
    "        for (start, end), index in indexed_edges:\n",
    "            self.graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        #history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        #future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        \"\"\"node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\"\"\"\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            #\"history_coordinates\": history_coordinates,\n",
    "            #\"future_coordinates\": future_coordinates,\n",
    "            #\"history_one_hot_edges\": history_one_hot_edges,\n",
    "            #\"future_one_hot_edges\": future_one_hot_edges,\n",
    "            #\"history_edge_orientations\": history_edge_orientations,\n",
    "            #\"future_edge_orientations\": future_edge_orientations,\n",
    "            #\"node_in_history\": node_in_history,\n",
    "        }, self.graph# , self.edges\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    \"\"\"def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\"\"\"\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    # edges = [item[2] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    #history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    #future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    #history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    #future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    #history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    #future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    #history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    \"\"\"if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\"\"\"\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            #\"history_coordinates\": history_coordinates,\n",
    "            #\"future_coordinates\": future_coordinates,\n",
    "            #\"history_one_hot_edges\": history_one_hot_edges,\n",
    "            #\"future_one_hot_edges\": future_one_hot_edges,\n",
    "            #\"history_edge_orientations\": history_edge_orientations,\n",
    "            #\"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            #\"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "            # \"edges\": edges,\n",
    "        }'''\n",
    "        \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None, device=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.device = device\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = self.load_new_format(file_path, self.device)\n",
    "        \n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_new_format(file_path, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            #edges = torch.tensor(new_hf['graph']['edges'][:], dtype=torch.long, device=device)\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            #edges = [(torch.tensor(edge[0], device=device), torch.tensor(edge[1], device=device)) for edge in edges]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            \n",
    "        return paths, nodes, edges, edge_coordinates\n",
    "    \n",
    "    # @staticmethod\n",
    "    def build_graph(self):\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(self.nodes)\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "        for (start, end), index in indexed_edges:\n",
    "            graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        return graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "    \n",
    "        edge_idxs = trajectory['edge_idxs']\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = trajectory['edge_orientations']\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices, orientations, and coordinates\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "\n",
    "        # Extract and generate features\n",
    "        history_edge_features, future_edge_features = self.generate_edge_features(history_indices, future_indices, self.edge_coordinates)\n",
    "\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "        }\n",
    "\n",
    "    def generate_edge_features(self, history_indices, future_indices, history_edge_orientations=None, future_edge_orientations=None):\n",
    "        # Binary on/off edges\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            pass\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return history_edge_features, future_edge_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    history_indices = torch.stack([item['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item['future_indices'] for item in batch])\n",
    "    history_edge_features = torch.stack([item['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item['future_edge_features'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"history_indices\": history_indices,\n",
    "        \"future_indices\": future_indices,\n",
    "        \"history_edge_features\": history_edge_features,\n",
    "        \"future_edge_features\": future_edge_features,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Diffusion for discrete state spaces.\"\"\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_diffusion(diffusion_config, model_config, num_edges, future_len, device):\n",
    "    \"\"\"HParams -> diffusion object.\"\"\"\n",
    "    return CategoricalDiffusion(\n",
    "        betas=get_diffusion_betas(diffusion_config, device),\n",
    "        model_prediction=model_config['model_prediction'],\n",
    "        model_output=model_config['model_output'],\n",
    "        transition_mat_type=model_config['transition_mat_type'],\n",
    "        transition_bands=model_config['transition_bands'],\n",
    "        loss_type=model_config['loss_type'],\n",
    "        hybrid_coeff=model_config['hybrid_coeff'],\n",
    "        num_edges=num_edges,\n",
    "        model_name=model_config['name'],\n",
    "        future_len=future_len,\n",
    "        device=device\n",
    ")\n",
    "\n",
    "\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        steps = torch.linspace(0, 1, spec['num_timesteps'] + 1, dtype=torch.float64)\n",
    "        alpha_bar = torch.cos((steps + 0.008) / 1.008 * torch.pi / 2)\n",
    "        betas = torch.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], torch.tensor(0.999))\n",
    "        return betas.to(device)\n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])\n",
    "\n",
    "\n",
    "class CategoricalDiffusion:\n",
    "    \"\"\"Discrete state space diffusion process.\n",
    "\n",
    "    Time convention: noisy data is labeled x_0, ..., x_{T-1}, and original data\n",
    "    is labeled x_start (or x_{-1}). This convention differs from the papers,\n",
    "    which use x_1, ..., x_T for noisy data and x_0 for original data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, betas, model_prediction, model_output,\n",
    "               transition_mat_type, transition_bands, loss_type, hybrid_coeff,\n",
    "               num_edges, torch_dtype=torch.float32, model_name=None, future_len=None, device=None):\n",
    "\n",
    "        self.model_prediction = model_prediction  # *x_start*, xprev\n",
    "        self.model_output = model_output  # logits or *logistic_pars*\n",
    "        self.loss_type = loss_type  # kl, *hybrid*, cross_entropy_x_start\n",
    "        self.hybrid_coeff = hybrid_coeff\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "        # Data \\in {0, ..., num_edges-1}\n",
    "        self.num_classes = 2 # 0 or 1\n",
    "        self.num_edges = num_edges\n",
    "        self.future_len = future_len\n",
    "        self.class_weights = torch.tensor([self.future_len / self.num_edges, 1 - self.future_len / self.num_edges], dtype=torch.float64)\n",
    "        self.class_probs = torch.tensor([1 - self.future_len / self.num_edges, self.future_len / self.num_edges], dtype=torch.float64)\n",
    "        self.transition_bands = transition_bands\n",
    "        self.transition_mat_type = transition_mat_type\n",
    "        self.eps = 1.e-6\n",
    "\n",
    "        if not isinstance(betas, torch.Tensor):\n",
    "            raise ValueError('expected betas to be a torch tensor')\n",
    "        if not ((betas > 0).all() and (betas <= 1).all()):\n",
    "            raise ValueError('betas must be in (0, 1]')\n",
    "\n",
    "        # Computations here in float64 for accuracy\n",
    "        self.betas = betas.to(dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        self.num_timesteps, = betas.shape\n",
    "\n",
    "        # Construct transition matrices for q(x_t|x_{t-1})\n",
    "        # NOTE: t goes from {0, ..., T-1}\n",
    "        if self.transition_mat_type == 'uniform':\n",
    "            q_one_step_mats = [self._get_transition_mat(t) \n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'gaussian':\n",
    "            q_one_step_mats = [self._get_gaussian_transition_mat(t)\n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'absorbing':\n",
    "            q_one_step_mats = [self._get_absorbing_transition_mat(t)\n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'marginal_prior':\n",
    "            q_one_step_mats = [self._get_prior_distribution_transition_mat(t)\n",
    "                               for t in range(0, self.num_timesteps)]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"transition_mat_type must be 'gaussian', 'uniform', 'absorbing' \"\n",
    "                f\", but is {self.transition_mat_type}\"\n",
    "                )\n",
    "\n",
    "        self.q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to(self.device, non_blocking=True)\n",
    "        assert self.q_onestep_mats.shape == (self.num_timesteps,\n",
    "                                            self.num_classes,\n",
    "                                            self.num_classes)\n",
    "\n",
    "        # Construct transition matrices for q(x_t|x_start)\n",
    "        q_mat_t = self.q_onestep_mats[0]\n",
    "        q_mats = [q_mat_t]\n",
    "        for t in range(1, self.num_timesteps):\n",
    "            # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "            q_mat_t = torch.tensordot(q_mat_t, self.q_onestep_mats[t],\n",
    "                                    dims=[[1], [0]])\n",
    "            q_mats.append(q_mat_t)\n",
    "        self.q_mats = torch.stack(q_mats, axis=0)\n",
    "        assert self.q_mats.shape == (self.num_timesteps, self.num_classes,\n",
    "                                    self.num_classes), self.q_mats.shape\n",
    "\n",
    "        # Don't precompute transition matrices for q(x_{t-1} | x_t, x_start)\n",
    "        # Can be computed from self.q_mats and self.q_one_step_mats.\n",
    "        # Only need transpose of q_onestep_mats for posterior computation.\n",
    "        self.transpose_q_onestep_mats = torch.transpose(self.q_onestep_mats, dim0=1, dim1=2)\n",
    "        del self.q_onestep_mats\n",
    "\n",
    "    def _get_full_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        Contrary to the band diagonal version, this method constructs a transition\n",
    "        matrix with uniform probability to all other states.\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "        # Create a matrix filled with beta_t/num_classes\n",
    "        mat = torch.full((self.num_classes, self.num_classes), \n",
    "                            fill_value=beta_t / float(self.num_classes),\n",
    "                            dtype=torch.float64)\n",
    "\n",
    "        # Create a diagonal matrix with values to be set on the diagonal of mat\n",
    "        diag_val = 1. - beta_t * (self.num_classes - 1.) / self.num_classes\n",
    "        diag_matrix = torch.diag(torch.full((self.num_classes,), diag_val, dtype=torch.float64))\n",
    "\n",
    "        # Set the diagonal values\n",
    "        mat.fill_diagonal_(diag_val)\n",
    "\n",
    "        return mat\n",
    "\n",
    "    def _get_transition_mat(self, t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition\n",
    "        matrix Q with\n",
    "        Q_{ij} = beta_t / num_classes       if |i-j| <= self.transition_bands\n",
    "                1 - \\sum_{l \\neq i} Q_{il} if i==j.\n",
    "                0                          else.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        if self.transition_bands is None:\n",
    "            return self._get_full_transition_mat(t)\n",
    "        # Assumes num_off_diags < num_classes\n",
    "        beta_t = self.betas[t]\n",
    "        \n",
    "        mat = torch.zeros((self.num_classes, self.num_classes),\n",
    "                        dtype=torch.float64)\n",
    "        off_diag = torch.full((self.num_classes - 1,), fill_value=beta_t / float(self.num_classes), dtype=torch.float64)\n",
    "\n",
    "        for k in range(1, self.transition_bands + 1):\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "            off_diag = off_diag[:-1]\n",
    "\n",
    "        # Add diagonal values such that rows sum to one\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag(diag)\n",
    "        \n",
    "        return mat\n",
    "\n",
    "    def _get_gaussian_transition_mat(self, t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition matrix Q with\n",
    "        decaying entries as a function of how far off diagonal the entry is.\n",
    "        Normalization option 1:\n",
    "        Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= self.transition_bands\n",
    "                    1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                    0                          else.\n",
    "\n",
    "        Normalization option 2:\n",
    "        tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= self.transition_bands\n",
    "                            0                        else.\n",
    "\n",
    "        Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        transition_bands = self.transition_bands if self.transition_bands else self.num_classes - 1\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "\n",
    "        mat = torch.zeros((self.num_classes, self.num_classes),\n",
    "                        dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Make the values correspond to a similar type of gaussian as in the\n",
    "        # gaussian diffusion case for continuous state spaces.\n",
    "        values = torch.linspace(torch.tensor(0.), torch.tensor(self.num_classes-1), self.num_classes, dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        values = values * 2./ (self.num_classes - 1.)\n",
    "        values = values[:transition_bands+1]\n",
    "        values = -values * values / beta_t\n",
    "        \n",
    "        # To reverse the tensor 'values' starting from the second element\n",
    "        reversed_values = values[1:].flip(dims=[0])\n",
    "        # Concatenating the reversed values with the original values\n",
    "        values = torch.cat([reversed_values, values], dim=0)\n",
    "        values = F.softmax(values, dim=0)\n",
    "        values = values[transition_bands:]\n",
    "        \n",
    "        for k in range(1, transition_bands + 1):\n",
    "            off_diag = torch.full((self.num_classes - k,), values[k], dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "\n",
    "        # Add diagonal values such that rows and columns sum to one.\n",
    "        # Technically only the ROWS need to sum to one\n",
    "        # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "        # which is necessary if we want to have a uniform stationary distribution.\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag_embed(diag)\n",
    "\n",
    "        return mat.to(self.device, non_blocking=True)\n",
    "\n",
    "    def _get_absorbing_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        Has an absorbing state for pixelvalues self.num_classes//2.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "\n",
    "        diag = torch.full((self.num_classes,), 1. - beta_t, dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        mat = torch.diag(diag)\n",
    "\n",
    "        # Add beta_t to the num_classes/2-th column for the absorbing state\n",
    "        mat[:, self.num_classes // 2] += beta_t\n",
    "\n",
    "        return mat\n",
    "    \n",
    "    def _get_prior_distribution_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "        Use cosine schedule for these transition matrices.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "        mat = torch.zeros((self.num_classes, self.num_classes), dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                if i != j:\n",
    "                    mat[i, j] = beta_t * self.class_probs[j]\n",
    "                else:\n",
    "                    mat[i, j] = 1 - beta_t + beta_t * self.class_probs[j]\n",
    "        \n",
    "        return mat\n",
    "\n",
    "    def _at(self, a, t, x):\n",
    "        \"\"\"\n",
    "        Extract coefficients at specified timesteps t and conditioning data x in PyTorch.\n",
    "\n",
    "        Args:\n",
    "        a: torch.Tensor: PyTorch tensor of constants indexed by time, dtype should be pre-set.\n",
    "        t: torch.Tensor: PyTorch tensor of time indices, shape = (batch_size,).\n",
    "        x: torch.Tensor: PyTorch tensor of shape (bs, ...) of int32 or int64 type.\n",
    "            (Noisy) data. Should not be of one-hot representation, but have integer\n",
    "            values representing the class values. --> NOT A LOT NEEDS TO CHANGE, MY CLASS VALUES ARE SIMPLY 0 AND 1\n",
    "\n",
    "        Returns:\n",
    "        a[t, x]: torch.Tensor: PyTorch tensor.\n",
    "        \"\"\"\n",
    "        ### Original ###\n",
    "        # x.shape = (bs, height, width, channels)\n",
    "        # t_broadcast_shape = (bs, 1, 1, 1)\n",
    "        # a.shape = (num_timesteps, num_pixel_vals, num_pixel_vals)\n",
    "        # out.shape = (bs, height, width, channels, num_pixel_vals)\n",
    "        # out[i, j, k, l, m] = a[t[i, j, k, l], x[i, j, k, l], m]\n",
    "        \n",
    "        ### New ###\n",
    "        # x.shape = (bs, num_edges, channels=1) \n",
    "        # t_broadcast_shape = (bs, 1, 1)\n",
    "        # a.shape = (num_timesteps, num_classes, num_classes) \n",
    "        # out.shape = (bs, num_edges, channels, num_classes) \n",
    "        \n",
    "        # Convert `a` to the desired dtype if not already\n",
    "        a = a.type(self.torch_dtype)\n",
    "\n",
    "        # Prepare t for broadcasting by adding necessary singleton dimensions\n",
    "        t_broadcast = t.view(-1, *((1,) * (x.ndim - 1))).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Advanced indexing in PyTorch to select elements\n",
    "        return a[t_broadcast, x.long()].to(self.device, non_blocking=True)\n",
    "\n",
    "    def _at_onehot(self, a, t, x):\n",
    "        \"\"\"Extract coefficients at specified timesteps t and conditioning data x.\n",
    "\n",
    "        Args:\n",
    "        a: torch.Tensor: PyTorch tensor of constants indexed by time, dtype should be pre-set.\n",
    "        t: torch.Tensor: PyTorch tensor of time indices, shape = (batch_size,).\n",
    "        x: torch.Tensor: PyTorch tensor of shape (bs, ...) of float32 type.\n",
    "            (Noisy) data. Should be of one-hot-type representation.\n",
    "\n",
    "        Returns:\n",
    "        out: torch.tensor: output of dot(x, a[t], axis=[[-1], [1]]).\n",
    "            shape = (bs, num_edges, channels=1, num_classes)\n",
    "        \"\"\"\n",
    "        a = a.type(self.torch_dtype)\n",
    "        \n",
    "        ### Final ###\n",
    "        # t.shape = (bs)\n",
    "        # x.shape = (bs, num_edges, num_classes)\n",
    "        # a[t].shape = (bs, num_classes, num_classes)\n",
    "        # out.shape = (bs, num_edges, num_classes)\n",
    "\n",
    "        a_t = a[t]\n",
    "        out = torch.einsum('bik,bkj->bij', x, a_t).to(self.device, non_blocking=True)\n",
    "        \n",
    "        return out.to(self.device, non_blocking=True)\n",
    "\n",
    "    def q_probs(self, x_start, t):\n",
    "        \"\"\"Compute probabilities of q(x_t | x_start).\n",
    "\n",
    "        Args:\n",
    "        x_start: torch.tensor: tensor of shape (bs, ...) of int32 or int64 type.\n",
    "            Should not be of one hot representation, but have integer values\n",
    "            representing the class values.\n",
    "        t: torch.tensor: torch tensor of shape (bs,).\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.tensor: shape (bs, x_start.shape[1:],\n",
    "                                                num_classes).\n",
    "        \"\"\"\n",
    "        return self._at(self.q_mats, t, x_start)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        \"\"\"\n",
    "        Sample from q(x_t | x_start) (i.e. add noise to the data) using Gumbel softmax trick.\n",
    "\n",
    "        Args:\n",
    "        x_start: torch.tensor: original clean data, in integer form (not onehot).\n",
    "            shape = (bs, num_edges).\n",
    "        t: torch.tensor: timestep of the diffusion process, shape (bs,).\n",
    "        noise: torch.tensor: uniform noise on [0, 1) used to sample noisy data.\n",
    "            shape should match (*x_start.shape, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        sample: torch.tensor: same shape as x_start. noisy data.\n",
    "        \"\"\"\n",
    "        assert noise.shape == x_start.shape + (self.num_classes,)\n",
    "        logits = torch.log(self.q_probs(x_start, t) + self.eps)\n",
    "\n",
    "        # To avoid numerical issues, clip the noise to a minimum value\n",
    "        noise = torch.clamp(noise, min=torch.finfo(noise.dtype).tiny, max=1.)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise)).to(self.device, non_blocking=True)\n",
    "        return torch.argmax(logits + gumbel_noise, dim=-1)\n",
    "    \n",
    "    def _get_logits_from_logistic_pars(self, loc, log_scale):\n",
    "        \"\"\"\n",
    "        Computes logits for an underlying logistic distribution.\n",
    "\n",
    "        Args:\n",
    "        loc: torch.tensor: location parameter of logistic distribution.\n",
    "        log_scale: torch.tensor: log scale parameter of logistic distribution.\n",
    "\n",
    "        Returns:\n",
    "        logits: torch.tensor: logits corresponding to logistic distribution\n",
    "        \"\"\"\n",
    "        loc = loc.unsqueeze(-1)\n",
    "        log_scale = log_scale.unsqueeze(-1)\n",
    "\n",
    "        # Adjust the scale such that if it's zero, the probabilities have a scale\n",
    "        # that is neither too wide nor too narrow.\n",
    "        inv_scale = torch.exp(- (log_scale - 2.))\n",
    "\n",
    "        bin_width = 2. / (self.num_classes - 1.)\n",
    "        bin_centers = torch.linspace(-1., 1., self.num_classes)\n",
    "\n",
    "        bin_centers = bin_centers.unsqueeze(0)  # Add batch dimension\n",
    "        bin_centers = bin_centers - loc\n",
    "\n",
    "        log_cdf_min = -F.softplus(-inv_scale * (bin_centers - 0.5 * bin_width))\n",
    "        log_cdf_plus = -F.softplus(-inv_scale * (bin_centers + 0.5 * bin_width))\n",
    "\n",
    "        logits = torch.log(torch.exp(log_cdf_plus) - torch.exp(log_cdf_min) + self.eps)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def q_posterior_logits(self, x_start, x_t, t, x_start_logits):\n",
    "        \"\"\"Compute logits of q(x_{t-1} | x_t, x_start) in PyTorch.\"\"\"\n",
    "        \n",
    "        if x_start_logits:\n",
    "            assert x_start.shape == x_t.shape + (self.num_classes,), (x_start.shape, x_t.shape)\n",
    "        else:\n",
    "            assert x_start.shape == x_t.shape, (x_start.shape, x_t.shape)\n",
    "            \n",
    "        fact1 = self._at(self.transpose_q_onestep_mats, t, x_t)\n",
    "        if x_start_logits:\n",
    "            fact2 = self._at_onehot(self.q_mats, t-1, F.softmax(x_start, dim=-1))\n",
    "            tzero_logits = x_start\n",
    "        else:\n",
    "            fact2 = self._at(self.q_mats, t-1, x_start)\n",
    "            tzero_logits = torch.log(F.one_hot(x_start.to(torch.int64), num_classes=self.num_classes) + self.eps)\n",
    "\n",
    "        out = torch.log(fact1 + self.eps) + torch.log(fact2 + self.eps)\n",
    "\n",
    "        t_broadcast = t.unsqueeze(1).unsqueeze(2)  # Adds new dimensions: [batch_size, 1, 1]\n",
    "        t_broadcast = t_broadcast.expand(-1, tzero_logits.size(1), tzero_logits.size(-1)).to(self.device, non_blocking=True)   # tzero_logits.size(1) = num_edges, tzero_logits.size(-1) = num_classes\n",
    "\n",
    "        return torch.where(t_broadcast == 0, tzero_logits, out) # (bs, num_edges, num_classes)\n",
    "\n",
    "    def p_logits(self, model_fn, x, t, edge_features=None, edge_index=None, condition=None):\n",
    "        \"\"\"Compute logits of p(x_{t-1} | x_t) in PyTorch.\n",
    "\n",
    "        Args:\n",
    "            model_fn (function): The model function that takes input `x` and `t` and returns the model output.\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_size) representing the noised input at time t.\n",
    "            t (torch.Tensor): The time tensor of shape (batch_size,) representing the time step.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two tensors:\n",
    "                - model_logits (torch.Tensor): The logits of p(x_{t-1} | x_t) of shape (batch_size, input_size, num_classes).\n",
    "                - pred_x_start_logits (torch.Tensor): The logits of p(x_{t-1} | x_start) of shape (batch_size, input_size, num_classes).\n",
    "        \"\"\"\n",
    "        assert t.shape == (x.shape[0],)\n",
    "        model_output = model_fn(edge_features, edge_index, t, condition=condition)\n",
    "\n",
    "        if self.model_output == 'logits':\n",
    "            model_logits = model_output\n",
    "        elif self.model_output == 'logistic_pars':\n",
    "            loc, log_scale = model_output\n",
    "            model_logits = self._get_logits_from_logistic_pars(loc, log_scale)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_output)\n",
    "\n",
    "        if self.model_prediction == 'x_start':\n",
    "            pred_x_start_logits = model_logits\n",
    "            t_broadcast = t.unsqueeze(1).unsqueeze(2)  # Adds new dimensions: [batch_size, 1, 1]\n",
    "            t_broadcast = t_broadcast.expand(-1, pred_x_start_logits.size(1), pred_x_start_logits.size(-1)).to(self.device, non_blocking=True)   # pred_x_start_logits.size(1) = num_edges, pred_x_start_logits.size(-1) = num_classes\n",
    "            model_logits = torch.where(t_broadcast == 0, pred_x_start_logits,\n",
    "                                       self.q_posterior_logits(x_start=pred_x_start_logits, x_t=x, t=t, x_start_logits=True))\n",
    "            \n",
    "        elif self.model_prediction == 'xprev':\n",
    "            pred_x_start_logits = model_logits\n",
    "            raise NotImplementedError(self.model_prediction)\n",
    "        \n",
    "        assert (model_logits.shape == pred_x_start_logits.shape == x.shape + (self.num_classes,))\n",
    "        return model_logits, pred_x_start_logits    # (bs, num_eedges, 2)\n",
    "    \n",
    "    # === Sampling ===\n",
    "\n",
    "    def p_sample(self, model_fn, x, t, noise, edge_features=None, edge_index=None, condition=None):\n",
    "        \"\"\"Sample one timestep from the model p(x_{t-1} | x_t).\"\"\"\n",
    "        # Get model logits\n",
    "        model_logits, pred_x_start_logits = self.p_logits(model_fn=model_fn, x=x, t=t, edge_features=edge_features, edge_index=edge_index, condition=condition)\n",
    "        assert noise.shape == model_logits.shape, noise.shape\n",
    "\n",
    "        # No noise when t == 0\n",
    "        nonzero_mask = (t != 0).float().reshape(x.shape[0], *([1] * (len(x.shape) - 1)))\n",
    "        # For numerical precision clip the noise to a minimum value\n",
    "        noise = torch.clamp(noise, min=torch.finfo(noise.dtype).eps, max=1.)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise))\n",
    "\n",
    "        sample = torch.argmax(model_logits + nonzero_mask * gumbel_noise, dim=-1)\n",
    "\n",
    "        assert sample.shape == x.shape\n",
    "        assert pred_x_start_logits.shape == model_logits.shape\n",
    "        return sample, F.softmax(pred_x_start_logits, dim=-1)\n",
    "\n",
    "    def p_sample_loop(self, model_fn, shape, num_timesteps=None, return_x_init=False, edge_features=None, edge_index=None, line_graph=None, condition=None):\n",
    "        \"\"\"Ancestral sampling.\"\"\"\n",
    "        if num_timesteps is None:\n",
    "            num_timesteps = self.num_timesteps\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.transition_mat_type in ['gaussian', 'uniform', 'marginal_prior']:\n",
    "            x_init = torch.randint(0, self.num_classes, size=shape, device=device)\n",
    "        elif self.transition_mat_type == 'absorbing':\n",
    "            x_init = torch.full(shape, fill_value=self.num_classes // 2, dtype=torch.int32, device=device)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid transition_mat_type {self.transition_mat_type}\")\n",
    "\n",
    "        x = x_init.clone()  # (bs, num_edges)\n",
    "        edge_attr = x_init.float()\n",
    "        #new_line_graph_x = line_graph.x.clone()\n",
    "        #new_line_graph_x[:, :, 0] = edge_attr\n",
    "        new_edge_features = edge_features.clone()\n",
    "        new_edge_features[:, :, 0] = edge_attr\n",
    "        \n",
    "        for i in range(num_timesteps):\n",
    "            t = torch.full([shape[0]], self.num_timesteps - 1 - i, dtype=torch.long, device=device)\n",
    "            noise = torch.rand(x.shape + (self.num_classes,), device=device, dtype=torch.float32)\n",
    "            x, _ = self.p_sample(model_fn=model_fn, x=x, t=t, noise=noise, edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            new_edge_features[:, :, 0] = x.float()\n",
    "\n",
    "        if return_x_init:\n",
    "            return x_init, x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "  # === Log likelihood / loss calculation ===\n",
    "        \n",
    "    def cross_entropy_x_start(self, x_start, pred_x_start_logits, class_weights):\n",
    "        \"\"\"Calculate binary weighted cross entropy between x_start and predicted x_start logits.\n",
    "\n",
    "        Args:\n",
    "            x_start (torch.Tensor): original clean data, expected binary labels (0 or 1), shape (bs, num_edges)\n",
    "            pred_x_start_logits (torch.Tensor): logits as predicted by the model\n",
    "            class_weights (torch.Tensor): tensor with weights for class 0 and class 1\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: scalar tensor representing the mean binary weighted cross entropy loss.\n",
    "        \"\"\"\n",
    "        # Calculate binary cross-entropy with logits\n",
    "        x_start = x_start.long().to(self.device, non_blocking=True)\n",
    "        pred_x_start_logits = pred_x_start_logits.permute(0, 2, 1).float() # (bs, num_edges, num_classes) -> (bs, num_classes, num_edges)\n",
    "        ce = F.cross_entropy(pred_x_start_logits, x_start, weight=class_weights.float().to(self.device, non_blocking=True), reduction='mean')\n",
    "\n",
    "        return ce\n",
    "\n",
    "    def training_losses(self, model_fn, condition=None, *, x_start, edge_features, edge_index, line_graph=None):\n",
    "        \"\"\"Training loss calculation.\"\"\"\n",
    "        # Add noise to data\n",
    "        noise = torch.rand(x_start.shape + (self.num_classes,), dtype=torch.float32)\n",
    "        t = torch.randint(0, self.num_timesteps, (x_start.shape[0],))\n",
    "\n",
    "        # t starts at zero. so x_0 is the first noisy datapoint, not the datapoint itself.\n",
    "        x_t = self.q_sample(x_start=x_start, t=t, noise=noise)  # (bs, num_edges)\n",
    "        \n",
    "        edge_attr_t = x_t.float()\n",
    "        # new_line_graph_x = line_graph.x.clone()\n",
    "        new_edge_features = edge_features.clone()\n",
    "        for i in range(edge_attr_t.shape[0]):\n",
    "            # new_line_graph_x[i, :, 0] = edge_attr_t[i]  # Update the edge attributes in the line graph with the noised trajectory x_t\n",
    "            new_edge_features[i, :, 0] = edge_attr_t[i]\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        if self.loss_type == 'kl':\n",
    "            losses, pred_x_start_logits = self.vb_terms_bpd(model_fn=model_fn, x_start=x_start, x_t=x_t, t=t,\n",
    "                                                               edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            \n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(2)    # (bs, num_edges, channels, classes) -> (bs, num_edges, classes)\n",
    "            # NOTE: Currently only works for batch size of 1\n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(0)    # (bs, num_edges, classes) -> (num_edges, classes)\n",
    "            pred = pred_x_start_logits.argmax(dim=1)                # (num_edges, classes) -> (num_edges,)\n",
    "            \n",
    "            return losses, pred\n",
    "            \n",
    "        elif self.loss_type == 'cross_entropy_x_start':\n",
    "            \n",
    "            _, pred_x_start_logits = self.p_logits(model_fn, x=x_t, t=t, edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            losses = self.cross_entropy_x_start(x_start=x_start, pred_x_start_logits=pred_x_start_logits, class_weights=self.class_weights)\n",
    "            \n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(2)    # (bs, num_edges, channels, classes) -> (bs, num_edges, classes)\n",
    "            \n",
    "            if (self.model_name == 'edge_encoder') | (self.model_name == 'edge_encoder_residual'):\n",
    "                # NOTE: Currently only works for batch size of 1\n",
    "                pred_x_start_logits = pred_x_start_logits.squeeze(0)    # (bs, num_edges, classes) -> (num_edges, classes)\n",
    "                pred = pred_x_start_logits.argmax(dim=1)    # (num_edges, classes) -> (num_edges,)\n",
    "            elif self.model_name == 'edge_encoder_mlp':\n",
    "                pred = pred_x_start_logits.argmax(dim=2)\n",
    "            \n",
    "            return losses, pred\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "from torch_geometric.utils import from_networkx\n",
    "#from dataset.trajctory_dataset import TrajectoryDataset, collate_fn\n",
    "#from .d3pm_diffusion import make_diffusion\n",
    "#from .d3pm_edge_encoder import Edge_Encoder\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "class Graph_Diffusion_Model(nn.Module):\n",
    "    def __init__(self, data_config, diffusion_config, model_config, train_config, test_config, wandb_config, model):\n",
    "        super(Graph_Diffusion_Model, self).__init__()\n",
    "        \n",
    "        # Data\n",
    "        self.data_config = data_config\n",
    "        self.train_data_path = self.data_config['train_data_path']\n",
    "        self.val_data_path = self.data_config['val_data_path']\n",
    "        self.history_len = self.data_config['history_len']\n",
    "        self.future_len = self.data_config['future_len']\n",
    "        self.num_classes = self.data_config['num_classes']\n",
    "        self.edge_features = self.data_config['edge_features']\n",
    "        \n",
    "        # Diffusion\n",
    "        self.diffusion_config = diffusion_config\n",
    "        self.num_timesteps = self.diffusion_config['num_timesteps']\n",
    "        \n",
    "        # Model\n",
    "        self.model_config = model_config\n",
    "        self.model = model # Edge_Encoder\n",
    "        self.hidden_channels = self.model_config['hidden_channels']\n",
    "        self.time_embedding_dim = self.model_config['time_embedding_dim']\n",
    "        self.condition_dim = self.model_config['condition_dim']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        \n",
    "        # Training\n",
    "        self.train_config = train_config\n",
    "        self.lr = self.train_config['lr']\n",
    "        self.lr_decay_parameter = self.train_config['lr_decay']\n",
    "        self.learning_rate_warmup_steps = self.train_config['learning_rate_warmup_steps']\n",
    "        self.num_epochs = self.train_config['num_epochs']\n",
    "        self.gradient_accumulation = self.train_config['gradient_accumulation']\n",
    "        self.gradient_accumulation_steps = self.train_config['gradient_accumulation_steps']\n",
    "        self.batch_size = self.train_config['batch_size'] if not self.gradient_accumulation else self.train_config['batch_size'] * self.gradient_accumulation_steps\n",
    "        \n",
    "        # Testing\n",
    "        self.test_config = test_config\n",
    "        self.test_batch_size = self.test_config['batch_size']\n",
    "        self.model_path = self.test_config['model_path']\n",
    "        self.eval_every_steps = self.test_config['eval_every_steps']\n",
    "        \n",
    "        # WandB\n",
    "        self.wandb_config = wandb_config\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            project=self.wandb_config['project'],\n",
    "            entity=self.wandb_config['entity'],\n",
    "            notes=self.wandb_config['notes'],\n",
    "            job_type=self.wandb_config['job_type'],\n",
    "            config={**self.data_config, **self.diffusion_config, **self.model_config, **self.train_config}\n",
    "        )\n",
    "        self.exp_name = self.wandb_config['exp_name']\n",
    "        wandb.run.name = self.exp_name\n",
    "\n",
    "        # Logging\n",
    "        self.dataset = self.data_config['dataset']\n",
    "        self.model_dir = os.path.join(\"experiments\", self.exp_name)\n",
    "        os.makedirs(self.model_dir,exist_ok=True)\n",
    "        log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        log_name = f\"{self.dataset}_{log_name}\"\n",
    "        \n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        log_dir = os.path.join(self.model_dir, log_name)\n",
    "        file_handler = logging.FileHandler(log_dir)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.log.addHandler(file_handler)\n",
    "        \n",
    "        self.log_loss_every_steps = self.train_config['log_loss_every_steps']        \n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Build Components\n",
    "        self._build_train_dataloader()\n",
    "        self._build_test_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        \n",
    "        # Move model to GPU\n",
    "        \n",
    "        self.model.to(self.device, non_blocking=True)\n",
    "        print(\"device\", self.device)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the diffusion-based trajectory prediction model.\n",
    "\n",
    "        This function performs the training of the diffusion-based trajectory prediction model. It iterates over the specified number of epochs and updates the model's parameters based on the training data. The training process includes forward propagation, loss calculation, gradient computation, and parameter updates.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        dif = make_diffusion(self.diffusion_config, self.model_config, num_edges=self.num_edges, future_len=self.future_len, device=self.device)\n",
    "        def model_fn(x, edge_index, t, condition=None):\n",
    "            if self.model_config['name'] == 'edge_encoder':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                return self.model.forward(x, t=t, condition=condition, mode='future')\n",
    "                \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            wandb.log({\"epoch\": epoch, \"learning_rate\": current_lr})\n",
    "            \n",
    "            total_loss = 0\n",
    "            ground_truth_fut = []\n",
    "            pred_fut = []\n",
    "            #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                #with record_function(\"model_training\"):\n",
    "            if self.gradient_accumulation:\n",
    "                for data in self.train_data_loader:\n",
    "                    history_edge_features = data[\"history_edge_features\"]\n",
    "                    future_edge_indices_one_hot = data[\"future_edge_features\"][:, :, 0]\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    for i in range(min(self.gradient_accumulation_steps, history_edge_features.size(0))):\n",
    "                        # Calculate history condition c\n",
    "                        \n",
    "                        if self.model_config['name'] == 'edge_encoder':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, mode='history')\n",
    "                        elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, mode='history')\n",
    "                        elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), mode='history')\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.model_config['name'])\n",
    "                        \n",
    "                        x_start = future_edge_indices_one_hot[i].unsqueeze(0)   # (1, num_edges)\n",
    "                        # Get loss and predictions\n",
    "                        loss, preds = dif.training_losses(model_fn, c, x_start=x_start, edge_features=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, line_graph=None)   # preds are of shape (num_edges,)\n",
    "                        \n",
    "                        total_loss += loss / self.gradient_accumulation_steps\n",
    "                        (loss / self.gradient_accumulation_steps).backward() # Gradient accumulation\n",
    "                        \n",
    "                        ground_truth_fut.append(x_start.detach())\n",
    "                        pred_fut.append(preds.detach())\n",
    "                        \n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "            else:\n",
    "                for data in self.train_data_loader:\n",
    "                    history_edge_features = data[\"history_edge_features\"]\n",
    "                    future_edge_indices_one_hot = data[\"future_edge_features\"][:, :, 0]\n",
    "                    \n",
    "                    batch_size = future_edge_indices_one_hot.size(0)\n",
    "                    if self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                        if batch_size == self.batch_size:\n",
    "                            future_edge_indices_one_hot = future_edge_indices_one_hot.view(self.batch_size, self.num_edges)\n",
    "                        else:\n",
    "                            future_edge_indices_one_hot = future_edge_indices_one_hot.view(batch_size, self.num_edges)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Calculate history condition c\n",
    "                    if self.model_config['name'] == 'edge_encoder':\n",
    "                        c = self.model.forward(x=history_edge_features, edge_index=self.edge_index, mode='history')\n",
    "                    elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                        c = self.model.forward(x=history_edge_features, edge_index=self.edge_index, mode='history')\n",
    "                    elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                        c = self.model.forward(x=history_edge_features, mode='history')\n",
    "                    else:\n",
    "                        raise NotImplementedError(self.model_config['name'])\n",
    "                    \n",
    "                    x_start = future_edge_indices_one_hot\n",
    "                    # Get loss and predictions\n",
    "                    loss, preds = dif.training_losses(model_fn, c, x_start=x_start, edge_features=history_edge_features, edge_index=self.edge_index, line_graph=None)\n",
    "                                        \n",
    "                    total_loss += loss\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    ground_truth_fut.append(x_start.detach())\n",
    "                    pred_fut.append(preds.detach())\n",
    "            \n",
    "            self.scheduler.step()\n",
    "                    \n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach().to('cpu'), torch.flatten(torch.cat(ground_truth_fut)).detach().to('cpu'))\n",
    "            if epoch % self.log_loss_every_steps == 0:\n",
    "                wandb.log({\"epoch\": epoch, \"average_loss\": avg_loss.item()})\n",
    "                wandb.log({\"epoch\": epoch, \"average_F1_score\": f1_epoch.item()})\n",
    "                self.log.info(f\"Epoch {epoch} Average Loss: {avg_loss.item()}\")\n",
    "                print(\"Epoch:\", epoch+1)\n",
    "                print(\"Loss:\", avg_loss.item())\n",
    "                print(\"F1:\", f1_epoch.item())\n",
    "                \n",
    "            if (epoch + 1) % self.eval_every_steps == 0:\n",
    "                print(\"Evaluating on test set...\")\n",
    "                sample_list, ground_truth_hist, ground_truth_fut = self.get_samples(task='predict')\n",
    "                fut_ratio, f1, avg_sample_length = self.eval(sample_list, ground_truth_hist, ground_truth_fut)\n",
    "                print(\"Samples\", sample_list)\n",
    "                print(\"Ground truth\", ground_truth_fut)\n",
    "                print(\"Test F1 Score\", f1.item())\n",
    "                wandb.log({\"Test F1 Score\": f1.item()})\n",
    "                wandb.log({\"Test Future ratio\": fut_ratio})\n",
    "                wandb.log({\"Average test sample length\": avg_sample_length})\n",
    "                        \n",
    "            if self.train_config['save_model'] and (epoch + 1) % self.train_config['save_model_every_steps'] == 0:\n",
    "                self.save_model()\n",
    "            \n",
    "    def get_samples(self, load_model=False, model_path=None, task='predict', number_samples=1, save=False):\n",
    "        \"\"\"\n",
    "        Retrieves samples from the model.\n",
    "\n",
    "        Args:\n",
    "            load_model (bool, optional): Whether to load a pre-trained model. Defaults to False.\n",
    "            model_path (str, optional): The path to the pre-trained model. Required if `load_model` is True.\n",
    "            task (str, optional): The task to perform. Defaults to 'predict'. Other possible value: 'generate' to generate realistic trajectories\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing three lists:\n",
    "                - sample_list (list): A list of samples generated by the model.\n",
    "                - ground_truth_hist (list): A list of ground truth history edge indices.\n",
    "                - ground_truth_fut (list): A list of ground truth future trajectory indices.\n",
    "        \"\"\"\n",
    "        \n",
    "        if load_model:\n",
    "            if model_path is None:\n",
    "                raise ValueError(\"Model path must be provided to load model.\")\n",
    "            self.load_model(model_path)\n",
    "        \n",
    "        if self.test_config['number_samples'] is not None:\n",
    "            number_samples = self.test_config['number_samples']\n",
    "        \n",
    "        def model_fn(x, edge_index, t, condition=None):\n",
    "            if self.model_config['name'] == 'edge_encoder':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                return self.model.forward(x=x, t=t, condition=condition, mode='future')\n",
    "        \n",
    "        sample_list = []\n",
    "        ground_truth_hist = []\n",
    "        ground_truth_fut = []\n",
    "        \n",
    "        if task == 'predict':\n",
    "            for data in tqdm(self.test_dataloader):\n",
    "                history_edge_features = data[\"history_edge_features\"]\n",
    "\n",
    "                history_edge_indices = data[\"history_indices\"]\n",
    "\n",
    "                future_trajectory_indices = data[\"future_indices\"]\n",
    "                # with torch.no_grad():\n",
    "                if self.model_config['name'] == 'edge_encoder':\n",
    "                    c = self.model.forward(x=self.line_graph.x, edge_index=self.line_graph.edge_index, mode='history')\n",
    "                elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                    c = self.model.forward(x=self.line_graph.x, edge_index=self.line_graph.edge_index, mode='history')\n",
    "                elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                    c = self.model.forward(x=history_edge_features, mode='history')\n",
    "            \n",
    "                if number_samples > 1:\n",
    "                    new_seed = torch.seed() + torch.randint(0, 100000, (1,)).item()\n",
    "                    torch.manual_seed(new_seed)\n",
    "                    sample_sublist = []\n",
    "                    for _ in range(number_samples):\n",
    "                        samples = make_diffusion(self.diffusion_config, self.model_config, \n",
    "                                                num_edges=self.num_edges, future_len=self.future_len).p_sample_loop(model_fn=model_fn,\n",
    "                                                                                        shape=(self.test_batch_size, self.num_edges),\n",
    "                                                                                        edge_features=history_edge_features,\n",
    "                                                                                        edge_index=self.edge_index,\n",
    "                                                                                        line_graph=None,\n",
    "                                                                                        condition=c)\n",
    "                        samples = torch.where(samples == 1)[1]\n",
    "                        sample_sublist.append(samples.detach())\n",
    "                    sample_list.append(sample_sublist)\n",
    "                elif number_samples == 1:\n",
    "                    samples = make_diffusion(self.diffusion_config, self.model_config, \n",
    "                                            num_edges=self.num_edges, future_len=self.future_len, device=self.device).p_sample_loop(model_fn=model_fn,\n",
    "                                                                                    shape=(self.test_batch_size, self.num_edges), \n",
    "                                                                                    edge_features=history_edge_features,\n",
    "                                                                                    edge_index=self.edge_index,\n",
    "                                                                                    line_graph=None,\n",
    "                                                                                    condition=c)\n",
    "                    samples = torch.where(samples == 1)[1]\n",
    "                    sample_list.append(samples.detach())\n",
    "                else:\n",
    "                    raise ValueError(\"Number of samples must be greater than 0.\")\n",
    "                ground_truth_hist.append(history_edge_indices.detach())\n",
    "                ground_truth_fut.append(future_trajectory_indices.detach())\n",
    "            \n",
    "            if number_samples == 1:\n",
    "                fut_ratio, f1, avg_sample_length = self.eval(sample_list, ground_truth_hist, ground_truth_fut)\n",
    "                wandb.log({\"F1 Score\": f1.item()})\n",
    "                wandb.log({\"Future ratio\": fut_ratio})\n",
    "                wandb.log({\"Average sample length\": avg_sample_length})\n",
    "            \n",
    "            if save:\n",
    "                torch.save(sample_list, os.path.join(self.model_dir, f'{self.exp_name}_samples.pth'))\n",
    "                torch.save(ground_truth_hist, os.path.join(self.model_dir, f'{self.exp_name}_ground_truth_hist.pth'))\n",
    "                torch.save(ground_truth_fut, os.path.join(self.model_dir, f'{self.exp_name}_ground_truth_fut.pth'))\n",
    "                print(f\"Samples saved at {os.path.join(self.model_dir, f'{self.exp_name}_samples.pth')}!\")\n",
    "            else:\n",
    "                return sample_list, ground_truth_hist, ground_truth_fut\n",
    "        \n",
    "        elif task == 'generate':\n",
    "            # Generate realistic trajectories without condition\n",
    "            # Edge encoder model needs to be able to funciton with no edge_attr and no condition\n",
    "            # Add generate mode to p_logits, p_sample, and p_sample_loop\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(task)\n",
    "    \n",
    "    def visualize_sample_density(self, samples, ground_truth_hist, ground_truth_fut, number_plots=5, number_samples=10):\n",
    "        \"\"\"\n",
    "        Visualize the density of the samples generated by the model.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        samples, ground_truth_hist, ground_truth_fut = self.get_samples(load_model=True, model_path=self.test_config['model_path'], number_samples=number_samples)\n",
    "        save_dir = f'{os.path.join(self.model_dir, f'{self.exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.nodes)\n",
    "        all_edges = {tuple(self.edges[idx]) for idx in range(len(self.edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "\n",
    "        pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "        for i in range(min(number_plots, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i][0]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i][0]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'\n",
    "                if plot_num == 2:\n",
    "                    edge_counts = np.zeros(len(all_edges))\n",
    "                    for sample in samples[i]:\n",
    "                        for edge in sample:\n",
    "                            edge_counts[edge] += 1\n",
    "                    max_count = np.max(edge_counts)\n",
    "                    edge_widths = edge_counts / max_count\n",
    "                    \n",
    "                    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                    nx.draw_networkx_edges(G, pos, edgelist=all_edges, edge_color='red', width=edge_widths*5, alpha=edge_widths/np.max(edge_widths))\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "                else:\n",
    "                    subgraph_edges = {tuple(self.edges[idx]) for idx in edge_indices if idx < len(self.edges)}\n",
    "                    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                    nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            # Save plot\n",
    "            plt.savefig(os.path.join(save_dir, f'sample_{i+1}.png'))\n",
    "            plt.close()  # Close the figure to free memory\n",
    "    \n",
    "    def visualize_predictions(self, samples, ground_truth_hist, ground_truth_fut, number_plots=5):\n",
    "        \"\"\"\n",
    "        Visualize the predictions of the model along with ground truth data.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        :param number_plots: Number of samples to visualize.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        save_dir = f'{os.path.join(self.model_dir, f'{self.exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.nodes)\n",
    "        all_edges = {tuple(self.edges[idx]) for idx in range(len(self.edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "        \n",
    "        pos = nx.get_node_attributes(G, 'pos')  # Retrieve node positions stored in node attributes\n",
    "\n",
    "        for i in range(min(number_plots, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i][0]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i][0]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "                subgraph_edges = {tuple(self.edges[idx]) for idx in edge_indices if idx < len(self.edges)}\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'# if plot_num == 0 else 'lightgreen' if plot_num == 1 else 'orange'\n",
    "                nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            # Save plot\n",
    "            plt.savefig(os.path.join(save_dir, f'sample_{i+1}.png'))\n",
    "            plt.close()  # Close the figure to free memory\n",
    "    \n",
    "    def eval(self, sample_list, ground_truth_hist, ground_truth_fut):\n",
    "        \"\"\"\n",
    "        Evaluate the model's performance.\n",
    "\n",
    "        :param sample_list: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        \"\"\"\n",
    "        def calculate_fut_ratio(sample_list, ground_truth_fut):\n",
    "            \"\"\"\n",
    "            Calculates the ratio of samples in `sample_list` that have at least one or two edges in common with the ground truth future trajectory.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "                ground_truth_fut (list): A list of ground truth future trajectories.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple containing the ratios of samples that have at least one or two edges in common with the ground truth future trajectory.\n",
    "            \"\"\"\n",
    "            count_1 = 0\n",
    "            count_2 = 0\n",
    "            total = len(sample_list)\n",
    "\n",
    "            for i, sample in enumerate(sample_list):\n",
    "                edges_count = sum(1 for edge in ground_truth_fut[i][0] if edge in sample)\n",
    "                if edges_count >= 1:\n",
    "                    count_1 += 1\n",
    "                if edges_count >= 2:\n",
    "                    count_2 += 1\n",
    "\n",
    "            ratio_1 = count_1 / total\n",
    "            ratio_2 = count_2 / total\n",
    "            return ratio_1, ratio_2\n",
    "        \n",
    "        def calculate_sample_f1(sample_list, ground_truth_fut):\n",
    "            \"\"\"\n",
    "            Calculates the F1 score for a given list of samples and ground truth futures.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "                ground_truth_fut (list): A list of ground truth futures.\n",
    "\n",
    "            Returns:\n",
    "                float: The F1 score.\n",
    "\n",
    "            \"\"\"\n",
    "            one_hot_samples = [torch.zeros(self.num_edges) for _ in range(len(sample_list))]\n",
    "            one_hot_futures = [torch.zeros(self.num_edges) for _ in range(len(ground_truth_fut))]\n",
    "            for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "                for edge_index, edge in enumerate(self.edges):\n",
    "                    if edge_index in sample_list[i]:\n",
    "                        one_hot_sample[edge_index] = 1\n",
    "            for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "                for edge_index, edge in enumerate(self.edges):\n",
    "                    if edge_index in ground_truth_fut[i]:\n",
    "                        one_hot_fut[edge_index] = 1\n",
    "            metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "\n",
    "            return f1\n",
    "        \n",
    "        def calculate_avg_sample_length(sample_list):\n",
    "            \"\"\"\n",
    "            Calculate the average sample length.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "\n",
    "            Returns:\n",
    "                float: The average sample length.\n",
    "            \"\"\"\n",
    "            return sum(len(sample) for sample in sample_list) / len(sample_list)\n",
    "        \n",
    "        fut_ratio = calculate_fut_ratio(sample_list, ground_truth_fut)\n",
    "        f1 = calculate_sample_f1(sample_list, ground_truth_fut)\n",
    "        avg_sample_length = calculate_avg_sample_length(sample_list)\n",
    "        \n",
    "        return fut_ratio, f1, avg_sample_length\n",
    "    \n",
    "    def save_model(self):\n",
    "        save_path = os.path.join(self.model_dir, \n",
    "                                 self.exp_name + '_' + self.model_config['name'] + '_' +  self.model_config['transition_mat_type'] + '_' +  self.diffusion_config['type'] + \n",
    "                                 f'_hidden_dim_{self.hidden_channels}_time_dim_{str(self.time_embedding_dim)}_condition_dim_{self.condition_dim}_layers_{self.num_layers}.pth')\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        self.log.info(f\"Model saved at {save_path}!\")\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.log.info(\"Model loaded!\")\n",
    "    \n",
    "    def _build_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.learning_rate_warmup_steps:\n",
    "                return 1.0\n",
    "            else:\n",
    "                decay_lr = self.lr_decay_parameter ** (epoch - self.learning_rate_warmup_steps)\n",
    "                return max(decay_lr, 2e-5 / self.lr)\n",
    "            \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "        print(\"> Optimizer and Scheduler built!\")\n",
    "        \n",
    "        \"\"\"print(\"Parameters to optimize:\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\"\"\"\n",
    "        \n",
    "    def _build_train_dataloader(self):\n",
    "        print(\"Loading Training Dataset...\")\n",
    "        self.train_dataset = TrajectoryDataset(self.train_data_path, self.history_len, self.future_len, self.edge_features, device=self.device)\n",
    "        self.G = self.train_dataset.build_graph()\n",
    "        self.nodes = self.G.nodes\n",
    "        self.edges = self.G.edges(data=True)\n",
    "        self.num_edges = self.G.number_of_edges()\n",
    "        self.indexed_edges = self.train_dataset.edges\n",
    "        self.num_edge_features = self.train_dataset.num_edge_features\n",
    "        \n",
    "        # Build the line graph and corresponding edge index\n",
    "        self.edge_index = self._build_edge_index()\n",
    "                \n",
    "        self.train_data_loader = DataLoader(self.train_dataset, \n",
    "                                            batch_size=self.batch_size, \n",
    "                                            shuffle=True, \n",
    "                                            collate_fn=collate_fn, \n",
    "                                            num_workers=0,\n",
    "                                            pin_memory=False)\n",
    "                        \n",
    "        print(\"> Training Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.edges], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.edges}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        return edge_index.to(self.device, non_blocking=True)\n",
    "    \n",
    "    def _build_test_dataloader(self):\n",
    "        self.test_dataset = TrajectoryDataset(self.val_data_path, self.history_len, self.future_len, self.edge_features, device=self.device)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        print(\"> Test Dataset loaded!\")\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.model = self.model(self.model_config, self.history_len, self.future_len, self.num_classes,\n",
    "                                num_edges=self.num_edges, hidden_channels=self.hidden_channels, num_edge_features=self.num_edge_features, num_timesteps=self.num_timesteps)\n",
    "        print(\"> Model built!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_time=1000., device=None):\n",
    "    \"\"\"\n",
    "    Build sinusoidal embeddings (from Fairseq).\n",
    "\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "\n",
    "    Args:\n",
    "        timesteps: torch.Tensor: generate embedding vectors at these timesteps\n",
    "        embedding_dim: int: dimension of the embeddings to generate\n",
    "        max_time: float: largest time input\n",
    "\n",
    "    Returns:\n",
    "        embedding vectors with shape `(len(timesteps), embedding_dim)`\n",
    "    \"\"\"\n",
    "    timesteps = timesteps.to(device)\n",
    "    assert timesteps.dim() == 1  # Ensure timesteps is a 1D tensor\n",
    "\n",
    "    # Scale timesteps by the maximum time\n",
    "    timesteps = timesteps.float() * (1000. / max_time)\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = torch.log(torch.tensor(10000.0, device=device)) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=device) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "\n",
    "    if embedding_dim % 2 == 1:  # Add zero-padding if embedding dimension is odd\n",
    "        zero_pad = torch.zeros((timesteps.shape[0], 1), dtype=torch.float32)\n",
    "        emb = torch.cat([emb, zero_pad], dim=1)\n",
    "\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb.to(device)\n",
    "\n",
    "class Edge_Encoder_MLP(nn.Module):\n",
    "    def __init__(self, model_config, history_len, future_len, num_classes, num_edges, hidden_channels, num_edge_features, num_timesteps):\n",
    "        super(Edge_Encoder_MLP, self).__init__()\n",
    "        # Config\n",
    "        self.config = model_config\n",
    "        \n",
    "        # Data\n",
    "        self.num_edges = num_edges\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Time embedding\n",
    "        self.max_time = num_timesteps\n",
    "        self.time_embedding_dim = self.config['time_embedding_dim']\n",
    "        self.time_linear0 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "        self.time_linear1 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "    \n",
    "        # Model\n",
    "        # GNN layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = self.config['num_layers']\n",
    "        self.lin_layers = nn.ModuleList()\n",
    "        self.lin_layers.append(nn.Linear(self.num_edge_features, self.hidden_channels))\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.lin_layers.append(nn.Linear(self.hidden_channels, self.hidden_channels))\n",
    "        \n",
    "        # Output layers for each task\n",
    "        self.condition_dim = self.config['condition_dim']\n",
    "        self.history_encoder = nn.Linear(self.hidden_channels, self.condition_dim)  # To encode history to c\n",
    "        self.future_decoder = nn.Linear(self.hidden_channels + self.condition_dim + self.time_embedding_dim,\n",
    "                                        self.hidden_channels)  # To predict future edges\n",
    "        self.adjust_to_class_shape = nn.Linear(self.hidden_channels, self.num_classes)\n",
    "\n",
    "    def forward(self, x, t=None, condition=None, mode=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        Args:\n",
    "            x: torch.Tensor: input tensor: noised future trajectory indices / history trajectory indices\n",
    "            t: torch.Tensor: timestep tensor\n",
    "        \"\"\"    \n",
    "        \n",
    "        # GNN forward pass\n",
    "        \n",
    "        # Edge Embedding        \n",
    "        for layer in self.lin_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        if mode == 'history':\n",
    "            c = self.history_encoder(x) # (bs, num_edges, condition_dim)\n",
    "            return c\n",
    "        \n",
    "        elif mode == 'future':\n",
    "            # Time embedding\n",
    "            t_emb = get_timestep_embedding(t, embedding_dim=self.time_embedding_dim, max_time=self.max_time, device=x.device)\n",
    "            t_emb = self.time_linear0(t_emb)\n",
    "            t_emb = F.silu(t_emb)  # SiLU activation, equivalent to Swish\n",
    "            t_emb = self.time_linear1(t_emb)\n",
    "            t_emb = F.silu(t_emb)   # (bs, time_embedding_dim)\n",
    "            t_emb = t_emb.unsqueeze(1).repeat(1, x.size(1), 1) # (bs, num_edges, time_embedding_dim)\n",
    "            \n",
    "            #Concatenation\n",
    "            x = torch.cat((x, t_emb), dim=2) # Concatenate with time embedding\n",
    "            x = torch.cat((x, condition), dim=2) # Concatenate with condition c, (bs, num_edges, hidden_channels + condition_dim + time_embedding_dim)\n",
    "            \n",
    "            logits = self.future_decoder(x) # (bs, num_edges, hidden_channels)\n",
    "            logits = self.adjust_to_class_shape(logits) # (bs, num_edges, num_classes=2)\n",
    "\n",
    "            return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_samples.pth\", map_location=torch.device('cpu'))\n",
    "hist = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_ground_truth_hist.pth\", map_location=torch.device('cpu'))\n",
    "fut = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_ground_truth_fut.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_model = Edge_Encoder\n",
    "encoder_model = Edge_Encoder_MLP\n",
    "#encoder_model = Edge_Encoder_Residual\n",
    "\n",
    "\n",
    "    \n",
    "data_config = {\"dataset\": \"synthetic_20_traj\",\n",
    "    \"train_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_train.h5',\n",
    "    \"val_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_val.h5',\n",
    "    \"history_len\": 5,\n",
    "    \"future_len\": 2,\n",
    "    \"num_classes\": 2,\n",
    "    \"edge_features\": ['one_hot_edges', 'coordinates']\n",
    "    }\n",
    "\n",
    "diffusion_config = {\"type\": 'linear', # Options: 'linear', 'cosine', 'jsd'\n",
    "    \"start\": 0.9,  # 1e-4 gauss, 0.02 uniform\n",
    "    \"stop\": 1.0,  # 0.02 gauss, 1. uniform\n",
    "    \"num_timesteps\": 200}\n",
    "\n",
    "model_config = {\"name\": \"edge_encoder_mlp\",\n",
    "    \"hidden_channels\": 32,\n",
    "    \"time_embedding_dim\": 16,\n",
    "    \"condition_dim\": 16,\n",
    "    \"out_ch\": 1,\n",
    "    \"num_heads\": 1,\n",
    "    \"num_layers\": 2,\n",
    "    \"theta\": 1.0, # controls strength of conv layers in residual model\n",
    "    \"dropout\": 0.1,\n",
    "    \"model_output\": \"logits\",\n",
    "    \"model_prediction\": \"x_start\",  # Options: 'x_start','xprev'\n",
    "    \"transition_mat_type\": 'marginal_prior',  # Options: 'gaussian','uniform','absorbing', 'marginal_prior'\n",
    "    \"transition_bands\": 1,\n",
    "    \"loss_type\": \"cross_entropy_x_start\",  # Options: kl, cross_entropy_x_start, hybrid\n",
    "    \"hybrid_coeff\": 0.001,  # Only used for hybrid loss type.\n",
    "    \"class_weights\": [0.05, 0.95] # = future_len/num_edges and (num_edges - future_len)/num_edges\n",
    "    }\n",
    "\n",
    "train_config = {\"batch_size\": 32,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"gradient_accumulation\": False,\n",
    "    \"gradient_accumulation_steps\": 128,\n",
    "    \"num_epochs\": 500,\n",
    "    \"learning_rate_warmup_steps\": 2000, # previously 10000\n",
    "    \"lr_decay\": 0.9999, # previously 0.9999\n",
    "    \"log_loss_every_steps\": 1,\n",
    "    \"save_model\": True,\n",
    "    \"save_model_every_steps\": 50}\n",
    "\n",
    "test_config = {\"batch_size\": 1, # currently only 1 works\n",
    "    \"model_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/synthetic_d3pm_residual_fixed/synthetic_d3pm_residual_fixed_hidden_dim_32_time_dim_16_condition_dim_16_layers_2_weights_0.1.pth',\n",
    "    \"number_samples\": 1,\n",
    "    \"eval_every_steps\": 1000\n",
    "  }\n",
    "\n",
    "wandb_config = {\"exp_name\": \"tdrive_d3pm_test\",\n",
    "    \"project\": \"trajectory_prediction_using_denoising_diffusion_models\",\n",
    "    \"entity\": \"joeschmit99\",\n",
    "    \"job_type\": \"eval\",\n",
    "    \"notes\": \"\",\n",
    "    \"tags\": [\"tdrive\", \"edge_encoder\"]} \n",
    "\n",
    "model = Graph_Diffusion_Model(data_config, diffusion_config, model_config, train_config, test_config, wandb_config, model=encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fut_ratio, f1, avg_sample_length\n",
    "model.eval(sample_list=samples, ground_truth_hist=hist, ground_truth_fut=fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MyData(Data):\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        # Specify that 'future_edge_features' should not be concatenated across the zeroth dimension\n",
    "        if (key == 'y') or (key == 'history_indices') or (key == 'future_indices'):\n",
    "            return None  # This will add a new batch dimension during batching\n",
    "        else:\n",
    "            return super().__cat_dim__(key, value, *args, **kwargs)  # Default behaviour\n",
    "\n",
    "class TrajectoryGeoDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None, device=None, embedding_dim=None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        '''self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features += 4\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features += 1'''\n",
    "        '''if 'pos_encoding' in self.edge_features:\n",
    "            self.num_edge_features += self.embedding_dim'''\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = self.load_new_format(self.file_path, self.device)\n",
    "        \n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n",
    "        '''self.positional_encoding = self.generate_positional_encodings().float()'''\n",
    "        self.edge_index = self._build_edge_index()\n",
    "        # self.num_edges = self.edge_index.size(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_new_format(file_path, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            # Normalize the coordinates to (0, 1) if any of the coordinates is larger than 1\n",
    "            if node_coordinates.max() > 1:\n",
    "                max_values = node_coordinates.max(0)[0]\n",
    "                min_values = node_coordinates.min(0)[0]\n",
    "                node_coordinates[:, 0] = (node_coordinates[:, 0] - min_values[0]) / (max_values[0] - min_values[0])\n",
    "                node_coordinates[:, 1] = (node_coordinates[:, 1] - min_values[1]) / (max_values[1] - min_values[1])\n",
    "            #edges = torch.tensor(new_hf['graph']['edges'][:], dtype=torch.long, device=device)\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            #edges = [(torch.tensor(edge[0], device=device), torch.tensor(edge[1], device=device)) for edge in edges]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            \n",
    "        return paths, nodes, edges, edge_coordinates\n",
    "    \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        self.G = self.build_graph()\n",
    "        self.num_edges = self.G.number_of_edges()\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.G.edges(data=True)], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.G.edges(data=True)}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        return edge_index.to(self.device, non_blocking=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = trajectory['edge_idxs']\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = trajectory['edge_orientations']\n",
    "        \n",
    "        padding_length = max(self.history_len + self.future_len - len(edge_idxs), 0)\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "\n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "\n",
    "        # Extract and generate features\n",
    "        history_edge_features, future_edge_features = self.generate_edge_features(history_indices, future_indices, self.edge_coordinates)\n",
    "        data = MyData(x=history_edge_features,          # (batch_size * num_edges, num_edge_features)\n",
    "                    edge_index=self.edge_index,         # (2, num_edges)\n",
    "                    y=future_edge_features,             # (batch_size, num_edges, 1)\n",
    "                    history_indices=history_indices,    # (batch_size, history_len)\n",
    "                    future_indices=future_indices,      # (batch_size, future_len)\n",
    "                    num_nodes=self.num_edges)      \n",
    "        \n",
    "        return data\n",
    "\n",
    "    def generate_edge_features(self, history_indices, future_indices, history_edge_orientations=None, future_edge_orientations=None):\n",
    "        # Binary on/off edges\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "        \n",
    "        return history_edge_features, future_edge_features\n",
    "    \n",
    "    def build_graph(self):\n",
    "        import networkx as nx\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(self.nodes)\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "        for (start, end), index in indexed_edges:\n",
    "            graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        return graph\n",
    "    \n",
    "def custom_collate_fn(batch):\n",
    "    batch_size = len(batch)\n",
    "    num_edges = batch[0].num_nodes\n",
    "\n",
    "    x = torch.cat([data.x for data in batch], dim=0)\n",
    "    y = torch.cat([data.y for data in batch], dim=0)\n",
    "    history_indices = torch.cat([data.history_indices for data in batch], dim=0)\n",
    "    future_indices = torch.cat([data.future_indices for data in batch], dim=0)\n",
    "\n",
    "    edge_indices = [data.edge_index for data in batch]\n",
    "    for i, edge_index in enumerate(edge_indices):\n",
    "        edge_indices[i] = edge_index + i * num_edges\n",
    "\n",
    "    edge_index = torch.cat(edge_indices, dim=1)\n",
    "    num_nodes = batch_size * num_edges\n",
    "\n",
    "    return MyData(x=x, edge_index=edge_index, y=y, history_indices=history_indices, future_indices=future_indices, num_nodes=num_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import GATv2Conv\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_time=1000., device=None):\n",
    "    \"\"\"\n",
    "    Build sinusoidal embeddings (from Fairseq).\n",
    "\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "\n",
    "    Args:\n",
    "        timesteps: torch.Tensor: generate embedding vectors at these timesteps\n",
    "        embedding_dim: int: dimension of the embeddings to generate\n",
    "        max_time: float: largest time input\n",
    "\n",
    "    Returns:\n",
    "        embedding vectors with shape `(len(timesteps), embedding_dim)`\n",
    "    \"\"\"\n",
    "    timesteps = timesteps.to(device)\n",
    "    assert timesteps.dim() == 1  # Ensure timesteps is a 1D tensor\n",
    "\n",
    "    # Scale timesteps by the maximum time\n",
    "    timesteps = timesteps.float() * (1000. / max_time)\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = torch.log(torch.tensor(10000.0, device=device)) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=device) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "\n",
    "    if embedding_dim % 2 == 1:  # Add zero-padding if embedding dimension is odd\n",
    "        zero_pad = torch.zeros((timesteps.shape[0], 1), dtype=torch.float32)\n",
    "        emb = torch.cat([emb, zero_pad], dim=1)\n",
    "\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb.to(device)\n",
    "\n",
    "\n",
    "def generate_positional_encodings(history_len, embedding_dim, device, n=1000):\n",
    "    PE = torch.zeros((history_len, embedding_dim), device=device)\n",
    "    for k in range(history_len):\n",
    "        for i in torch.arange(int(embedding_dim/2)):\n",
    "            denominator = torch.pow(n, 2*i/embedding_dim)\n",
    "            PE[k, 2*i] = torch.sin(k/denominator)\n",
    "            PE[k, 2*i+1] = torch.cos(k/denominator)\n",
    "    return PE\n",
    "\n",
    "class Edge_Encoder_Residual(nn.Module):\n",
    "    def __init__(self, model_config, history_len, future_len, num_classes, num_edges, hidden_channels, edge_features, num_edge_features, num_timesteps, pos_encoding_dim):\n",
    "        super(Edge_Encoder_Residual, self).__init__()\n",
    "        # Config\n",
    "        self.config = model_config\n",
    "        \n",
    "        # Data\n",
    "        self.num_edges = num_edges\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.model_output = self.config['model_output']\n",
    "        \n",
    "        # Time embedding\n",
    "        self.max_time = num_timesteps\n",
    "        self.time_embedding_dim = self.config['time_embedding_dim']\n",
    "        self.time_linear0 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "        self.time_linear1 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoding_dim = pos_encoding_dim\n",
    "        if 'pos_encoding' in self.edge_features:\n",
    "            self.pos_linear0 = nn.Linear(self.pos_encoding_dim, self.pos_encoding_dim)\n",
    "            self.pos_linear1 = nn.Linear(self.pos_encoding_dim, self.pos_encoding_dim)\n",
    "    \n",
    "        # Model\n",
    "        # GNN layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_heads = self.config['num_heads']\n",
    "        self.num_layers = self.config['num_layers']\n",
    "        self.theta = self.config['theta']\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(self.num_edge_features, self.hidden_channels, heads=self.num_heads, bias=False))\n",
    "        self.res_layers = nn.ModuleList()\n",
    "        self.res_layers.append(nn.Linear(self.num_edge_features, self.hidden_channels * self.num_heads, bias=False))\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.convs.append(GATv2Conv(self.hidden_channels * self.num_heads, self.hidden_channels, heads=self.num_heads, bias=False))\n",
    "            self.res_layers.append(nn.Linear(self.hidden_channels * self.num_heads, self.hidden_channels * self.num_heads, bias=False))\n",
    "\n",
    "        # Output layers for each task\n",
    "        self.condition_dim = self.config['condition_dim']\n",
    "        self.history_encoder = nn.Linear(self.hidden_channels * self.num_heads, self.condition_dim)  # To encode history to c\n",
    "        if 'pos_encoding' in self.edge_features:\n",
    "            self.future_decoder = nn.Linear(self.hidden_channels * self.num_heads + self.condition_dim + self.time_embedding_dim + self.pos_encoding_dim,\n",
    "                                        self.hidden_channels)  # To predict future edges\n",
    "        else:\n",
    "            self.future_decoder = nn.Linear(self.hidden_channels * self.num_heads + self.condition_dim + self.time_embedding_dim,\n",
    "                                            self.hidden_channels)  # To predict future edges\n",
    "        self.adjust_to_class_shape = nn.Linear(self.hidden_channels, self.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, indices=None, t=None, condition=None, mode=None, batch=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        Args:\n",
    "            x: torch.Tensor: input tensor: edge attributes, including binary encoding fo edges present/absent in trajectory\n",
    "            t: torch.Tensor: timestep tensor\n",
    "        \"\"\"    \n",
    "        print(\"Edge Index\", edge_index.size())\n",
    "        print(edge_index)\n",
    "        \n",
    "        # GNN forward pass\n",
    "        # Edge Embedding\n",
    "        if indices is not None:\n",
    "            print(\"Indices\", indices)\n",
    "        batch_size = x.size(0) // self.num_edges\n",
    "        enc = 1\n",
    "        if x.dim() == 3:\n",
    "            enc = 0\n",
    "            x = x.squeeze(0)    # (bs, num_edges, num_edge_features) -> (num_edges, num_edge_features)\n",
    "        #print(\"Input\", x.size())\n",
    "        #print(x)\n",
    "        for conv, res_layer in zip(self.convs, self.res_layers):\n",
    "            res = F.relu(res_layer(x))\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "            x = self.theta * x# + res        # (batch_size * num_edges, hidden_channels * num_heads)\n",
    "            #x = F.relu(res_layer(x))\n",
    "            \n",
    "        print(\"x\", x.size())\n",
    "        print(x)\n",
    "        if mode == 'history':\n",
    "            c = self.history_encoder(x)     # (batch_size * num_edges, condition_dim)\n",
    "            print(\"Condition before\", c.size())\n",
    "            print(c)\n",
    "            if 'pos_encoding' in self.edge_features:\n",
    "                pos_encoding = generate_positional_encodings(self.history_len, self.pos_encoding_dim, device=x.device)\n",
    "                encodings = F.silu(self.pos_linear0(pos_encoding))\n",
    "                encodings = F.silu(self.pos_linear1(encodings))\n",
    "                print(\"Encodings\", encodings.size())\n",
    "                print(encodings)\n",
    "                if enc == 0:\n",
    "                    c = self.integrate_encodings_old(c, indices, encodings)     # (batch_size * num_edges, condition_dim + pos_encoding_dim\n",
    "                else:\n",
    "                    c = self.integrate_encodings(c, indices, encodings)     # (batch_size * num_edges, condition_dim + pos_encoding_dim)\n",
    "            print(\"Condition after\", c.size())\n",
    "            print(c)\n",
    "            return c\n",
    "        \n",
    "        elif mode == 'future':\n",
    "            # Time embedding\n",
    "            # TODO: Check if time embedding and condition generation are the same for each datapoint in a batch and hence the problems while sampling\n",
    "            #print(\"Time\", t)\n",
    "            t_emb = get_timestep_embedding(t, embedding_dim=self.time_embedding_dim, max_time=self.max_time, device=x.device)\n",
    "            t_emb = self.time_linear0(t_emb)\n",
    "            t_emb = F.silu(t_emb)  # SiLU activation, equivalent to Swish\n",
    "            t_emb = self.time_linear1(t_emb)\n",
    "            t_emb = F.silu(t_emb)   # (batch_size, time_embedding_dim)\n",
    "            #print(\"T_emb\", t_emb)\n",
    "            if enc == 0:\n",
    "                t_emb = t_emb.repeat(self.num_edges, 1)\n",
    "            else:\n",
    "                t_emb = torch.repeat_interleave(t_emb, self.num_edges, dim=0) # (batch_size * num_edges, time_embedding_dim)\n",
    "            #print(\"T_emb after\", t_emb)\n",
    "            #print(\"T_emb\", t_emb.size())\n",
    "            \n",
    "            #Concatenation\n",
    "            x = torch.cat((x, t_emb), dim=1) # Concatenate with time embedding, (batch_size * num_edges, hidden_dim')\n",
    "            #print(\"x\", x.size())\n",
    "            x = torch.cat((x, condition), dim=1) # Concatenate with condition c, (batch_size * num_edges, hidden_dim')\n",
    "            #print(\"x\", x.size())\n",
    "            logits = self.future_decoder(x) # (batch_size * num_edges, hidden_channels)\n",
    "            #print(\"logits\", logits.size())\n",
    "            logits = self.adjust_to_class_shape(logits) # (batch_size * num_edges, num_classes=2)\n",
    "            #print(\"Logits size\", logits.size())\n",
    "            if enc == 0:\n",
    "                return logits.unsqueeze(0)\n",
    "            else:\n",
    "                return logits.view(batch_size, self.num_edges, -1)  # (1, num_edges, num_classes=2)\n",
    "\n",
    "    def integrate_encodings(self, features, indices, encodings):\n",
    "        \"\"\"\n",
    "        Integrates positional encodings into the feature matrix.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): Original feature matrix [num_edges, num_features].\n",
    "            indices (torch.Tensor): Indices of edges where the encodings should be added.\n",
    "            encodings (torch.Tensor): Positional encodings [len(indices), encoding_dim].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated feature matrix with positional encodings integrated.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Ensure that features are on the same device as encodings\n",
    "        batch_size = indices.shape[0]# // self.history_len\n",
    "        encodings = encodings.repeat(batch_size, 1)\n",
    "        #print(encodings)\n",
    "        # Ensure that features and encodings are on the same device\n",
    "        features = features.to(encodings.device)\n",
    "        \n",
    "        # Expand the features tensor to accommodate the positional encodings\n",
    "        new_features = torch.cat([features, torch.zeros(features.size(0), encodings.size(1), device=features.device)], dim=1)\n",
    "        \n",
    "        # Calculate batch offsets\n",
    "        batch_offsets = torch.arange(batch_size, device=features.device) * self.num_edges\n",
    "        \n",
    "        # Reshape indices to batched format to adjust with batch offsets\n",
    "        # indices_batched = indices.view(batch_size, self.history_len)\n",
    "        \n",
    "        # Flatten indices for direct access, adjust with batch offsets\n",
    "        flat_indices = (indices + batch_offsets.unsqueeze(1)).flatten()\n",
    "        \n",
    "        # Place the positional encodings in the correct rows across all batches\n",
    "        new_features[flat_indices, -encodings.size(1):] = encodings\n",
    "\n",
    "        return new_features\n",
    "    \n",
    "    def integrate_encodings_old(self, features, indices, encodings):\n",
    "        \"\"\"\n",
    "        Integrates positional encodings into the feature matrix.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): Original feature matrix [num_edges, num_features].\n",
    "            indices (torch.Tensor): Indices of edges where the encodings should be added.\n",
    "            encodings (torch.Tensor): Positional encodings [len(indices), encoding_dim].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Updated feature matrix with positional encodings integrated.\n",
    "        \"\"\"\n",
    "        # Ensure that features are on the same device as encodings\n",
    "        features = features.to(encodings.device)\n",
    "        \n",
    "        # Expand the features tensor to accommodate the positional encodings\n",
    "        new_features = torch.cat([features, torch.zeros(features.size(0), encodings.size(1), device=features.device)], dim=1)\n",
    "        \n",
    "        # Place the positional encodings in the rows specified by indices\n",
    "        new_features[indices, -encodings.size(1):] = encodings\n",
    "\n",
    "        return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3207662/1086019238.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
      "100%|| 4/4 [00:00<00:00, 891.69it/s]\n",
      "/tmp/ipykernel_3207662/1086019238.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge index for line graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edges: 100%|| 46/46 [00:00<00:00, 47780.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Edge index built!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_4_traj.h5'\n",
    "history_len = 5\n",
    "future_len = 5\n",
    "edge_features = ['one_hot_edges', 'coordinates', 'pos_encoding']\n",
    "train_dataset = TrajectoryGeoDataset(train_data_path, history_len, future_len, edge_features, device='cpu')\n",
    "from torch_geometric.data import DataLoader\n",
    "train_data_loader = DataLoader(train_dataset, \n",
    "                                    batch_size=2, \n",
    "                                    shuffle=False, \n",
    "                                    collate_fn=custom_collate_fn, \n",
    "                                    num_workers=0,\n",
    "                                    pin_memory=False,\n",
    "                                    follow_batch=['x', 'y', 'history_indices', 'future_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import networkx as nx\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None, device=None, embedding_dim=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features += 4\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features += 1\n",
    "        '''if 'pos_encoding' in self.edge_features:\n",
    "            self.num_edge_features += self.embedding_dim'''\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = self.load_new_format(file_path, self.device)\n",
    "        \n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n",
    "        self.edge_index = self._build_edge_index()\n",
    "        # self.positional_encoding = self.generate_positional_encodings().float()\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_new_format(file_path, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            # Normalize the coordinates to (0, 1) if any of the coordinates is larger than 1\n",
    "            if node_coordinates.max() > 1:\n",
    "                max_values = node_coordinates.max(0)[0]\n",
    "                min_values = node_coordinates.min(0)[0]\n",
    "                node_coordinates[:, 0] = (node_coordinates[:, 0] - min_values[0]) / (max_values[0] - min_values[0])\n",
    "                node_coordinates[:, 1] = (node_coordinates[:, 1] - min_values[1]) / (max_values[1] - min_values[1])\n",
    "            #edges = torch.tensor(new_hf['graph']['edges'][:], dtype=torch.long, device=device)\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            #edges = [(torch.tensor(edge[0], device=device), torch.tensor(edge[1], device=device)) for edge in edges]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            \n",
    "        return paths, nodes, edges, edge_coordinates\n",
    "    \n",
    "    # @staticmethod\n",
    "    def build_graph(self):\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(self.nodes)\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "        for (start, end), index in indexed_edges:\n",
    "            graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        return graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "    \n",
    "        edge_idxs = trajectory['edge_idxs']\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = trajectory['edge_orientations']\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices, orientations, and coordinates\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "\n",
    "        # Extract and generate features\n",
    "        history_edge_features, future_edge_features = self.generate_edge_features(history_indices, future_indices, self.edge_coordinates)\n",
    "\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "        }\n",
    "\n",
    "    def generate_edge_features(self, history_indices, future_indices, history_edge_orientations=None, future_edge_orientations=None):\n",
    "        # Binary on/off edges\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        '''if 'pos_encoding' in self.edge_features:\n",
    "            encoding_tensor = torch.zeros((len(self.edges), self.embedding_dim), dtype=torch.float64, device=self.device)\n",
    "            for i, index in enumerate(history_indices):\n",
    "                encoding_tensor[index] = self.positional_encoding[i]\n",
    "            history_edge_features = torch.cat((history_edge_features, encoding_tensor.float()), dim=1)    '''\n",
    "        \n",
    "        return history_edge_features, future_edge_features\n",
    "    \n",
    "    def generate_positional_encodings(self):\n",
    "        position = torch.arange(self.history_len)\n",
    "        angle_rates = 1 / torch.pow(10000, (2 * (torch.arange(self.embedding_dim) // 2)) / self.embedding_dim)\n",
    "        angle_rads = position.float().unsqueeze(1) * angle_rates.unsqueeze(0)\n",
    "        sines = torch.sin(angle_rads[:, 0::2])\n",
    "        cosines = torch.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        return torch.cat((sines.float(), cosines.float()), dim=-1).to(self.device, non_blocking=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "    \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        self.G = self.build_graph()\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.G.edges(data=True)], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.G.edges(data=True)}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        return edge_index.to(self.device, non_blocking=True)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    history_indices = torch.stack([item['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item['future_indices'] for item in batch])\n",
    "    history_edge_features = torch.stack([item['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item['future_edge_features'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"history_indices\": history_indices,\n",
    "        \"future_indices\": future_indices,\n",
    "        \"history_edge_features\": history_edge_features,\n",
    "        \"future_edge_features\": future_edge_features,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3207662/2380671171.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
      "100%|| 4/4 [00:00<00:00, 1018.34it/s]\n",
      "/tmp/ipykernel_3207662/2380671171.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge index for line graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edges: 100%|| 46/46 [00:00<00:00, 64269.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Edge index built!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_path = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_4_traj.h5'\n",
    "history_len = 5\n",
    "future_len = 5\n",
    "edge_features = ['one_hot_edges', 'coordinates', 'pos_encoding']\n",
    "old_train_dataset = TrajectoryDataset(train_data_path, history_len, future_len, edge_features, device='cpu')\n",
    "from torch.utils.data import DataLoader\n",
    "old_train_data_loader = DataLoader(old_train_dataset, \n",
    "                                    batch_size=2, \n",
    "                                    shuffle=False, \n",
    "                                    collate_fn=collate_fn, \n",
    "                                    num_workers=0,\n",
    "                                    pin_memory=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\"dataset\": \"synthetic_20_traj\",\n",
    "    \"train_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_4_traj.h5',\n",
    "    \"val_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_4_traj.h5',\n",
    "    \"history_len\": 5,\n",
    "    \"future_len\": 5,\n",
    "    \"num_classes\": 2,\n",
    "    \"pos_encoding_dim\": 2,\n",
    "    \"edge_features\": ['one_hot_edges', 'coordinates', 'pos_encoding']#, 'pos_encoding'\n",
    "    }\n",
    "\n",
    "diffusion_config = {\"type\": 'cosine', # Options: 'linear', 'cosine', 'jsd'\n",
    "    \"start\": 0.0001,  # 1e-4 gauss, 0.02 uniform\n",
    "    \"stop\": 0.01,  # 0.02 gauss, 1. uniform\n",
    "    \"num_timesteps\": 50}\n",
    "\n",
    "model_config = {\"name\": \"edge_encoder_residual\",\n",
    "    \"hidden_channels\": 4,\n",
    "    \"time_embedding_dim\": 2,\n",
    "    \"condition_dim\": 2,\n",
    "    \"num_heads\": 1,\n",
    "    \"num_layers\": 1,\n",
    "    \"theta\": 1.0, # controls strength of conv layers in residual model\n",
    "    \"dropout\": 0.1,\n",
    "    \"model_output\": \"logits\",\n",
    "    \"model_prediction\": \"x_start\",  # Options: 'x_start','xprev'\n",
    "    \"transition_mat_type\": 'custom',  # Options: 'gaussian', 'uniform', 'absorbing', 'marginal_prior', 'custom'\n",
    "    \"transition_bands\": 0,\n",
    "    \"loss_type\": \"cross_entropy_x_start\",  # Options: kl, cross_entropy_x_start, hybrid\n",
    "    \"hybrid_coeff\": 0.001,  # Only used for hybrid loss type.\n",
    "    }\n",
    "\n",
    "train_config = {\"batch_size\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"gradient_accumulation\": False,\n",
    "    \"gradient_accumulation_steps\": 16,\n",
    "    \"num_epochs\": 800,\n",
    "    \"learning_rate_warmup_steps\": 300, # previously 10000\n",
    "    \"lr_decay\": 0.9995, # previously 0.9999\n",
    "    \"log_loss_every_steps\": 2,\n",
    "    \"log_metrics_every_steps\": 2,\n",
    "    \"save_model\": False,\n",
    "    \"save_model_every_steps\": 1}\n",
    "\n",
    "test_config = {\"batch_size\": 1, # currently only 1 works\n",
    "    \"model_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/geolife_residual/geolife_residual_edge_encoder_residual_marginal_prior_cosine_hidden_dim_32_time_dim_16_condition_dim_16_layers_2.pth',\n",
    "    \"number_samples\": 1,\n",
    "    \"eval_every_steps\": 100\n",
    "  }\n",
    "\n",
    "wandb_config = {\"exp_name\": \"synthetic_d3pm_test\",\n",
    "    \"project\": \"trajectory_prediction_using_denoising_diffusion_models\",\n",
    "    \"entity\": \"joeschmit99\",\n",
    "    \"job_type\": \"test\",\n",
    "    \"notes\": \"\",\n",
    "    \"tags\": [\"synthetic\", \"edge_encoder\"]} \n",
    "G = train_dataset.build_graph()\n",
    "\n",
    "model = Edge_Encoder_Residual(model_config, history_len, future_len, num_classes=2, num_edges=G.number_of_edges(), hidden_channels=4, edge_features=edge_features, num_edge_features=5, num_timesteps=50, pos_encoding_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Index torch.Size([2, 488])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 47, 47, 47, 47, 47,\n",
      "         48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50,\n",
      "         50, 51, 51, 51, 51, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55,\n",
      "         55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 59,\n",
      "         59, 59, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62,\n",
      "         63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65,\n",
      "         65, 65, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68,\n",
      "         68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70,\n",
      "         70, 71, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74,\n",
      "         74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76,\n",
      "         76, 76, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 79, 79,\n",
      "         79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83,\n",
      "         83, 83, 83, 83, 84, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86,\n",
      "         87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 90, 90, 90, 90, 90, 90, 91, 91,\n",
      "         91, 91],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44, 47, 48, 87, 46, 48, 49, 50, 51,\n",
      "         46, 47, 49, 65, 76, 79, 47, 48, 50, 51, 65, 76, 79, 47, 49, 51, 62, 65,\n",
      "         66, 47, 49, 50, 52, 51, 53, 54, 52, 54, 61, 52, 53, 55, 56, 54, 56, 57,\n",
      "         58, 59, 54, 55, 57, 60, 55, 56, 58, 59, 60, 55, 57, 59, 70, 73, 90, 55,\n",
      "         57, 58, 56, 57, 64, 68, 69, 70, 53, 62, 63, 64, 50, 61, 63, 64, 65, 66,\n",
      "         61, 62, 64, 66, 67, 68, 60, 61, 62, 63, 68, 69, 70, 48, 49, 50, 62, 66,\n",
      "         76, 79, 50, 62, 63, 65, 67, 68, 63, 66, 68, 74, 75, 76, 77, 78, 60, 63,\n",
      "         64, 66, 67, 69, 70, 60, 64, 68, 70, 71, 72, 73, 58, 60, 64, 68, 69, 73,\n",
      "         90, 69, 72, 73, 82, 83, 69, 71, 73, 74, 58, 69, 70, 71, 72, 90, 67, 72,\n",
      "         75, 76, 77, 78, 67, 74, 76, 77, 78, 81, 84, 48, 49, 65, 67, 74, 75, 77,\n",
      "         78, 79, 67, 74, 75, 76, 78, 81, 82, 67, 74, 75, 76, 77, 79, 80, 48, 49,\n",
      "         65, 76, 78, 80, 78, 79, 84, 85, 86, 75, 77, 82, 84, 71, 77, 81, 83, 71,\n",
      "         82, 88, 90, 91, 75, 80, 81, 85, 86, 80, 84, 86, 87, 80, 84, 85, 88, 89,\n",
      "         46, 85, 83, 86, 89, 90, 91, 86, 88, 91, 58, 70, 73, 83, 88, 91, 83, 88,\n",
      "         89, 90]])\n",
      "Indices tensor([[ 8,  7, 15, 16, 19],\n",
      "        [44, 24, 18, 16, 19]])\n",
      "x torch.Size([92, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1241, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2324, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2840, 0.0000, 0.0045],\n",
      "        [0.0000, 0.2317, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1208, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0686, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1890, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2356, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1178, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0451, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0163, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0243, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0140, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0106, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([92, 2])\n",
      "tensor([[-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1551, -0.3586],\n",
      "        [-0.1267, -0.3395],\n",
      "        [-0.1552, -0.3591],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1183, -0.3333],\n",
      "        [-0.1689, -0.3709],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1933, -0.3884],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1168, -0.3322],\n",
      "        [-0.0924, -0.3141],\n",
      "        [-0.1646, -0.3676],\n",
      "        [-0.1364, -0.3467],\n",
      "        [-0.0779, -0.3033],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1022, -0.3213],\n",
      "        [-0.1115, -0.3283],\n",
      "        [-0.1358, -0.3463],\n",
      "        [-0.0936, -0.3150],\n",
      "        [-0.1068, -0.3248],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1236, -0.3337],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1551, -0.3586],\n",
      "        [-0.1267, -0.3395],\n",
      "        [-0.1552, -0.3591],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1487, -0.3558],\n",
      "        [-0.1704, -0.3720],\n",
      "        [-0.1154, -0.3311],\n",
      "        [-0.1039, -0.3226],\n",
      "        [-0.1646, -0.3676],\n",
      "        [-0.1364, -0.3467],\n",
      "        [-0.0779, -0.3033],\n",
      "        [-0.0814, -0.3059],\n",
      "        [-0.0680, -0.2960],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0717, -0.2987],\n",
      "        [-0.1022, -0.3213],\n",
      "        [-0.1115, -0.3283],\n",
      "        [-0.1358, -0.3463],\n",
      "        [-0.0936, -0.3150],\n",
      "        [-0.1068, -0.3248],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1236, -0.3337],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0610, -0.2908],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0669, -0.2952],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0653, -0.2940]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([92, 4])\n",
      "tensor([[-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1551, -0.3586,  0.0000,  0.0000],\n",
      "        [-0.1267, -0.3395,  0.0000,  0.0000],\n",
      "        [-0.1552, -0.3591,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1183, -0.3333,  0.0000,  0.0000],\n",
      "        [-0.1689, -0.3709,  0.1196,  0.1054],\n",
      "        [-0.0603, -0.2903,  0.1198,  0.1403],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1933, -0.3884,  0.1080,  0.1321],\n",
      "        [-0.1686, -0.3706,  0.1023,  0.1904],\n",
      "        [-0.1168, -0.3322,  0.0000,  0.0000],\n",
      "        [-0.0924, -0.3141,  0.0000,  0.0000],\n",
      "        [-0.1646, -0.3676,  0.1013,  0.2107],\n",
      "        [-0.1364, -0.3467,  0.0000,  0.0000],\n",
      "        [-0.0779, -0.3033,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1022, -0.3213,  0.0000,  0.0000],\n",
      "        [-0.1115, -0.3283,  0.0000,  0.0000],\n",
      "        [-0.1358, -0.3463,  0.0000,  0.0000],\n",
      "        [-0.0936, -0.3150,  0.0000,  0.0000],\n",
      "        [-0.1068, -0.3248,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.3706,  0.0000,  0.0000],\n",
      "        [-0.1236, -0.3337,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1551, -0.3586,  0.0000,  0.0000],\n",
      "        [-0.1267, -0.3395,  0.0000,  0.0000],\n",
      "        [-0.1552, -0.3591,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1487, -0.3558,  0.0000,  0.0000],\n",
      "        [-0.1704, -0.3720,  0.1023,  0.1904],\n",
      "        [-0.1154, -0.3311,  0.0000,  0.0000],\n",
      "        [-0.1039, -0.3226,  0.1080,  0.1321],\n",
      "        [-0.1646, -0.3676,  0.1013,  0.2107],\n",
      "        [-0.1364, -0.3467,  0.0000,  0.0000],\n",
      "        [-0.0779, -0.3033,  0.0000,  0.0000],\n",
      "        [-0.0814, -0.3059,  0.0000,  0.0000],\n",
      "        [-0.0680, -0.2960,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.1196,  0.1054],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0717, -0.2987,  0.0000,  0.0000],\n",
      "        [-0.1022, -0.3213,  0.0000,  0.0000],\n",
      "        [-0.1115, -0.3283,  0.0000,  0.0000],\n",
      "        [-0.1358, -0.3463,  0.0000,  0.0000],\n",
      "        [-0.0936, -0.3150,  0.0000,  0.0000],\n",
      "        [-0.1068, -0.3248,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.3706,  0.0000,  0.0000],\n",
      "        [-0.1236, -0.3337,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0610, -0.2908,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0669, -0.2952,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.1198,  0.1403],\n",
      "        [-0.0653, -0.2940,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 488])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 47, 47, 47, 47, 47,\n",
      "         48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50,\n",
      "         50, 51, 51, 51, 51, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55,\n",
      "         55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 59,\n",
      "         59, 59, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62,\n",
      "         63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65,\n",
      "         65, 65, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68,\n",
      "         68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70,\n",
      "         70, 71, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74,\n",
      "         74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76,\n",
      "         76, 76, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 79, 79,\n",
      "         79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83,\n",
      "         83, 83, 83, 83, 84, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86,\n",
      "         87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 90, 90, 90, 90, 90, 90, 91, 91,\n",
      "         91, 91],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44, 47, 48, 87, 46, 48, 49, 50, 51,\n",
      "         46, 47, 49, 65, 76, 79, 47, 48, 50, 51, 65, 76, 79, 47, 49, 51, 62, 65,\n",
      "         66, 47, 49, 50, 52, 51, 53, 54, 52, 54, 61, 52, 53, 55, 56, 54, 56, 57,\n",
      "         58, 59, 54, 55, 57, 60, 55, 56, 58, 59, 60, 55, 57, 59, 70, 73, 90, 55,\n",
      "         57, 58, 56, 57, 64, 68, 69, 70, 53, 62, 63, 64, 50, 61, 63, 64, 65, 66,\n",
      "         61, 62, 64, 66, 67, 68, 60, 61, 62, 63, 68, 69, 70, 48, 49, 50, 62, 66,\n",
      "         76, 79, 50, 62, 63, 65, 67, 68, 63, 66, 68, 74, 75, 76, 77, 78, 60, 63,\n",
      "         64, 66, 67, 69, 70, 60, 64, 68, 70, 71, 72, 73, 58, 60, 64, 68, 69, 73,\n",
      "         90, 69, 72, 73, 82, 83, 69, 71, 73, 74, 58, 69, 70, 71, 72, 90, 67, 72,\n",
      "         75, 76, 77, 78, 67, 74, 76, 77, 78, 81, 84, 48, 49, 65, 67, 74, 75, 77,\n",
      "         78, 79, 67, 74, 75, 76, 78, 81, 82, 67, 74, 75, 76, 77, 79, 80, 48, 49,\n",
      "         65, 76, 78, 80, 78, 79, 84, 85, 86, 75, 77, 82, 84, 71, 77, 81, 83, 71,\n",
      "         82, 88, 90, 91, 75, 80, 81, 85, 86, 80, 84, 86, 87, 80, 84, 85, 88, 89,\n",
      "         46, 85, 83, 86, 89, 90, 91, 86, 88, 91, 58, 70, 73, 83, 88, 91, 83, 88,\n",
      "         89, 90]])\n",
      "x torch.Size([92, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1241, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2324, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2840, 0.0000, 0.0045],\n",
      "        [0.0000, 0.2317, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1208, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0686, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1890, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2356, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1178, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0451, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0163, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0243, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0140, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0106, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "Edge Index torch.Size([2, 488])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 47, 47, 47, 47, 47,\n",
      "         48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50,\n",
      "         50, 51, 51, 51, 51, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55,\n",
      "         55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 59,\n",
      "         59, 59, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62,\n",
      "         63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65,\n",
      "         65, 65, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68,\n",
      "         68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70,\n",
      "         70, 71, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74,\n",
      "         74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76,\n",
      "         76, 76, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 79, 79,\n",
      "         79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83,\n",
      "         83, 83, 83, 83, 84, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86,\n",
      "         87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 90, 90, 90, 90, 90, 90, 91, 91,\n",
      "         91, 91],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44, 47, 48, 87, 46, 48, 49, 50, 51,\n",
      "         46, 47, 49, 65, 76, 79, 47, 48, 50, 51, 65, 76, 79, 47, 49, 51, 62, 65,\n",
      "         66, 47, 49, 50, 52, 51, 53, 54, 52, 54, 61, 52, 53, 55, 56, 54, 56, 57,\n",
      "         58, 59, 54, 55, 57, 60, 55, 56, 58, 59, 60, 55, 57, 59, 70, 73, 90, 55,\n",
      "         57, 58, 56, 57, 64, 68, 69, 70, 53, 62, 63, 64, 50, 61, 63, 64, 65, 66,\n",
      "         61, 62, 64, 66, 67, 68, 60, 61, 62, 63, 68, 69, 70, 48, 49, 50, 62, 66,\n",
      "         76, 79, 50, 62, 63, 65, 67, 68, 63, 66, 68, 74, 75, 76, 77, 78, 60, 63,\n",
      "         64, 66, 67, 69, 70, 60, 64, 68, 70, 71, 72, 73, 58, 60, 64, 68, 69, 73,\n",
      "         90, 69, 72, 73, 82, 83, 69, 71, 73, 74, 58, 69, 70, 71, 72, 90, 67, 72,\n",
      "         75, 76, 77, 78, 67, 74, 76, 77, 78, 81, 84, 48, 49, 65, 67, 74, 75, 77,\n",
      "         78, 79, 67, 74, 75, 76, 78, 81, 82, 67, 74, 75, 76, 77, 79, 80, 48, 49,\n",
      "         65, 76, 78, 80, 78, 79, 84, 85, 86, 75, 77, 82, 84, 71, 77, 81, 83, 71,\n",
      "         82, 88, 90, 91, 75, 80, 81, 85, 86, 80, 84, 86, 87, 80, 84, 85, 88, 89,\n",
      "         46, 85, 83, 86, 89, 90, 91, 86, 88, 91, 58, 70, 73, 83, 88, 91, 83, 88,\n",
      "         89, 90]])\n",
      "Indices tensor([[33, 32, 31, 36, 25],\n",
      "        [41, 39, 34, 32, 28]])\n",
      "x torch.Size([92, 4])\n",
      "tensor([[0.0000e+00, 7.2055e-02, 3.2913e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.9906e-01, 4.4998e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3908e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5683e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5654e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.3934e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.8945e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3982e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4519e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6931e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6745e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9554e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9269e-01, 0.0000e+00, 5.1424e-03],\n",
      "        [0.0000e+00, 3.3398e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.8042e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0282e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1939e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4327e-01, 1.3791e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5702e-01, 1.5353e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 3.6775e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0970e-01, 2.9835e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1592e-01, 2.2413e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1781e-01, 1.0757e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.9809e-02, 2.4540e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2396e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5826e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.2865e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3435e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4726e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0744e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9319e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0950e-01, 0.0000e+00, 1.6729e-02],\n",
      "        [0.0000e+00, 3.9323e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4145e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.2273e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.6620e-01, 0.0000e+00, 1.7102e-02],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.5995e-01, 3.2607e-02, 1.0858e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([92, 2])\n",
      "tensor([[-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1536, -0.3574],\n",
      "        [-0.1253, -0.3385],\n",
      "        [-0.0815, -0.3060],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0685, -0.2964],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1336, -0.3447],\n",
      "        [-0.0667, -0.2950],\n",
      "        [-0.1335, -0.3446],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0949, -0.3159],\n",
      "        [-0.0972, -0.3177],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1724, -0.3734],\n",
      "        [-0.1749, -0.3753],\n",
      "        [-0.1862, -0.3836],\n",
      "        [-0.1853, -0.3830],\n",
      "        [-0.1984, -0.3927],\n",
      "        [-0.1974, -0.3914],\n",
      "        [-0.2164, -0.4061],\n",
      "        [-0.1914, -0.3875],\n",
      "        [-0.2018, -0.3953],\n",
      "        [-0.1161, -0.3317],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1624, -0.3557],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1159, -0.3266],\n",
      "        [-0.0931, -0.3135],\n",
      "        [-0.0815, -0.3060],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0685, -0.2964],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1035, -0.3223],\n",
      "        [-0.0667, -0.2950],\n",
      "        [-0.1343, -0.3452],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0944, -0.3156],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1698, -0.3715],\n",
      "        [-0.1759, -0.3760],\n",
      "        [-0.1621, -0.3658],\n",
      "        [-0.1573, -0.3622],\n",
      "        [-0.1973, -0.3919],\n",
      "        [-0.2059, -0.3965],\n",
      "        [-0.2441, -0.4266],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.2111, -0.4022],\n",
      "        [-0.2792, -0.4508],\n",
      "        [-0.1621, -0.3658],\n",
      "        [-0.3285, -0.4761],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([92, 4])\n",
      "tensor([[-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1536, -0.3574,  0.0000,  0.0000],\n",
      "        [-0.1253, -0.3385,  0.0000,  0.0000],\n",
      "        [-0.0815, -0.3060,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0685, -0.2964,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1336, -0.3447,  0.0000,  0.0000],\n",
      "        [-0.0667, -0.2950,  0.0000,  0.0000],\n",
      "        [-0.1335, -0.3446,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0949, -0.3159,  0.1013,  0.2107],\n",
      "        [-0.0972, -0.3177,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1724, -0.3734,  0.0000,  0.0000],\n",
      "        [-0.1749, -0.3753,  0.0000,  0.0000],\n",
      "        [-0.1862, -0.3836,  0.0000,  0.0000],\n",
      "        [-0.1853, -0.3830,  0.1080,  0.1321],\n",
      "        [-0.1984, -0.3927,  0.1196,  0.1054],\n",
      "        [-0.1974, -0.3914,  0.1198,  0.1403],\n",
      "        [-0.2164, -0.4061,  0.0000,  0.0000],\n",
      "        [-0.1914, -0.3875,  0.0000,  0.0000],\n",
      "        [-0.2018, -0.3953,  0.1023,  0.1904],\n",
      "        [-0.1161, -0.3317,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1624, -0.3557,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1159, -0.3266,  0.0000,  0.0000],\n",
      "        [-0.0931, -0.3135,  0.0000,  0.0000],\n",
      "        [-0.0815, -0.3060,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0685, -0.2964,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1035, -0.3223,  0.0000,  0.0000],\n",
      "        [-0.0667, -0.2950,  0.0000,  0.0000],\n",
      "        [-0.1343, -0.3452,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0944, -0.3156,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1698, -0.3715,  0.1013,  0.2107],\n",
      "        [-0.1759, -0.3760,  0.0000,  0.0000],\n",
      "        [-0.1621, -0.3658,  0.0000,  0.0000],\n",
      "        [-0.1573, -0.3622,  0.0000,  0.0000],\n",
      "        [-0.1973, -0.3919,  0.1023,  0.1904],\n",
      "        [-0.2059, -0.3965,  0.0000,  0.0000],\n",
      "        [-0.2441, -0.4266,  0.1080,  0.1321],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.2111, -0.4022,  0.0000,  0.0000],\n",
      "        [-0.2792, -0.4508,  0.1196,  0.1054],\n",
      "        [-0.1621, -0.3658,  0.0000,  0.0000],\n",
      "        [-0.3285, -0.4761,  0.1198,  0.1403],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 488])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 46, 46, 46, 47, 47, 47, 47, 47,\n",
      "         48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50,\n",
      "         50, 51, 51, 51, 51, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 55, 55, 55,\n",
      "         55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 59,\n",
      "         59, 59, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62,\n",
      "         63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65,\n",
      "         65, 65, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68,\n",
      "         68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70,\n",
      "         70, 71, 71, 71, 71, 71, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 74, 74,\n",
      "         74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76,\n",
      "         76, 76, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 79, 79,\n",
      "         79, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83,\n",
      "         83, 83, 83, 83, 84, 84, 84, 84, 84, 85, 85, 85, 85, 86, 86, 86, 86, 86,\n",
      "         87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 90, 90, 90, 90, 90, 90, 91, 91,\n",
      "         91, 91],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44, 47, 48, 87, 46, 48, 49, 50, 51,\n",
      "         46, 47, 49, 65, 76, 79, 47, 48, 50, 51, 65, 76, 79, 47, 49, 51, 62, 65,\n",
      "         66, 47, 49, 50, 52, 51, 53, 54, 52, 54, 61, 52, 53, 55, 56, 54, 56, 57,\n",
      "         58, 59, 54, 55, 57, 60, 55, 56, 58, 59, 60, 55, 57, 59, 70, 73, 90, 55,\n",
      "         57, 58, 56, 57, 64, 68, 69, 70, 53, 62, 63, 64, 50, 61, 63, 64, 65, 66,\n",
      "         61, 62, 64, 66, 67, 68, 60, 61, 62, 63, 68, 69, 70, 48, 49, 50, 62, 66,\n",
      "         76, 79, 50, 62, 63, 65, 67, 68, 63, 66, 68, 74, 75, 76, 77, 78, 60, 63,\n",
      "         64, 66, 67, 69, 70, 60, 64, 68, 70, 71, 72, 73, 58, 60, 64, 68, 69, 73,\n",
      "         90, 69, 72, 73, 82, 83, 69, 71, 73, 74, 58, 69, 70, 71, 72, 90, 67, 72,\n",
      "         75, 76, 77, 78, 67, 74, 76, 77, 78, 81, 84, 48, 49, 65, 67, 74, 75, 77,\n",
      "         78, 79, 67, 74, 75, 76, 78, 81, 82, 67, 74, 75, 76, 77, 79, 80, 48, 49,\n",
      "         65, 76, 78, 80, 78, 79, 84, 85, 86, 75, 77, 82, 84, 71, 77, 81, 83, 71,\n",
      "         82, 88, 90, 91, 75, 80, 81, 85, 86, 80, 84, 86, 87, 80, 84, 85, 88, 89,\n",
      "         46, 85, 83, 86, 89, 90, 91, 86, 88, 91, 58, 70, 73, 83, 88, 91, 83, 88,\n",
      "         89, 90]])\n",
      "x torch.Size([92, 4])\n",
      "tensor([[0.0000e+00, 7.2055e-02, 3.2913e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.9906e-01, 4.4998e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3908e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5683e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5654e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.3934e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.8945e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3982e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4519e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6931e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.6745e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9554e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9269e-01, 0.0000e+00, 5.1424e-03],\n",
      "        [0.0000e+00, 3.3398e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.8042e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0282e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1939e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.4327e-01, 1.3791e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5702e-01, 1.5353e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 3.6775e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0970e-01, 2.9835e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1592e-01, 2.2413e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1781e-01, 1.0757e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.9809e-02, 2.4540e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2396e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5826e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.2865e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3435e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4726e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0744e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9319e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0950e-01, 0.0000e+00, 1.6729e-02],\n",
      "        [0.0000e+00, 3.9323e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4145e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.2273e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.6620e-01, 0.0000e+00, 1.7102e-02],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.5995e-01, 3.2607e-02, 1.0858e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in train_data_loader:\n",
    "    batch_size = data['history_indices'].size(0)\n",
    "    history_edge_features = data.x\n",
    "    history_indices = data.history_indices\n",
    "    #print(\"x\", history_edge_features.size())\n",
    "    # future_edge_indices_one_hot = data.y[:, :, 0]\n",
    "    edge_index = data.edge_index\n",
    "    '''edge_index_in = edge_index.clone()\n",
    "    for i in range(batch_size):\n",
    "        edge_index_in = torch.cat((edge_index_in, edge_index + i * train_dataset.num_edges), dim=1)\n",
    "'''    \n",
    "    c = model.forward(x=history_edge_features, edge_index=edge_index, indices=history_indices, mode='history', batch=data)\n",
    "    out = model.forward(history_edge_features, edge_index, t=torch.tensor([34, 10]), condition=c, mode='future', batch=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "Indices tensor([ 8,  7, 15, 16, 19])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1241, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2324, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2840, 0.0000, 0.0045],\n",
      "        [0.0000, 0.2317, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1208, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0686, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([46, 2])\n",
      "tensor([[-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1551, -0.3586],\n",
      "        [-0.1267, -0.3395],\n",
      "        [-0.1552, -0.3591],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1183, -0.3333],\n",
      "        [-0.1689, -0.3709],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1933, -0.3884],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1168, -0.3322],\n",
      "        [-0.0924, -0.3141],\n",
      "        [-0.1646, -0.3676],\n",
      "        [-0.1364, -0.3467],\n",
      "        [-0.0779, -0.3033],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1022, -0.3213],\n",
      "        [-0.1115, -0.3283],\n",
      "        [-0.1358, -0.3463],\n",
      "        [-0.0936, -0.3150],\n",
      "        [-0.1068, -0.3248],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1236, -0.3337],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([46, 4])\n",
      "tensor([[-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1551, -0.3586,  0.0000,  0.0000],\n",
      "        [-0.1267, -0.3395,  0.0000,  0.0000],\n",
      "        [-0.1552, -0.3591,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1183, -0.3333,  0.0000,  0.0000],\n",
      "        [-0.1689, -0.3709,  0.1196,  0.1054],\n",
      "        [-0.0603, -0.2903,  0.1198,  0.1403],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1933, -0.3884,  0.1080,  0.1321],\n",
      "        [-0.1686, -0.3706,  0.1023,  0.1904],\n",
      "        [-0.1168, -0.3322,  0.0000,  0.0000],\n",
      "        [-0.0924, -0.3141,  0.0000,  0.0000],\n",
      "        [-0.1646, -0.3676,  0.1013,  0.2107],\n",
      "        [-0.1364, -0.3467,  0.0000,  0.0000],\n",
      "        [-0.0779, -0.3033,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1022, -0.3213,  0.0000,  0.0000],\n",
      "        [-0.1115, -0.3283,  0.0000,  0.0000],\n",
      "        [-0.1358, -0.3463,  0.0000,  0.0000],\n",
      "        [-0.0936, -0.3150,  0.0000,  0.0000],\n",
      "        [-0.1068, -0.3248,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.3706,  0.0000,  0.0000],\n",
      "        [-0.1236, -0.3337,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1241, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2324, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2840, 0.0000, 0.0045],\n",
      "        [0.0000, 0.2317, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1208, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0686, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "i: 1\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "Indices tensor([44, 24, 18, 16, 19])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1890, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2356, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1178, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0451, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0163, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0243, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0140, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0106, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([46, 2])\n",
      "tensor([[-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1551, -0.3586],\n",
      "        [-0.1267, -0.3395],\n",
      "        [-0.1552, -0.3591],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1487, -0.3558],\n",
      "        [-0.1704, -0.3720],\n",
      "        [-0.1154, -0.3311],\n",
      "        [-0.1039, -0.3226],\n",
      "        [-0.1646, -0.3676],\n",
      "        [-0.1364, -0.3467],\n",
      "        [-0.0779, -0.3033],\n",
      "        [-0.0814, -0.3059],\n",
      "        [-0.0680, -0.2960],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0717, -0.2987],\n",
      "        [-0.1022, -0.3213],\n",
      "        [-0.1115, -0.3283],\n",
      "        [-0.1358, -0.3463],\n",
      "        [-0.0936, -0.3150],\n",
      "        [-0.1068, -0.3248],\n",
      "        [-0.1686, -0.3706],\n",
      "        [-0.1236, -0.3337],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0610, -0.2908],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0669, -0.2952],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0653, -0.2940]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([46, 4])\n",
      "tensor([[-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1551, -0.3586,  0.0000,  0.0000],\n",
      "        [-0.1267, -0.3395,  0.0000,  0.0000],\n",
      "        [-0.1552, -0.3591,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1487, -0.3558,  0.0000,  0.0000],\n",
      "        [-0.1704, -0.3720,  0.1023,  0.1904],\n",
      "        [-0.1154, -0.3311,  0.0000,  0.0000],\n",
      "        [-0.1039, -0.3226,  0.1080,  0.1321],\n",
      "        [-0.1646, -0.3676,  0.1013,  0.2107],\n",
      "        [-0.1364, -0.3467,  0.0000,  0.0000],\n",
      "        [-0.0779, -0.3033,  0.0000,  0.0000],\n",
      "        [-0.0814, -0.3059,  0.0000,  0.0000],\n",
      "        [-0.0680, -0.2960,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.1196,  0.1054],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0717, -0.2987,  0.0000,  0.0000],\n",
      "        [-0.1022, -0.3213,  0.0000,  0.0000],\n",
      "        [-0.1115, -0.3283,  0.0000,  0.0000],\n",
      "        [-0.1358, -0.3463,  0.0000,  0.0000],\n",
      "        [-0.0936, -0.3150,  0.0000,  0.0000],\n",
      "        [-0.1068, -0.3248,  0.0000,  0.0000],\n",
      "        [-0.1686, -0.3706,  0.0000,  0.0000],\n",
      "        [-0.1236, -0.3337,  0.0000,  0.0000],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0610, -0.2908,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0669, -0.2952,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.1198,  0.1403],\n",
      "        [-0.0653, -0.2940,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.2023, 0.0434, 0.0000],\n",
      "        [0.0000, 0.1419, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2012, 0.0000, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1890, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2356, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1178, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0933, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2231, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1627, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0376, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0451, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0163, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0243, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0895, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1096, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1615, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0712, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0994, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2316, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1345, 0.0756, 0.0000],\n",
      "        [0.0000, 0.0541, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0013, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0140, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0106, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "i: 0\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "Indices tensor([33, 32, 31, 36, 25])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.1991, 0.0450, 0.0000],\n",
      "        [0.0000, 0.1391, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0454, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0175, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1568, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0136, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1565, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0739, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0789, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2398, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2452, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2693, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2675, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2955, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2927, 0.0000, 0.0051],\n",
      "        [0.0000, 0.3340, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2804, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3028, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1194, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([46, 2])\n",
      "tensor([[-0.0957, -0.3014],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1536, -0.3574],\n",
      "        [-0.1253, -0.3385],\n",
      "        [-0.0815, -0.3060],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0685, -0.2964],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1336, -0.3447],\n",
      "        [-0.0667, -0.2950],\n",
      "        [-0.1335, -0.3446],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0949, -0.3159],\n",
      "        [-0.0972, -0.3177],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1724, -0.3734],\n",
      "        [-0.1749, -0.3753],\n",
      "        [-0.1862, -0.3836],\n",
      "        [-0.1853, -0.3830],\n",
      "        [-0.1984, -0.3927],\n",
      "        [-0.1974, -0.3914],\n",
      "        [-0.2164, -0.4061],\n",
      "        [-0.1914, -0.3875],\n",
      "        [-0.2018, -0.3953],\n",
      "        [-0.1161, -0.3317],\n",
      "        [-0.1274, -0.3394],\n",
      "        [-0.1345, -0.3382],\n",
      "        [-0.0775, -0.3030],\n",
      "        [-0.1598, -0.3504],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([46, 4])\n",
      "tensor([[-0.0957, -0.3014,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1536, -0.3574,  0.0000,  0.0000],\n",
      "        [-0.1253, -0.3385,  0.0000,  0.0000],\n",
      "        [-0.0815, -0.3060,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0685, -0.2964,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1336, -0.3447,  0.0000,  0.0000],\n",
      "        [-0.0667, -0.2950,  0.0000,  0.0000],\n",
      "        [-0.1335, -0.3446,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0949, -0.3159,  0.1013,  0.2107],\n",
      "        [-0.0972, -0.3177,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1724, -0.3734,  0.0000,  0.0000],\n",
      "        [-0.1749, -0.3753,  0.0000,  0.0000],\n",
      "        [-0.1862, -0.3836,  0.0000,  0.0000],\n",
      "        [-0.1853, -0.3830,  0.1080,  0.1321],\n",
      "        [-0.1984, -0.3927,  0.1196,  0.1054],\n",
      "        [-0.1974, -0.3914,  0.1198,  0.1403],\n",
      "        [-0.2164, -0.4061,  0.0000,  0.0000],\n",
      "        [-0.1914, -0.3875,  0.0000,  0.0000],\n",
      "        [-0.2018, -0.3953,  0.1023,  0.1904],\n",
      "        [-0.1161, -0.3317,  0.0000,  0.0000],\n",
      "        [-0.1274, -0.3394,  0.0000,  0.0000],\n",
      "        [-0.1345, -0.3382,  0.0000,  0.0000],\n",
      "        [-0.0775, -0.3030,  0.0000,  0.0000],\n",
      "        [-0.1598, -0.3504,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000, 0.0721, 0.3291, 0.0000],\n",
      "        [0.0000, 0.0505, 0.0783, 0.0000],\n",
      "        [0.0000, 0.1991, 0.0450, 0.0000],\n",
      "        [0.0000, 0.1391, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0454, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0438, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0004, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0175, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1568, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0136, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1565, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0739, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0789, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2398, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2452, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2693, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2675, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2955, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2927, 0.0000, 0.0051],\n",
      "        [0.0000, 0.3340, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2804, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3028, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1194, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1433, 0.0138, 0.0000],\n",
      "        [0.0000, 0.1570, 0.1535, 0.0000],\n",
      "        [0.0000, 0.0368, 0.0000, 0.0000],\n",
      "        [0.0000, 0.2097, 0.2984, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
      "i: 1\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "Indices tensor([41, 39, 34, 32, 28])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000e+00, 2.1592e-01, 2.2413e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1781e-01, 1.0757e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.9809e-02, 2.4540e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2396e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5826e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.2865e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3435e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4726e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0744e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9319e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0950e-01, 0.0000e+00, 1.6729e-02],\n",
      "        [0.0000e+00, 3.9323e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4145e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.2273e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.6620e-01, 0.0000e+00, 1.7102e-02],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.5995e-01, 3.2607e-02, 1.0858e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Condition before torch.Size([46, 2])\n",
      "tensor([[-0.1624, -0.3557],\n",
      "        [-0.0843, -0.3045],\n",
      "        [-0.1159, -0.3266],\n",
      "        [-0.0931, -0.3135],\n",
      "        [-0.0815, -0.3060],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0402, -0.2892],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0601, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0685, -0.2964],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1035, -0.3223],\n",
      "        [-0.0667, -0.2950],\n",
      "        [-0.1343, -0.3452],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0944, -0.3156],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.1698, -0.3715],\n",
      "        [-0.1759, -0.3760],\n",
      "        [-0.1621, -0.3658],\n",
      "        [-0.1573, -0.3622],\n",
      "        [-0.1973, -0.3919],\n",
      "        [-0.2059, -0.3965],\n",
      "        [-0.2441, -0.4266],\n",
      "        [-0.0856, -0.3091],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.2111, -0.4022],\n",
      "        [-0.2792, -0.4508],\n",
      "        [-0.1621, -0.3658],\n",
      "        [-0.3285, -0.4761],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903],\n",
      "        [-0.0603, -0.2903]], grad_fn=<AddmmBackward0>)\n",
      "Encodings torch.Size([5, 2])\n",
      "tensor([[0.1198, 0.1403],\n",
      "        [0.1196, 0.1054],\n",
      "        [0.1080, 0.1321],\n",
      "        [0.1023, 0.1904],\n",
      "        [0.1013, 0.2107]], grad_fn=<SiluBackward0>)\n",
      "Condition after torch.Size([46, 4])\n",
      "tensor([[-0.1624, -0.3557,  0.0000,  0.0000],\n",
      "        [-0.0843, -0.3045,  0.0000,  0.0000],\n",
      "        [-0.1159, -0.3266,  0.0000,  0.0000],\n",
      "        [-0.0931, -0.3135,  0.0000,  0.0000],\n",
      "        [-0.0815, -0.3060,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0402, -0.2892,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0601, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0685, -0.2964,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1035, -0.3223,  0.0000,  0.0000],\n",
      "        [-0.0667, -0.2950,  0.0000,  0.0000],\n",
      "        [-0.1343, -0.3452,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0944, -0.3156,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.1698, -0.3715,  0.1013,  0.2107],\n",
      "        [-0.1759, -0.3760,  0.0000,  0.0000],\n",
      "        [-0.1621, -0.3658,  0.0000,  0.0000],\n",
      "        [-0.1573, -0.3622,  0.0000,  0.0000],\n",
      "        [-0.1973, -0.3919,  0.1023,  0.1904],\n",
      "        [-0.2059, -0.3965,  0.0000,  0.0000],\n",
      "        [-0.2441, -0.4266,  0.1080,  0.1321],\n",
      "        [-0.0856, -0.3091,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.2111, -0.4022,  0.0000,  0.0000],\n",
      "        [-0.2792, -0.4508,  0.1196,  0.1054],\n",
      "        [-0.1621, -0.3658,  0.0000,  0.0000],\n",
      "        [-0.3285, -0.4761,  0.1198,  0.1403],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000],\n",
      "        [-0.0603, -0.2903,  0.0000,  0.0000]], grad_fn=<CopySlices>)\n",
      "Edge Index torch.Size([2, 244])\n",
      "tensor([[ 0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
      "          3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  6,  6,  6,  7,  7,\n",
      "          7,  8,  8,  8,  8,  9,  9,  9,  9,  9, 10, 10, 10, 10, 11, 11, 11, 11,\n",
      "         11, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 14, 14, 14, 15, 15,\n",
      "         15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18,\n",
      "         18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21,\n",
      "         21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23,\n",
      "         23, 23, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 26, 26, 26, 26,\n",
      "         27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "         29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 32,\n",
      "         32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35,\n",
      "         35, 35, 35, 36, 36, 36, 36, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 39,\n",
      "         39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 42, 42, 42, 42, 42, 43, 43, 43,\n",
      "         44, 44, 44, 44, 44, 44, 45, 45, 45, 45],\n",
      "        [ 1,  2, 41,  0,  2,  3,  4,  5,  0,  1,  3, 19, 30, 33,  1,  2,  4,  5,\n",
      "         19, 30, 33,  1,  3,  5, 16, 19, 20,  1,  3,  4,  6,  5,  7,  8,  6,  8,\n",
      "         15,  6,  7,  9, 10,  8, 10, 11, 12, 13,  8,  9, 11, 14,  9, 10, 12, 13,\n",
      "         14,  9, 11, 13, 24, 27, 44,  9, 11, 12, 10, 11, 18, 22, 23, 24,  7, 16,\n",
      "         17, 18,  4, 15, 17, 18, 19, 20, 15, 16, 18, 20, 21, 22, 14, 15, 16, 17,\n",
      "         22, 23, 24,  2,  3,  4, 16, 20, 30, 33,  4, 16, 17, 19, 21, 22, 17, 20,\n",
      "         22, 28, 29, 30, 31, 32, 14, 17, 18, 20, 21, 23, 24, 14, 18, 22, 24, 25,\n",
      "         26, 27, 12, 14, 18, 22, 23, 27, 44, 23, 26, 27, 36, 37, 23, 25, 27, 28,\n",
      "         12, 23, 24, 25, 26, 44, 21, 26, 29, 30, 31, 32, 21, 28, 30, 31, 32, 35,\n",
      "         38,  2,  3, 19, 21, 28, 29, 31, 32, 33, 21, 28, 29, 30, 32, 35, 36, 21,\n",
      "         28, 29, 30, 31, 33, 34,  2,  3, 19, 30, 32, 34, 32, 33, 38, 39, 40, 29,\n",
      "         31, 36, 38, 25, 31, 35, 37, 25, 36, 42, 44, 45, 29, 34, 35, 39, 40, 34,\n",
      "         38, 40, 41, 34, 38, 39, 42, 43,  0, 39, 37, 40, 43, 44, 45, 40, 42, 45,\n",
      "         12, 24, 27, 37, 42, 45, 37, 42, 43, 44]])\n",
      "x torch.Size([46, 4])\n",
      "tensor([[0.0000e+00, 2.1592e-01, 2.2413e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 5.0489e-02, 7.8265e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 1.1781e-01, 1.0757e-01, 0.0000e+00],\n",
      "        [0.0000e+00, 6.9809e-02, 2.4540e-02, 0.0000e+00],\n",
      "        [0.0000e+00, 4.5402e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3846e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0536e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.7520e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.2396e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.3555e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.5826e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 7.2865e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.3435e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.4726e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.0744e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.9319e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.0950e-01, 0.0000e+00, 1.6729e-02],\n",
      "        [0.0000e+00, 3.9323e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.4145e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 3.2273e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.6620e-01, 0.0000e+00, 1.7102e-02],\n",
      "        [0.0000e+00, 2.1774e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.5995e-01, 3.2607e-02, 1.0858e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in old_train_data_loader:\n",
    "    history_edge_features = data[\"history_edge_features\"]\n",
    "    #print(\"x\", history_edge_features.size())\n",
    "    history_indices = data[\"history_indices\"]\n",
    "    future_edge_indices_one_hot = data[\"future_edge_features\"][:, :, 0]\n",
    "    t = torch.tensor([34, 10])\n",
    "    for i in range(min(2, history_edge_features.size(0))):\n",
    "        print(\"i:\", i)\n",
    "        #print(history_indices[i].unsqueeze(0))\n",
    "        #print(history_edge_features[i].unsqueeze(0))\n",
    "        \n",
    "        c = model.forward(x=history_edge_features[i].unsqueeze(0), edge_index=train_dataset.edge_index, indices=history_indices[i], mode='history')\n",
    "        out = model.forward(history_edge_features[i].unsqueeze(0), train_dataset.edge_index, t=t[i].unsqueeze(0), condition=c, mode='future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "# Define a simple dataset with two graphs\n",
    "data_list = [\n",
    "    Data(x=torch.randn(3, 3), edge_index=torch.tensor([[0, 1, 2], [1, 2, 0]], dtype=torch.long)),\n",
    "    Data(x=torch.randn(2, 3), edge_index=torch.tensor([[0, 1], [1, 0]], dtype=torch.long))\n",
    "]\n",
    "\n",
    "# Create a DataLoader with batch size of 2\n",
    "loader = DataLoader(data_list, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define a simple GATv2Conv model\n",
    "class SimpleGAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SimpleGAT, self).__init__()\n",
    "        self.conv = GATv2Conv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        print(\"Edge index\", edge_index.size())\n",
    "        print(edge_index)\n",
    "        x = self.conv(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleGAT(in_channels=3, out_channels=2)\n",
    "\n",
    "# Inspect the batched data\n",
    "for batch in loader:\n",
    "    #print(\"Batched x shape:\", batch.x.shape)\n",
    "    #print(\"Batched edge_index shape:\", batch.edge_index.shape)\n",
    "    #print(\"Batch tensor shape:\", batch.batch.shape)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch.x, batch.edge_index, batch.batch)\n",
    "    #print(\"Model output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_G = nx.line_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(map(sorted, line_G.edges())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
