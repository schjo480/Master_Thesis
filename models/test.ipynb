{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "nodes = [(0, {'pos': (0.1, 0.65)}),\n",
    "         (1, {'pos': (0.05, 0.05)}), \n",
    "         (2, {'pos': (0.2, 0.15)}), \n",
    "         (3, {'pos': (0.55, 0.05)}),\n",
    "         (4, {'pos': (0.8, 0.05)}),\n",
    "         (5, {'pos': (0.9, 0.1)}),\n",
    "         (6, {'pos': (0.75, 0.15)}),\n",
    "         (7, {'pos': (0.5, 0.2)}),\n",
    "         (8, {'pos': (0.3, 0.3)}),\n",
    "         (9, {'pos': (0.2, 0.3)}),\n",
    "         (10, {'pos': (0.3, 0.4)}),\n",
    "         (11, {'pos': (0.65, 0.35)}),\n",
    "         (12, {'pos': (0.8, 0.5)}),\n",
    "         (13, {'pos': (0.5, 0.5)}),\n",
    "         (14, {'pos': (0.4, 0.65)}),\n",
    "         (15, {'pos': (0.15, 0.6)}),\n",
    "         (16, {'pos': (0.3, 0.7)}),\n",
    "         (17, {'pos': (0.5, 0.7)}),\n",
    "         (18, {'pos': (0.8, 0.8)}),\n",
    "         (19, {'pos': (0.4, 0.8)}),\n",
    "         (20, {'pos': (0.25, 0.85)}),\n",
    "         (21, {'pos': (0.1, 0.9)}),\n",
    "         (22, {'pos': (0.2, 0.95)}),\n",
    "         (23, {'pos': (0.45, 0.9)}),\n",
    "         (24, {'pos': (0.95, 0.95)}),\n",
    "         (25, {'pos': (0.9, 0.4)}),\n",
    "         (26, {'pos': (0.95, 0.05)})]\n",
    "edges = [(0, 21), (0, 1), (0, 15), (21, 22), (22, 20), (20, 23), (23, 24), (24, 18), (19, 14), (14, 15), (15, 16), (16, 20), (19, 20), (19, 17), (14, 17), (14, 16), (17, 18), (12, 18), (12, 13), (13, 14), (10, 14), (1, 15), (9, 15), (1, 9), (1, 2), (11, 12), (9, 10), (3, 7), (2, 3), (7, 8), (8, 9), (8, 10), (10, 11), (8, 11), (6, 11), (3, 4), (4, 5), (4, 6), (5, 6), (24, 25), (12, 25), (5, 25), (11, 25), (5, 26)]\n",
    "\n",
    "def visualize_predictions(samples, ground_truth_hist, ground_truth_fut, num_samples=5):\n",
    "        \"\"\"\n",
    "        Visualize the predictions of the model along with ground truth data.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        :param num_samples: Number of samples to visualize.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import networkx as nx\n",
    "        '''save_dir = f'{os.path.join(model_dir, f'{exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)'''\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(nodes)\n",
    "        all_edges = {tuple(edges[idx]) for idx in range(len(edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "        \n",
    "        pos = nx.get_node_attributes(G, 'pos')  # Retrieve node positions stored in node attributes\n",
    "\n",
    "        for i in range(min(num_samples, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "                subgraph_edges = {tuple(edges[idx]) for idx in edge_indices if idx < len(edges)}\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'# if plot_num == 0 else 'lightgreen' if plot_num == 1 else 'orange'\n",
    "                nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m ground_truth_hist \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m6\u001b[39m], [\u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m23\u001b[39m], [\u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m29\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m35\u001b[39m]] \n\u001b[1;32m      3\u001b[0m ground_truth_fut \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m28\u001b[39m], [\u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m43\u001b[39m]]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_fut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mvisualize_predictions\u001b[0;34m(samples, ground_truth_hist, ground_truth_fut, num_samples)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mVisualize the predictions of the model along with ground truth data.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m:param num_samples: Number of samples to visualize.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''save_dir = f'{os.path.join(model_dir, f'{exp_name}', 'plots')}'\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mos.makedirs(save_dir, exist_ok=True)'''\u001b[39;00m\n\u001b[1;32m     45\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph()\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/lib/python3.12/site-packages/networkx/__init__.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadwrite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Need to test with SciPy, when available\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/lib/python3.12/site-packages/networkx/algorithms/__init__.py:76\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m community\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coloring\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flow\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m isomorphism\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m link_analysis\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/lib/python3.12/site-packages/networkx/algorithms/flow/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaxflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmincost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboykovkolmogorov\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/lib/python3.12/site-packages/networkx/algorithms/flow/maxflow.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdinitz_alg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dinitz\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medmondskarp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m edmonds_karp\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreflowpush\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preflow_push\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshortestaugmentingpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shortest_augmenting_path\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_flow_dict\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "samples = [[12, 13, 16], [4, 3], [21, 10], [37, 34]]\n",
    "ground_truth_hist = [[1, 0, 3, 4, 5], [32, 25, 17, 7, 6], [11, 15, 20, 26, 23], [22, 30, 29, 27, 35]] \n",
    "ground_truth_fut = [[6, 7], [5, 4], [24, 28], [36, 43]]\n",
    "\n",
    "visualize_predictions(samples, ground_truth_hist, ground_truth_fut, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tqdm import tqdm\n",
    "TDRIVE_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive.h5'\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        \n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "                if 'edge_orientation' in path:\n",
    "                    path['edge_orientations'] = path.pop('edge_orientation')\n",
    "                paths.append(path)\n",
    "    nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "    \n",
    "    return paths, nodes, edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7218/7218 [00:13<00:00, 542.96it/s]\n"
     ]
    }
   ],
   "source": [
    "paths, nodes, edges = load_new_format(TDRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 6154],\n",
       "       [   0, 8268],\n",
       "       [   0, 3616],\n",
       "       ...,\n",
       "       [9737, 9751],\n",
       "       [9747, 9805],\n",
       "       [9759, 9809]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "lr = 0.01\n",
    "num_epochs = 50000\n",
    "learning_rate_warmup_steps = 2500\n",
    "lr_decay_parameter = 0.9998\n",
    "\n",
    "# Learning rate schedule\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < learning_rate_warmup_steps:\n",
    "        return 1.0\n",
    "    else:\n",
    "        decay_lr = lr_decay_parameter ** (epoch - learning_rate_warmup_steps)\n",
    "        return max(decay_lr, 2e-5 / lr)\n",
    "\n",
    "\n",
    "# Calculate learning rates for each epoch\n",
    "learning_rates = [lr * lr_lambda(epoch) for epoch in range(num_epochs)]\n",
    "\n",
    "# Plot the learning rates\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, label='Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')#\n",
    "plt.yscale('log')\n",
    "plt.title('Learning Rate Schedule Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [(0, {'pos': (0.1, 0.65)}),\n",
    "         (1, {'pos': (0.05, 0.05)}), \n",
    "         (2, {'pos': (0.2, 0.15)}), \n",
    "         (3, {'pos': (0.55, 0.05)}),\n",
    "         (4, {'pos': (0.8, 0.05)}),\n",
    "         (5, {'pos': (0.9, 0.1)}),\n",
    "         (6, {'pos': (0.75, 0.15)}),\n",
    "         (7, {'pos': (0.5, 0.2)}),\n",
    "         (8, {'pos': (0.3, 0.3)}),\n",
    "         (9, {'pos': (0.2, 0.3)}),\n",
    "         (10, {'pos': (0.3, 0.4)}),\n",
    "         (11, {'pos': (0.65, 0.35)}),\n",
    "         (12, {'pos': (0.8, 0.5)}),\n",
    "         (13, {'pos': (0.5, 0.5)}),\n",
    "         (14, {'pos': (0.4, 0.65)}),\n",
    "         (15, {'pos': (0.15, 0.6)}),\n",
    "         (16, {'pos': (0.3, 0.7)}),\n",
    "         (17, {'pos': (0.5, 0.7)}),\n",
    "         (18, {'pos': (0.8, 0.8)}),\n",
    "         (19, {'pos': (0.4, 0.8)}),\n",
    "         (20, {'pos': (0.25, 0.85)}),\n",
    "         (21, {'pos': (0.1, 0.9)}),\n",
    "         (22, {'pos': (0.2, 0.95)}),\n",
    "         (23, {'pos': (0.45, 0.9)}),\n",
    "         (24, {'pos': (0.95, 0.95)}),\n",
    "         (25, {'pos': (0.9, 0.4)}),\n",
    "         (26, {'pos': (0.95, 0.05)}),\n",
    "         (27, {'pos': (0.75, 1.0)})]\n",
    "edges = [(0, 21), (0, 1), (0, 15), (21, 22), (22, 20), (20, 23), (23, 24), (24, 18), (19, 14), (14, 15), (15, 16), (16, 20), (19, 20), (19, 17), (14, 17), (14, 16), (17, 18), (12, 18), (12, 13), (13, 14), (10, 14), (1, 15), (9, 15), (1, 9), (1, 2), (11, 12), (9, 10), (3, 7), (2, 3), (7, 8), (8, 9), (8, 10), (10, 11), (8, 11), (6, 11), (3, 4), (4, 5), (4, 6), (5, 6), (24, 25), (12, 25), (5, 25), (11, 25), (5, 26), (23, 27), (24, 27)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "res = {'sample_list': [tensor([ 0,  2,  7, 11, 15, 20, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2,  7, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 30, 34, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37]), tensor([ 0,  2, 23, 24, 30]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37, 45]), tensor([ 0,  2, 23, 24, 30, 34, 36, 37]), tensor([ 4, 12, 17, 18, 21, 24, 40, 41]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2,  7, 11, 15, 20, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 35, 36, 37]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 4, 12, 17, 18, 19, 21, 24, 40, 41]), tensor([ 0,  2,  7, 23, 24, 26, 29, 30, 34, 35, 36, 37, 39, 45]), tensor([ 0,  2, 23, 24, 30, 34, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37]), tensor([ 0,  2, 23, 24, 26, 29, 30, 34, 36, 37])], 'ground_truth_hist': [tensor([[43, 36, 37, 34, 32]]), tensor([[23, 26, 20, 14, 16]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[39, 41, 36, 35, 27]]), tensor([[44,  5, 11, 10, 22]]), tensor([[24, 28, 35, 36, 41]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[35, 27, 29, 30, 22]]), tensor([[39, 42, 33, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[38, 34, 25, 18, 19]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[10, 15, 14, 16, 17]]), tensor([[26, 32, 34, 38, 41]]), tensor([[45,  7, 17, 25, 33]]), tensor([[37, 34, 32, 26, 22]]), tensor([[ 2, 22, 26, 32, 34]])], 'ground_truth_fut': [tensor([[20, 15]]), tensor([[ 7, 45]]), tensor([[12,  4]]), tensor([[34, 37]]), tensor([[29, 30]]), tensor([[23, 24]]), tensor([[40, 17]]), tensor([[36, 35]]), tensor([[2, 0]]), tensor([[21, 24]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[15, 11]]), tensor([[26, 23]]), tensor([[18, 40]]), tensor([[40, 41]]), tensor([[39, 45]]), tensor([[30, 23]]), tensor([[2, 0]]), tensor([[37, 36]])]}\n",
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2915)\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import F1Score\n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1900)\n"
     ]
    }
   ],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [[tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40]), tensor([17, 37, 40])], [tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([ 0,  1,  2, 16, 20, 21, 26])], [tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37]), tensor([37])], [tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36]), tensor([10, 11, 18, 32, 36])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40]), tensor([11, 18, 32, 36, 39, 40])], [tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34])], [tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40]), tensor([18, 40])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40]), tensor([11, 18, 32, 36, 40])], [tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41]), tensor([40, 41])], [tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19]), tensor([15, 18, 19])], [tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40]), tensor([11, 32, 36, 39, 40])], [tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45])], [tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18]), tensor([18])], [tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45]), tensor([15, 18, 19, 45])]], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
      "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
      "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
      "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])]\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "tensor(0.2393)\n"
     ]
    }
   ],
   "source": [
    "samples = [res['sample_list'][i][0] for i in range(len(res['sample_list']))]\n",
    "print(samples)\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1_tot = 0\n",
    "for i in range(len(res['sample_list'])):\n",
    "    f1 = metric(one_hot_samples[i], one_hot_futures[i])\n",
    "    f1_tot += f1\n",
    "f1 = f1_tot / len(res['sample_list'])\n",
    "print(one_hot_samples[0])\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of equal samples for sample 0: 1.0\n",
      "Ratio of equal samples for sample 1: 1.0\n",
      "Ratio of equal samples for sample 2: 1.0\n",
      "Ratio of equal samples for sample 3: 1.0\n",
      "Ratio of equal samples for sample 4: 1.0\n",
      "Ratio of equal samples for sample 5: 1.0\n",
      "Ratio of equal samples for sample 6: 1.0\n",
      "Ratio of equal samples for sample 7: 1.0\n",
      "Ratio of equal samples for sample 8: 1.0\n",
      "Ratio of equal samples for sample 9: 1.0\n"
     ]
    }
   ],
   "source": [
    "sample_10 = [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])]\n",
    "sample_1 = [tensor([17, 37, 40]), tensor([ 0,  1,  2,  6,  7, 16, 19, 20, 21, 26, 29, 38, 39, 45]), tensor([15, 18, 19]), tensor([ 0,  1,  2, 16, 20, 21, 26]), tensor([37]), tensor([10, 11, 18, 32, 36]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([11, 18, 32, 36, 39, 40]), tensor([ 0,  1,  2, 13, 21, 24, 27, 28, 29, 30, 34]), tensor([16, 17, 23, 24, 34, 37, 40, 43, 44, 45]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18, 40]), tensor([15, 18, 19]), tensor([11, 18, 32, 36, 40]), tensor([40, 41]), tensor([15, 18, 19]), tensor([11, 32, 36, 39, 40]), tensor([ 0,  1,  2,  6,  7,  8,  9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 34,\n",
    "        37, 40, 41, 42, 43, 44, 45]), tensor([18]), tensor([15, 18, 19, 45])]\n",
    "for j in range(10):\n",
    "        sample_10 = [res['sample_list'][i][j] for i in range(len(res['sample_list']))]\n",
    "        equal = 0\n",
    "        for i in range(len(sample_10)):\n",
    "                if sample_10[i].equal(sample_1[i]):\n",
    "                        equal += 1\n",
    "                \n",
    "        print(f\"Ratio of equal samples for sample {j}:\", equal/len(sample_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([11, 12, 18, 19]), tensor([18, 19]), tensor([18, 19]), tensor([35, 36]), tensor([11, 12]), tensor([11, 12]), tensor([18, 19]), tensor([41]), tensor([24, 35, 36]), tensor([24, 35, 36]), tensor([11, 12, 19]), tensor([20, 21, 40, 41]), tensor([20, 21, 40, 41]), tensor([20, 21, 41]), tensor([35, 36]), tensor([ 0,  2, 40, 41]), tensor([11, 12]), tensor([35, 36]), tensor([11, 12]), tensor([36])],\n",
    "\n",
    "\n",
    "'ground_truth_hist': [tensor([[45,  7, 17, 25, 33]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[43, 36, 37, 34, 32]]), tensor([[40, 18, 19, 14, 13]]), tensor([[41, 40, 18, 19,  8]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[37, 34, 32, 26, 22]]), tensor([[35, 27, 29, 30, 22]]), tensor([[39, 42, 33, 30, 22]]), tensor([[24, 28, 35, 36, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[39, 41, 36, 35, 27]]), tensor([[26, 32, 34, 38, 41]]), tensor([[38, 34, 25, 18, 19]]), tensor([[44,  5, 11, 10, 22]])],\n",
    "\n",
    "\n",
    "'ground_truth_fut': [tensor([[30, 23]]), tensor([[26, 23]]), tensor([[19, 18]]), tensor([[20, 15]]), tensor([[12,  4]]), tensor([[12,  4]]), tensor([[40, 41]]), tensor([[34, 37]]), tensor([[36, 35]]), tensor([[ 7, 45]]), tensor([[18, 40]]), tensor([[2, 0]]), tensor([[2, 0]]), tensor([[21, 24]]), tensor([[40, 17]]), tensor([[37, 36]]), tensor([[29, 30]]), tensor([[39, 45]]), tensor([[15, 11]]), tensor([[23, 24]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1778)\n"
     ]
    }
   ],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "            \n",
    "metric = F1Score(task='binary', average='micro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples).reshape(20, 46), torch.cat(one_hot_futures).reshape(20, 46))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat(one_hot_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(torch.cat(one_hot_samples).reshape(20, 46)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_Future = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "Predicted_Future = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
    "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
    "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8889)\n"
     ]
    }
   ],
   "source": [
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1 = metric(Predicted_Future, True_Future)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {'sample_list': [tensor([], dtype=torch.int64), tensor([ 7, 22, 23]), tensor([], dtype=torch.int64), tensor([ 4,  5, 34, 36, 37]), tensor([0]), tensor([10, 11, 15, 18, 19]), tensor([0]), tensor([0]), tensor([21, 23, 39, 45]), tensor([ 0, 29, 30, 36]), tensor([0, 4]), tensor([0]), tensor([0]), tensor([17, 35, 40]), tensor([], dtype=torch.int64), tensor([0]), tensor([0]), tensor([ 4,  5, 34, 36, 37]), tensor([21, 23, 24, 39, 40, 45]), tensor([21, 30])], 'ground_truth_hist': [tensor([[24, 28, 35, 36, 41]]), tensor([[23, 26, 20, 14, 16]]), tensor([[ 7, 16, 13,  8, 20]]), tensor([[ 5, 12,  8, 20, 32]]), tensor([[43, 36, 37, 34, 32]]), tensor([[38, 34, 25, 18, 19]]), tensor([[35, 27, 29, 30, 22]]), tensor([[41, 40, 18, 19,  8]]), tensor([[39, 42, 33, 30, 22]]), tensor([[39, 41, 36, 35, 27]]), tensor([[37, 34, 32, 26, 22]]), tensor([[40, 18, 19, 14, 13]]), tensor([[ 0,  3,  4, 11, 15]]), tensor([[ 9, 19, 18, 40, 41]]), tensor([[10, 15, 14, 16, 17]]), tensor([[ 3,  4, 11, 15, 19]]), tensor([[26, 32, 34, 38, 41]]), tensor([[ 2, 22, 26, 32, 34]]), tensor([[44,  5, 11, 10, 22]]), tensor([[45,  7, 17, 25, 33]])], 'ground_truth_fut': [tensor([[40, 17]]), tensor([[ 7, 45]]), tensor([[26, 23]]), tensor([[34, 37]]), tensor([[20, 15]]), tensor([[15, 11]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[21, 24]]), tensor([[29, 30]]), tensor([[2, 0]]), tensor([[12,  4]]), tensor([[19, 18]]), tensor([[36, 35]]), tensor([[40, 41]]), tensor([[18, 40]]), tensor([[39, 45]]), tensor([[37, 36]]), tensor([[23, 24]]), tensor([[30, 23]])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3721)\n"
     ]
    }
   ],
   "source": [
    "samples = res['sample_list']\n",
    "one_hot_samples = [torch.zeros(len(edges)) for _ in range(len(samples))]\n",
    "ground_truth_fut = res['ground_truth_fut']\n",
    "one_hot_futures = [torch.zeros(len(edges)) for _ in range(len(ground_truth_fut))]\n",
    "for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in samples[i]:\n",
    "            one_hot_sample[edges.index(edge)] = 1\n",
    "for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "    for edge_index, edge in enumerate(edges):\n",
    "        if edge_index in ground_truth_fut[i]:\n",
    "            one_hot_fut[edges.index(edge)] = 1\n",
    "metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9823, 0.0177],\n",
       "        [0.0177, 0.9823]], dtype=torch.float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_gaussian_transition_mat(t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition matrix Q with\n",
    "        decaying entries as a function of how far off diagonal the entry is.\n",
    "        Normalization option 1:\n",
    "        Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                    1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                    0                          else.\n",
    "\n",
    "        Normalization option 2:\n",
    "        tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= transition_bands\n",
    "                            0                        else.\n",
    "\n",
    "        Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        \n",
    "        num_classes = 2\n",
    "        transition_bands = num_classes - 1 # 1\n",
    "\n",
    "        betas = torch.linspace(0.9, 1.0, 1000)\n",
    "        beta_t = betas[t]\n",
    "\n",
    "        mat = torch.zeros((num_classes, num_classes),\n",
    "                        dtype=torch.float64)\n",
    "\n",
    "        # Make the values correspond to a similar type of gaussian as in the\n",
    "        # gaussian diffusion case for continuous state spaces.\n",
    "        values = torch.linspace(torch.tensor(0.), torch.tensor(num_classes-1), num_classes, dtype=torch.float64)\n",
    "        values = values * 2./ (num_classes - 1.)\n",
    "        values = values[:transition_bands+1]\n",
    "        values = -values * values / beta_t\n",
    "        \n",
    "        # To reverse the tensor 'values' starting from the second element\n",
    "        reversed_values = values[1:].flip(dims=[0])\n",
    "        # Concatenating the reversed values with the original values\n",
    "        values = torch.cat([reversed_values, values], dim=0)\n",
    "        values = F.softmax(values, dim=0)\n",
    "        values = values[transition_bands:]\n",
    "        \n",
    "        for k in range(1, transition_bands + 1):\n",
    "            off_diag = torch.full((num_classes - k,), values[k], dtype=torch.float64)\n",
    "\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "\n",
    "        # Add diagonal values such that rows and columns sum to one.\n",
    "        # Technically only the ROWS need to sum to one\n",
    "        # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "        # which is necessary if we want to have a uniform stationary distribution.\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag_embed(diag)\n",
    "\n",
    "        return mat\n",
    "\n",
    "_get_gaussian_transition_mat(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9990, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9001, 0.0999],\n",
       "        [0.8991, 0.1009]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_prior_distribution_transition_mat(t):\n",
    "    \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "    Args:\n",
    "    t: timestep. integer scalar.\n",
    "\n",
    "    Returns:\n",
    "    Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    betas = torch.linspace(0.001, 0.2, 1000)\n",
    "    beta_t = betas[t]\n",
    "    steps = torch.linspace(0, 1, 1000 + 1, dtype=torch.float64)\n",
    "    alpha_bar = torch.cos((steps + 0.008) / 1.008 * torch.pi / 2)\n",
    "    betas = torch.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], torch.tensor(0.999))\n",
    "    beta_t = betas[t]\n",
    "    print(beta_t)\n",
    "    num_classes = 2\n",
    "    class_weights = [0.9, 0.1]\n",
    "    mat = torch.zeros((num_classes, num_classes), dtype=torch.float64)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                mat[i, j] = beta_t * class_weights[j]\n",
    "            else:\n",
    "                mat[i, j] = 1 - beta_t + beta_t * class_weights[j]\n",
    "    \n",
    "    return mat\n",
    "\n",
    "_get_prior_distribution_transition_mat(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005]], dtype=torch.float64)\n",
      "Transition matrix for edges (q_e) at the final timestep:\n",
      "tensor([[0.9444, 0.0556],\n",
      "        [0.5000, 0.5000]], dtype=torch.float64)\n",
      "tensor(0.3333, dtype=torch.float64)\n",
      "tensor([[0.9667, 0.0333],\n",
      "        [0.3000, 0.7000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def cosine_beta_schedule_discrete(timesteps, s=0.008):\n",
    "    \"\"\" Cosine schedule as proposed in https://openreview.net/forum?id=-NEXDKk8gZ. \"\"\"\n",
    "    steps = timesteps + 2\n",
    "    x = np.linspace(0, steps, steps)\n",
    "\n",
    "    alphas_cumprod = np.cos(0.5 * np.pi * ((x / steps) + s) / (1 + s)) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    alphas = (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    betas = 1 - alphas\n",
    "    return torch.tensor(betas, dtype=torch.float64)\n",
    "\n",
    "class PlaceHolder:\n",
    "    def __init__(self, X, E, y):\n",
    "        self.X = X\n",
    "        self.E = E\n",
    "        self.y = y\n",
    "\n",
    "    def type_as(self, x: torch.Tensor):\n",
    "        \"\"\" Changes the device and dtype of X, E, y. \"\"\"\n",
    "        self.X = self.X.type_as(x)\n",
    "        self.E = self.E.type_as(x)\n",
    "        self.y = self.y.type_as(x)\n",
    "        return self\n",
    "\n",
    "    def mask(self, node_mask, collapse=False):\n",
    "        x_mask = node_mask.unsqueeze(-1)          # bs, n, 1\n",
    "        e_mask1 = x_mask.unsqueeze(2)             # bs, n, 1, 1\n",
    "        e_mask2 = x_mask.unsqueeze(1)             # bs, 1, n, 1\n",
    "\n",
    "        if collapse:\n",
    "            self.X = torch.argmax(self.X, dim=-1)\n",
    "            self.E = torch.argmax(self.E, dim=-1)\n",
    "\n",
    "            self.X[node_mask == 0] = - 1\n",
    "            self.E[(e_mask1 * e_mask2).squeeze(-1) == 0] = - 1\n",
    "        else:\n",
    "            self.X = self.X * x_mask\n",
    "            self.E = self.E * e_mask1 * e_mask2\n",
    "            assert torch.allclose(self.E, torch.transpose(self.E, 1, 2))\n",
    "        return self\n",
    "\n",
    "class DiscreteUniformTransition:\n",
    "    def __init__(self, x_classes: int, e_classes: int, y_classes: int):\n",
    "        self.X_classes = x_classes\n",
    "        self.E_classes = e_classes\n",
    "        self.y_classes = y_classes\n",
    "        self.u_x = torch.ones(1, self.X_classes, self.X_classes)\n",
    "        if self.X_classes > 0:\n",
    "            self.u_x = self.u_x / self.X_classes\n",
    "\n",
    "        self.u_e = torch.ones(1, self.E_classes, self.E_classes)\n",
    "        if self.E_classes > 0:\n",
    "            self.u_e = self.u_e / self.E_classes\n",
    "\n",
    "        self.u_y = torch.ones(1, self.y_classes, self.y_classes)\n",
    "        if self.y_classes > 0:\n",
    "            self.u_y = self.u_y / self.y_classes\n",
    "\n",
    "    def get_Qt(self, beta_t, device):\n",
    "        \"\"\" Returns one-step transition matrices for X and E, from step t - 1 to step t.\n",
    "        Qt = (1 - beta_t) * I + beta_t / K\n",
    "\n",
    "        beta_t: (bs)                         noise level between 0 and 1\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy).\n",
    "        \"\"\"\n",
    "        beta_t = beta_t.unsqueeze(1).unsqueeze(1)\n",
    "        beta_t = beta_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = beta_t * self.u_x + (1 - beta_t) * torch.eye(self.X_classes, device=device).unsqueeze(0)\n",
    "        q_e = beta_t * self.u_e + (1 - beta_t) * torch.eye(self.E_classes, device=device).unsqueeze(0)\n",
    "        q_y = beta_t * self.u_y + (1 - beta_t) * torch.eye(self.y_classes, device=device).unsqueeze(0)\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n",
    "    \n",
    "class MarginalUniformTransition:\n",
    "    def __init__(self, x_marginals, e_marginals, y_classes):\n",
    "        self.X_classes = len(x_marginals)\n",
    "        self.E_classes = len(e_marginals)\n",
    "        self.y_classes = y_classes\n",
    "        self.x_marginals = x_marginals\n",
    "        self.e_marginals = e_marginals\n",
    "\n",
    "        self.u_x = x_marginals.unsqueeze(0).expand(self.X_classes, -1).unsqueeze(0)\n",
    "        self.u_e = e_marginals.unsqueeze(0).expand(self.E_classes, -1).unsqueeze(0)\n",
    "        self.u_y = torch.ones(1, self.y_classes, self.y_classes)\n",
    "        if self.y_classes > 0:\n",
    "            self.u_y = self.u_y / self.y_classes\n",
    "\n",
    "    def get_Qt(self, beta_t, device):\n",
    "        \"\"\" Returns one-step transition matrices for X and E, from step t - 1 to step t.\n",
    "        Qt = (1 - beta_t) * I + beta_t / K\n",
    "\n",
    "        beta_t: (bs)                         noise level between 0 and 1\n",
    "        returns: qx (bs, dx, dx), qe (bs, de, de), qy (bs, dy, dy). \"\"\"\n",
    "        beta_t = beta_t.unsqueeze(1).unsqueeze(1)\n",
    "        print(beta_t[100])\n",
    "        beta_t = beta_t.to(device)\n",
    "        self.u_x = self.u_x.to(device)\n",
    "        self.u_e = self.u_e.to(device)\n",
    "        self.u_y = self.u_y.to(device)\n",
    "\n",
    "        q_x = beta_t * self.u_x + (1 - beta_t) * torch.eye(self.X_classes, device=device).unsqueeze(0)\n",
    "        q_e = beta_t * self.u_e + (1 - beta_t) * torch.eye(self.E_classes, device=device).unsqueeze(0)\n",
    "        q_y = beta_t * self.u_y + (1 - beta_t) * torch.eye(self.y_classes, device=device).unsqueeze(0)\n",
    "\n",
    "        return PlaceHolder(X=q_x, E=q_e, y=q_y)\n",
    "\n",
    "# Example usage\n",
    "timesteps = 1000\n",
    "betas = cosine_beta_schedule_discrete(timesteps)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transition = MarginalUniformTransition(x_marginals=torch.tensor([0.5, 0.5]), e_marginals=torch.tensor([0.9, 0.1]), y_classes=2)\n",
    "beta_t = betas.to(device)\n",
    "q_t_matrices = transition.get_Qt(beta_t, device)\n",
    "\n",
    "# Print the transition matrix for edges at the final timestep\n",
    "print(\"Transition matrix for edges (q_e) at the final timestep:\")\n",
    "t = 997\n",
    "print(q_t_matrices.E[t+1])\n",
    "print(_get_prior_distribution_transition_mat(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X = 0) = 0.0000057141\n",
      "P(X = 1) = 0.0000694015\n",
      "P(X = 2) = 0.0004210452\n",
      "P(X = 3) = 0.0017012270\n",
      "P(X = 4) = 0.0051501721\n",
      "P(X = 5) = 0.0124604973\n",
      "P(X = 6) = 0.0250975603\n",
      "P(X = 7) = 0.0432856709\n",
      "P(X = 8) = 0.0652570918\n",
      "P(X = 9) = 0.0873617208\n",
      "P(X = 10) = 0.1051523870\n",
      "P(X = 11) = 0.1149439048\n",
      "P(X = 12) = 0.1150602448\n",
      "P(X = 13) = 0.1062094568\n",
      "P(X = 14) = 0.0909445348\n",
      "P(X = 15) = 0.0726083493\n",
      "P(X = 16) = 0.0542909089\n",
      "P(X = 17) = 0.0381678407\n",
      "P(X = 18) = 0.0253164558\n",
      "P(X = 19) = 0.0158922392\n",
      "P(X = 20) = 0.0094677854\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stem() got an unexpected keyword argument 'use_line_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Plot the probabilities\u001b[39;00m\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_line_collection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of successes (n)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProbability\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: stem() got an unexpected keyword argument 'use_line_collection'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters\n",
    "N = 1000  # number of trials\n",
    "p = 0.012  # probability of success\n",
    "n_values = np.arange(0, 21)  # number of successes from 0 to 20\n",
    "\n",
    "# Calculate the probabilities\n",
    "probabilities = binom.pmf(n_values, N, p)\n",
    "\n",
    "# Display the probabilities\n",
    "for n, prob in zip(n_values, probabilities):\n",
    "    print(f\"P(X = {n}) = {prob:.10f}\")\n",
    "\n",
    "# Plot the probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.stem(n_values, probabilities, use_line_collection=True)\n",
    "plt.xlabel('Number of successes (n)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Binomial Distribution PMF (N=1000, p=0.012)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        \n",
    "        # Convert edges to a list of tuples\n",
    "        edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "        self.graph.add_edges_from(self.edges)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # trajectory_name = self.keys[idx]\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        # edge_coordinates_data = trajectory.get('coordinates', [])\n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.stack((history_one_hot_edges, history_edge_orientations), dim=1)\n",
    "            future_edge_features = torch.stack((future_one_hot_edges, future_edge_orientations), dim=1)\n",
    "        else:\n",
    "            history_edge_features = history_one_hot_edges\n",
    "            future_edge_features = future_one_hot_edges\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"node_in_history\": node_in_history,\n",
    "        }, self.graph\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TrajectoryDataset(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_1001_1200.h5\", 5, 2, edge_features=['one_hot_edges', 'coordinates'])\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor([[0, 0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Given tensor\n",
    "original_tensor = torch.tensor([\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Current edge tensor\n",
    "current_edge = torch.tensor([39, 17])\n",
    "\n",
    "# Transforming the tensor\n",
    "def transform_tensor(original_tensor, current_edge):\n",
    "    batch_size = original_tensor.size(0)\n",
    "    max_neighbors = (original_tensor == 1).sum(dim=1).max().item()\n",
    "    transformed_tensor = torch.zeros((batch_size, max_neighbors), dtype=torch.long)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Find indices of neighbors\n",
    "        neighbor_indices = (original_tensor[i] == 1).nonzero(as_tuple=False).squeeze()\n",
    "        # Only keep the index where it matches the current edge\n",
    "        valid_indices = (neighbor_indices == current_edge[i]).nonzero(as_tuple=False).squeeze()    \n",
    "        # Set the values in the transformed tensor\n",
    "        transformed_tensor[i, valid_indices] = 1\n",
    "\n",
    "    return transformed_tensor\n",
    "\n",
    "transformed_tensor = transform_tensor(original_tensor, current_edge)\n",
    "print(transformed_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.transforms import LineGraph\n",
    "#from dataset.trajctory_dataset import TrajectoryDataset, collate_fn\n",
    "#from .d3pm_diffusion import make_diffusion\n",
    "#from .d3pm_edge_encoder import Edge_Encoder\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "\n",
    "class Benchmark_Models(nn.Module):\n",
    "    def __init__(self, data_config, model_config, train_config, test_config, wandb_config, model):\n",
    "        super(Benchmark_Models, self).__init__()\n",
    "        # Data\n",
    "        self.data_config = data_config\n",
    "        self.train_data_path = self.data_config['train_data_path']\n",
    "        self.test_data_path = self.data_config['test_data_path']\n",
    "        self.history_len = self.data_config['history_len']\n",
    "        self.future_len = self.data_config['future_len']\n",
    "        self.num_classes = self.data_config['num_classes']\n",
    "        self.edge_features = self.data_config['edge_features']\n",
    "        \n",
    "        # Model\n",
    "        self.model_config = model_config\n",
    "        self.model = model # Edge_Encoder\n",
    "        self.hidden_channels = self.model_config['hidden_channels']\n",
    "        self.condition_dim = self.model_config['condition_dim']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        \n",
    "        # Training\n",
    "        self.train_config = train_config\n",
    "        self.lr = self.train_config['lr']\n",
    "        self.lr_decay_parameter = self.train_config['lr_decay']\n",
    "        self.learning_rate_warmup_steps = self.train_config['learning_rate_warmup_steps']\n",
    "        self.num_epochs = self.train_config['num_epochs']\n",
    "        self.gradient_accumulation = self.train_config['gradient_accumulation']\n",
    "        self.gradient_accumulation_steps = self.train_config['gradient_accumulation_steps']\n",
    "        self.batch_size = self.train_config['batch_size'] if not self.gradient_accumulation else self.train_config['batch_size'] * self.gradient_accumulation_steps\n",
    "        \n",
    "        # Testing\n",
    "        self.test_config = test_config\n",
    "        self.test_batch_size = self.test_config['batch_size']\n",
    "        self.model_path = self.test_config['model_path']\n",
    "        self.eval_every_steps = self.test_config['eval_every_steps']\n",
    "        \n",
    "        # WandB\n",
    "        self.wandb_config = wandb_config\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            project=self.wandb_config['project'],\n",
    "            entity=self.wandb_config['entity'],\n",
    "            notes=self.wandb_config['notes'],\n",
    "            job_type=self.wandb_config['job_type'],\n",
    "            config={**self.data_config, **self.model_config, **self.train_config}\n",
    "        )\n",
    "        self.exp_name = self.wandb_config['exp_name']\n",
    "        wandb.run.name = self.exp_name\n",
    "\n",
    "        # Logging\n",
    "        self.dataset = self.data_config['dataset']\n",
    "        self.model_dir = os.path.join(\"experiments\", self.exp_name)\n",
    "        os.makedirs(self.model_dir,exist_ok=True)\n",
    "        log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        log_name = f\"{self.dataset}_{log_name}\"\n",
    "        \n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        log_dir = os.path.join(self.model_dir, log_name)\n",
    "        file_handler = logging.FileHandler(log_dir)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.log.addHandler(file_handler)\n",
    "        \n",
    "        self.log_loss_every_steps = self.train_config['log_loss_every_steps']        \n",
    "        \n",
    "        # Build Components\n",
    "        self._build_train_dataloader()\n",
    "        self._build_test_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "            \n",
    "    def train(self):\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        '''def get_neighbors(line_graph, edge, device):\n",
    "            neighbors = []\n",
    "            for e in edge:\n",
    "                neighbor_set = set()\n",
    "                for i in range(line_graph.edge_index.size(1)):\n",
    "                    if line_graph.edge_index[0, i] == e:\n",
    "                        neighbor_set.add(line_graph.edge_index[1, i].item())\n",
    "                    elif line_graph.edge_index[1, i] == e:\n",
    "                        neighbor_set.add(line_graph.edge_index[0, i].item())\n",
    "                neighbors.append(list(neighbor_set))\n",
    "            # Create a binary tensor for neighbors\n",
    "            neighbors_binary = torch.zeros((len(edge), self.num_edges), dtype=torch.long, device=device)\n",
    "            \n",
    "            for i, n in enumerate(neighbors):\n",
    "                neighbors_binary[i, n] = 1\n",
    "            \n",
    "            return neighbors_binary\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            ground_truth_fut = []\n",
    "            pred_fut = []\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            for batch in self.train_data_loader:\n",
    "                history_edge_features = batch[\"history_edge_features\"]\n",
    "                last_history_edge = batch[\"history_indices\"][:, -1]\n",
    "                future_edge_indices = batch[\"future_indices\"]\n",
    "                future_edge_features = batch[\"future_edge_features\"]\n",
    "                future_edge_indices_one_hot = future_edge_features[:, :, 0]\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                visited_edges = [set() for _ in range(history_edge_features.size(0))]   # keep track of visited edges, to avoid cycles\n",
    "                prediction = []\n",
    "                for idx in range(self.future_len):\n",
    "                    future = future_edge_indices_one_hot.clone()\n",
    "                    future.zero_()\n",
    "                    future[:, idx] = 1\n",
    "                    \"\"\"for j in range(future_edge_indices.size(0)):\n",
    "                        future[j, future_edge_indices[j, idx]] = 1\"\"\"\"\"\"\n",
    "                    \n",
    "                    neighbors = get_neighbors(self.line_graph, last_history_edge, device=history_edge_features.device)\n",
    "                    logits, preds = self.model(history_edge_features, neighbors)\n",
    "                    \n",
    "                    # Mask logits for visited edges\n",
    "                    #logits = logits.clone()\n",
    "                    \"\"\"for i in range(len(preds)):\n",
    "                        for visited_edge in visited_edges[i]:\n",
    "                            logits[i, neighbors[i] == visited_edge] = float('-inf')\n",
    "                    \n",
    "                    # Update history and visited edges\n",
    "                    for i in range(len(preds)):\n",
    "                        visited_edges[i].add(preds[i].item())  # Add predicted edge to visited set\n",
    "                        history_edge_features[i] = history_edge_features[i].clone()\n",
    "                        history_edge_features[i, preds[i], 0] = 1\"\"\"  # Add it to history edge features\n",
    "                    last_history_edge = preds   # Update last history edge\n",
    "                    loss = F.binary_cross_entropy_with_logits(logits, future)\n",
    "                    prediction.append(preds)\n",
    "                    loss.backward()\n",
    "\n",
    "                pred_fut.append(prediction)\n",
    "                ground_truth_fut.append(future_edge_indices_one_hot)\n",
    "                # Calculate the loss (cross-entropy)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            #f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach(), torch.flatten(torch.cat(ground_truth_fut)).detach())\n",
    "            # Logging\n",
    "            if (epoch + 1) % self.log_loss_every_steps == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "                wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "                #wandb.log({\"epoch\": epoch + 1, \"average_F1_score\": f1_epoch.item()})\n",
    "                print(pred_fut)\n",
    "                print(future_edge_indices)\n",
    "                self.log.info(f\"Epoch {epoch + 1} Average Loss: {avg_loss}\")\n",
    "                print(\"Epoch:\", epoch + 1)\n",
    "                print(\"Loss:\", avg_loss)\n",
    "                #print(\"F1:\", f1_epoch.item())'''\n",
    "        def transform_tensor(neighbor_tensor, current_edge):\n",
    "            batch_size = neighbor_tensor.size(0)\n",
    "            max_neighbors = (neighbor_tensor == 1).sum(dim=1).max().item()\n",
    "            transformed_tensor = torch.zeros((batch_size, max_neighbors), dtype=torch.long)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Find indices of neighbors\n",
    "                neighbor_indices = (neighbor_tensor[i] == 1).nonzero(as_tuple=False).squeeze()\n",
    "                # Only keep the index where it matches the current edge\n",
    "                valid_indices = (neighbor_indices == current_edge[i]).nonzero(as_tuple=False).squeeze()\n",
    "                # Set the values in the transformed tensor\n",
    "                transformed_tensor[i, valid_indices] = 1\n",
    "\n",
    "            return transformed_tensor\n",
    "        \n",
    "        '''def group_and_pad(tensor, batch_size, pad_value=0):\n",
    "            # Get unique groups\n",
    "            groups = torch.unique(tensor[:, 0])\n",
    "            grouped_sequences = []\n",
    "            \n",
    "            # Find the maximum length of sequences\n",
    "            max_len = 0\n",
    "            for group in groups:\n",
    "                group_elements = tensor[tensor[:, 0] == group][:, 1]\n",
    "                grouped_sequences.append(group_elements)\n",
    "                max_len = max(max_len, len(group_elements))\n",
    "            \n",
    "            # Pad the sequences\n",
    "            padded_sequences = []\n",
    "            for seq in grouped_sequences:\n",
    "                padded_seq = torch.cat([seq, torch.full((max_len - len(seq),), pad_value)])\n",
    "                padded_sequences.append(padded_seq)\n",
    "            \n",
    "            return torch.stack(padded_sequences)'''\n",
    "        \n",
    "        def group_and_pad(tensor, batch_size, pad_value=0, missing_value=-1):\n",
    "            grouped_sequences = []\n",
    "\n",
    "            # Iterate through all possible groups\n",
    "            for group in range(batch_size):\n",
    "                group_elements = tensor[tensor[:, 0] == group][:, 1]\n",
    "                if len(group_elements) == 0:\n",
    "                    # If the group is missing, add the missing value\n",
    "                    group_elements = torch.tensor([missing_value])\n",
    "                grouped_sequences.append(group_elements)\n",
    "\n",
    "            # Find the maximum length of sequences\n",
    "            max_len = max(len(seq) for seq in grouped_sequences)\n",
    "            \n",
    "            # Pad the sequences\n",
    "            padded_sequences = []\n",
    "            for seq in grouped_sequences:\n",
    "                padded_seq = torch.cat([seq, torch.full((max_len - len(seq),), pad_value)])\n",
    "                padded_sequences.append(padded_seq)\n",
    "            \n",
    "            return torch.stack(padded_sequences)\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            pred_fut = []\n",
    "            ground_truth_fut = []\n",
    "            for batch in self.train_data_loader:\n",
    "                hidden = self.model.init_hidden(self.batch_size)\n",
    "                history_edge_features = batch[\"history_edge_features\"]\n",
    "                history_edge_indices = batch[\"history_indices\"]\n",
    "                history_edge_indices_one_hot = history_edge_features[:, :, 0]\n",
    "                initial_edge = batch[\"history_indices\"][:, -1]\n",
    "                future_edge_indices = batch[\"future_indices\"]\n",
    "                future_edge_features = batch[\"future_edge_features\"]\n",
    "                future_edge_indices_one_hot = future_edge_features[:, :, 0]\n",
    "                batch_size_act = history_edge_features.size(0)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = 0\n",
    "                preds = []\n",
    "                preds_binary = torch.zeros_like(future_edge_indices_one_hot)\n",
    "                for t in range(self.future_len):\n",
    "                    if t == 0:\n",
    "                        current_edge = initial_edge\n",
    "                    else:\n",
    "                        current_edge = future_edge_indices[:, t-1]\n",
    "                    \n",
    "                    true_neighbors = self.get_neighbors(self.line_graph, current_edge)\n",
    "                    # TODO: Use history edge features at t == 0, then add true future_edge_features at t-1\n",
    "                    input_features = self.model.get_neighbor_features(history_edge_features, true_neighbors)\n",
    "                    out, hidden = self.model(input_features, hidden)\n",
    "                    logits = out.squeeze(-1)  # Remove the last dimension to match (bs, num_neighbors)\n",
    "                    \n",
    "                    #masked_logits = logits.clone()\n",
    "                    #masked_logits[true_neighbors == 0] = -100    # Mask out true neighbors\n",
    "                    #masked_logits[history_edge_indices_one_hot == 1] = -100   # Mask out history\n",
    "                    \n",
    "                    predicted_edge_indices = torch.argmax(logits, dim=1)\n",
    "                    true_neighbors_padded = group_and_pad(torch.argwhere(true_neighbors == 1), batch_size_act)  # (bs, num_neighbors)\n",
    "                    \n",
    "                    predicted_edges = true_neighbors_padded.gather(1, predicted_edge_indices.unsqueeze(1)).squeeze(1)\n",
    "                    preds.append(predicted_edges)\n",
    "                    ground_truth = transform_tensor(true_neighbors, future_edge_indices[:, t])\n",
    "                    loss += criterion(logits, ground_truth.float())\n",
    "                \n",
    "                for i in range(self.batch_size):\n",
    "                    pred_binary = torch.zeros(self.num_edges, dtype=torch.float)\n",
    "                    pred_binary[torch.stack(preds).t()[i]] = 1\n",
    "                    preds_binary[i] = pred_binary.clone().detach()\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pred_fut.append(preds_binary)\n",
    "                ground_truth_fut.append(future_edge_indices_one_hot)\n",
    "                \n",
    "            preds = [torch.stack(preds).t()]\n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach(), torch.flatten(torch.cat(ground_truth_fut)).detach())\n",
    "\n",
    "            if (epoch + 1) % self.log_loss_every_steps == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{self.num_epochs}, Loss: {avg_loss:.4f}, F1 Score: {f1_epoch.item()}\")\n",
    "                print(\"History Edges:\", history_edge_indices)\n",
    "                print(\"Predicted Edges:\", preds)\n",
    "                print(\"Future_edge_indices\", future_edge_indices)\n",
    "        \n",
    "        print(\"> Training Complete!\\n\")\n",
    "        \n",
    "    '''def get_neighbors(self, line_graph, edge):\n",
    "        neighbors = []\n",
    "        for e in edge:\n",
    "            # When e == -1, neighbor_tensor = torch.zeros(self.num_edges, dtype=torch.long)\n",
    "            neighbor_set = set()\n",
    "            for i in range(line_graph.edge_index.size(1)):\n",
    "                if line_graph.edge_index[0, i] == e:\n",
    "                    neighbor_set.add(line_graph.edge_index[1, i].item())\n",
    "                elif line_graph.edge_index[1, i] == e:\n",
    "                    neighbor_set.add(line_graph.edge_index[0, i].item())\n",
    "            neighbor_tensor = torch.zeros(self.num_edges, dtype=torch.long)\n",
    "            neighbor_tensor[list(neighbor_set)] = 1\n",
    "            neighbors.append(neighbor_tensor)\n",
    "        \n",
    "        return torch.stack(neighbors)'''\n",
    "        \n",
    "    def get_neighbors(self, line_graph, edge):\n",
    "        edge_tensor = torch.tensor(edge, dtype=torch.long)\n",
    "\n",
    "        # Loop through each edge in batch and update the corresponding row in neighbor_tensor\n",
    "        for i, e in enumerate(edge_tensor):\n",
    "            # Create a 2D boolean mask for matching edges in edge_index\n",
    "            mask0 = line_graph.edge_index[0].unsqueeze(1) == e.unsqueeze(0)\n",
    "            mask1 = line_graph.edge_index[1].unsqueeze(1) == e.unsqueeze(0)\n",
    "\n",
    "            # Find neighbors: if mask0[i, j] is True, the neighbor is edge_index[1, i], and vice versa for mask1\n",
    "            neighbors_index_0 = torch.where(mask0, line_graph.edge_index[1].unsqueeze(1), torch.full_like(line_graph.edge_index[1].unsqueeze(1), -1))\n",
    "            neighbor_indices = neighbors_index_0[:, 0][neighbors_index_0[:, 0] != -1]\n",
    "            \n",
    "            print(\"Edge\", e)\n",
    "            print(\"neighbor_indices\", neighbor_indices)\n",
    "\n",
    "            # We now need to construct the final neighbor tensor\n",
    "            neighbor_tensor = torch.zeros((edge_tensor.size(0), self.num_edges), dtype=torch.long, device=edge_tensor.device)\n",
    "            # Get unique neighbor indices for the current edge e\n",
    "            for j in range(len(neighbor_indices)):\n",
    "                neighbor_tensor[i, neighbor_indices[j]] = 1\n",
    "\n",
    "        print(neighbor_tensor)\n",
    "        return neighbor_tensor\n",
    "        \n",
    "    def save_model(self):\n",
    "        save_path = os.path.join(self.model_dir, \n",
    "                                 self.exp_name + '_' + self.model_config['name'] + '_' +  self.model_config['transition_mat_type'] + '_'  + \n",
    "                                 f'_hidden_dim_{self.hidden_channels}_condition_dim_{self.condition_dim}_layers_{self.num_layers}.pth')\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        self.log.info(f\"Model saved at {save_path}!\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.log.info(\"Model loaded!\\n\")\n",
    "    \n",
    "    def _build_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.learning_rate_warmup_steps:\n",
    "                return 1.0\n",
    "            else:\n",
    "                decay_lr = self.lr_decay_parameter ** (epoch - self.learning_rate_warmup_steps)\n",
    "                return max(decay_lr, 2e-5 / self.lr)\n",
    "            \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "        print(\"> Optimizer and Scheduler built!\\n\")\n",
    "        \n",
    "    def _build_train_dataloader(self):\n",
    "        print(\"Loading Training Dataset...\")\n",
    "        self.train_dataset = TrajectoryDataset(self.train_data_path, self.history_len, self.future_len, self.edge_features)\n",
    "        self.G = self.train_dataset.graph\n",
    "        self.nodes = self.G.nodes\n",
    "        self.edges = self.G.edges(data=True)\n",
    "        self.indexed_edges = self.train_dataset.edges\n",
    "        self.num_edge_features = self.train_dataset.num_edge_features\n",
    "        \n",
    "        # Build the line graph and corresponding edge index\n",
    "        edge_index = self._build_edge_index()\n",
    "        self.line_graph = Data(edge_index=edge_index)\n",
    "        \n",
    "        self.edge_tensor = self.train_dataset.get_all_edges_tensor()\n",
    "        self.num_edges = self.train_dataset.get_n_edges()\n",
    "        \n",
    "        self.train_data_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "                \n",
    "        print(\"> Training Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.edges], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.edges}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        \n",
    "        neighbor_counts = {edge_to_index[(u1, v1)]: 0 for u1, v1 in edge_list}\n",
    "        \n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "                    neighbor_counts[edge_to_index[(u1, v1)]] += 1\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        # Find the maximum neighbor degree\n",
    "        self.max_degree = max(neighbor_counts.values())\n",
    "        \n",
    "        return edge_index    \n",
    "\n",
    "    def _build_test_dataloader(self):\n",
    "        self.test_dataset = TrajectoryDataset(self.test_data_path, self.history_len, self.future_len, self.edge_features)\n",
    "        self.test_data_loader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        print(\"> Test Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_model(self):\n",
    "        #self.model = self.model(self.model_config, self.history_len, self.future_len, self.num_classes,\n",
    "        #                        nodes=self.nodes, edges=self.edges,\n",
    "        #                        num_edges=self.num_edges, hidden_channels=self.hidden_channels, num_edge_features=self.num_edge_features, max_degree=self.max_degree)\n",
    "        self.model = self.model(self.model_config, self.num_edge_features, self.num_edges)\n",
    "        print(\"> Model built!\\n\")\n",
    "        \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        if 'edge_indices' in new_hf['graph']:\n",
    "            edge_indices = new_hf['graph']['edge_indices'][:]\n",
    "            # Convert edges to a list of tuples\n",
    "            # Sort edges based on their saved indices\n",
    "            indexed_edges = sorted(zip(edges, edge_indices), key=lambda x: x[1])\n",
    "            edges = [edge for edge, _ in indexed_edges]\n",
    "        else:\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "\n",
    "        # Add edges with index to the graph\n",
    "        for (start, end), index in indexed_edges:\n",
    "            self.graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.stack((history_one_hot_edges, history_edge_orientations), dim=1)\n",
    "            future_edge_features = torch.stack((future_one_hot_edges, future_edge_orientations), dim=1)\n",
    "        else:\n",
    "            history_edge_features = history_one_hot_edges\n",
    "            future_edge_features = future_one_hot_edges\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"node_in_history\": node_in_history,\n",
    "        }, self.graph, self.edges\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    edges = [item[2] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_coordinates\": history_coordinates,\n",
    "            \"future_coordinates\": future_coordinates,\n",
    "            \"history_one_hot_edges\": history_one_hot_edges,\n",
    "            \"future_one_hot_edges\": future_one_hot_edges,\n",
    "            \"history_edge_orientations\": history_edge_orientations,\n",
    "            \"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            \"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "            \"edges\": edges,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeRNN(nn.Module):\n",
    "    def __init__(self, model_config, num_edge_features, num_edges):\n",
    "        super(EdgeRNN, self).__init__()\n",
    "        self.model_config = model_config\n",
    "        self.input_dim = num_edge_features\n",
    "        self.num_edges = num_edges\n",
    "        self.hidden_dim = self.model_config['hidden_channels']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        self.rnn = nn.RNN(self.input_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, 1)  # Output one logit per neighbor\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    def recursive_edge_prediction(self, edge_features, initial_edge, line_graph, future_len):\n",
    "        batch_size, num_edges, num_features = edge_features.size()\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(edge_features.device)\n",
    "        current_edge = initial_edge\n",
    "        predicted_edges = []\n",
    "        logits_list = []\n",
    "\n",
    "        for _ in range(future_len):\n",
    "            neighbors = self.get_neighbors(line_graph, current_edge, edge_features.device)  # shape: [batch_size, num_edges]\n",
    "            neighbor_features = self.get_neighbor_features(edge_features, neighbors)  # shape: [batch_size, num_neighbors, num_features]\n",
    "            max_num_neighbors = neighbors.sum(1).max().item()\n",
    "            logits, hidden = self.forward(neighbor_features, hidden)\n",
    "            logits = logits.squeeze(-1)  # Remove the last dimension to match [batch_size, num_neighbors]\n",
    "            \n",
    "            # Mask logits\n",
    "            mask = torch.arange(max_num_neighbors, device=edge_features.device).expand(len(neighbors), max_num_neighbors) < neighbors.sum(1, keepdim=True)\n",
    "            logits[~mask] = float(-10)\n",
    "            \n",
    "            logits_list.append(logits)\n",
    "            \n",
    "            _, predicted_edge_idx = torch.max(logits, dim=1)\n",
    "            #print(\"predicted_edge_idx\", predicted_edge_idx)\n",
    "            neighbor_indices = torch.nonzero(neighbors, as_tuple=True)\n",
    "            neighbor_indices = torch.split(neighbor_indices[1], neighbors.sum(dim=1).tolist())\n",
    "            max_len = max(len(s) for s in neighbor_indices)\n",
    "            neighbor_indices = torch.stack([F.pad(s, (0, max_len - len(s))) for s in neighbor_indices])\n",
    "            #print(\"Neighbor indices\", neighbor_indices)\n",
    "            predicted_edge = neighbor_indices.gather(1, predicted_edge_idx.unsqueeze(1)).squeeze(1)\n",
    "            #print(\"predicted_edge\", predicted_edge)\n",
    "\n",
    "            predicted_edges.append(predicted_edge)\n",
    "            current_edge = predicted_edge.unsqueeze(1)  # Update current edge for the next iteration\n",
    "        return logits_list, predicted_edges\n",
    "    \n",
    "    def get_neighbor_features(self, edge_features, neighbors):\n",
    "        batch_size, num_edges, num_features = edge_features.size()\n",
    "        neighbor_list = []\n",
    "        for i in range(batch_size):\n",
    "            if neighbors[i].sum() == 0:\n",
    "                neighbor_list.append(torch.zeros(1, num_features, dtype=torch.float))\n",
    "                continue\n",
    "            neighbor_indices = neighbors[i].nonzero(as_tuple=False).squeeze(1)\n",
    "            neighbor_list.append(edge_features[i, neighbor_indices])\n",
    "        \n",
    "        neighbor_features = torch.nn.utils.rnn.pad_sequence(neighbor_list, batch_first=True)\n",
    "        '''\n",
    "        # Filter out the row indicating the edge in the history!\n",
    "        filtered_neighbor_features = neighbor_features[~(neighbor_features[:, :, 0] == 1)]\n",
    "        filtered_neighbor_features = filtered_neighbor_features.view(neighbor_features.size(0), -1, neighbor_features.size(2))\n",
    "        return filtered_neighbor_features\n",
    "        '''\n",
    "        return neighbor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bhijrth9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-salad-1326</strong> at: <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/bhijrth9' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/bhijrth9</a><br/> View project at: <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_142558-bhijrth9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bhijrth9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/models/wandb/run-20240620_142605-xscqbo2n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/xscqbo2n' target=\"_blank\">youthful-violet-1327</a></strong> to <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/xscqbo2n' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/xscqbo2n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 609.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge index for line graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edges: 100%|██████████| 46/46 [00:00<00:00, 68417.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Edge index built!\n",
      "\n",
      "> Training Dataset loaded!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 391.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Test Dataset loaded!\n",
      "\n",
      "> Model built!\n",
      "\n",
      "> Optimizer and Scheduler built!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_2078184/1243082763.py:331: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_tensor = torch.tensor(edge, dtype=torch.long)\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge tensor(16)\n",
      "neighbor_indices tensor([ 7, 17, 14, 13])\n",
      "Edge tensor(41)\n",
      "neighbor_indices tensor([39, 40, 42, 38, 36, 43])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 2, 32), got [1, 10, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 57\u001b[0m\n\u001b[1;32m     49\u001b[0m wandb_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtdrive_benchmark_test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory_prediction_using_denoising_diffusion_models\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoeschmit99\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBenchmark test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP_Benchmark\u001b[39m\u001b[38;5;124m\"\u001b[39m]} \n\u001b[1;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m Benchmark_Models(data_config, model_config, train_config, test_config, wandb_config, encoder_model)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 274\u001b[0m, in \u001b[0;36mBenchmark_Models.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# TODO: Use history edge features at t == 0, then add true future_edge_features at t-1\u001b[39;00m\n\u001b[1;32m    273\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_neighbor_features(history_edge_features, true_neighbors)\n\u001b[0;32m--> 274\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Remove the last dimension to match (bs, num_neighbors)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m#masked_logits = logits.clone()\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m#masked_logits[true_neighbors == 0] = -100    # Mask out true neighbors\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m#masked_logits[history_edge_indices_one_hot == 1] = -100   # Mask out history\u001b[39;00m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mEdgeRNN.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[0;32m---> 13\u001b[0m     out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, hidden\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/rnn.py:550\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    547\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_RELU\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/rnn.py:276\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m    274\u001b[0m expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m--> 276\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hidden_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/hdd/students/schmitj/miniconda3/envs/diffusion/lib/python3.12/site-packages/torch/nn/modules/rnn.py:259\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    257\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[0;32m--> 259\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden size (1, 2, 32), got [1, 10, 32]"
     ]
    }
   ],
   "source": [
    "encoder_model = EdgeRNN\n",
    "\n",
    "    \n",
    "data_config = {\"dataset\": \"synthetic_2_traj\",\n",
    "    \"train_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_2_traj.h5',\n",
    "    \"test_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/synthetic_2_traj.h5',\n",
    "    \"history_len\": 5,\n",
    "    \"future_len\": 2,\n",
    "    \"num_classes\": 2,\n",
    "    \"edge_features\": ['one_hot_edges', 'coordinates'],\n",
    "    \"one_hot_nodes\": False}\n",
    "\n",
    "model_config = {\"name\": \"mlp_benchmark\",\n",
    "    \"hidden_channels\": 32,\n",
    "    \"time_embedding_dim\": 16,\n",
    "    \"condition_dim\": 16,\n",
    "    \"out_ch\": 1,\n",
    "    \"num_heads\": 1,\n",
    "    \"num_layers\": 1,\n",
    "    \"theta\": 1.0, # controls strength of conv layers in residual model\n",
    "    \"dropout\": 0.1,\n",
    "    \"model_output\": \"logits\",\n",
    "    \"model_prediction\": \"x_start\",  # Options: 'x_start','xprev'\n",
    "    \"transition_mat_type\": 'gaussian',  # Options: 'gaussian','uniform','absorbing', 'marginal_prior'\n",
    "    \"transition_bands\": 1,\n",
    "    \"loss_type\": \"cross_entropy_x_start\",  # Options: kl, cross_entropy_x_start, hybrid\n",
    "    \"hybrid_coeff\": 0.001,  # Only used for hybrid loss type.\n",
    "    \"class_weights\": [0.05, 0.95] # = future_len/num_edges and (num_edges - future_len)/num_edges\n",
    "    }\n",
    "\n",
    "train_config = {\"batch_size\": 10,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"gradient_accumulation\": False,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"num_epochs\": 1000,\n",
    "    \"learning_rate_warmup_steps\": 2000, # previously 10000\n",
    "    \"lr_decay\": 0.9999, # previously 0.9999\n",
    "    \"log_loss_every_steps\": 20,\n",
    "    \"save_model\": False,\n",
    "    \"save_model_every_steps\": 1000}\n",
    "\n",
    "test_config = {\"batch_size\": 1, # currently only 1 works\n",
    "    \"model_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/synthetic_d3pm_residual_fixed/synthetic_d3pm_residual_fixed_hidden_dim_32_time_dim_16_condition_dim_16_layers_2_weights_0.1.pth',\n",
    "    \"number_samples\": 1,\n",
    "    \"eval_every_steps\": 1000\n",
    "  }\n",
    "\n",
    "wandb_config = {\"exp_name\": \"tdrive_benchmark_test\",\n",
    "    \"project\": \"trajectory_prediction_using_denoising_diffusion_models\",\n",
    "    \"entity\": \"joeschmit99\",\n",
    "    \"job_type\": \"test\",\n",
    "    \"notes\": \"Benchmark test\",\n",
    "    \"tags\": [\"synthetic\", \"MLP_Benchmark\"]} \n",
    "\n",
    "model = Benchmark_Models(data_config, model_config, train_config, test_config, wandb_config, encoder_model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "\n",
    "def load_new_format(new_file_path):\n",
    "    paths = []\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with h5py.File(new_file_path, 'r') as new_hf:\n",
    "        node_coordinates = new_hf['graph']['node_coordinates'][:]\n",
    "        edges = new_hf['graph']['edges'][:]\n",
    "        edge_coordinates = node_coordinates[edges]\n",
    "        nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "        \n",
    "        if 'edge_indices' in new_hf['graph']:\n",
    "            edge_indices = new_hf['graph']['edge_indices'][:]\n",
    "            # Convert edges to a list of tuples\n",
    "            # Sort edges based on their saved indices\n",
    "            indexed_edges = sorted(zip(edges, edge_indices), key=lambda x: x[1])\n",
    "            edges = [edge for edge, _ in indexed_edges]\n",
    "        else:\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "        for i in tqdm(new_hf['trajectories'].keys()):\n",
    "            path_group = new_hf['trajectories'][i]\n",
    "            path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "            if 'edge_orientation' in path:\n",
    "                path['edge_orientations'] = path.pop('edge_orientation')\n",
    "            paths.append(path)\n",
    "\n",
    "    return paths, nodes, edges, edge_coordinates\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = load_new_format(file_path)\n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64)\n",
    "        \n",
    "        self.graph = nx.Graph()\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "\n",
    "        # Add edges with index to the graph\n",
    "        for (start, end), index in indexed_edges:\n",
    "            self.graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        self.graph.add_nodes_from(self.nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        edge_idxs = torch.tensor(trajectory['edge_idxs'][:], dtype=torch.long)\n",
    "        edge_orientations = torch.tensor(trajectory['edge_orientations'][:], dtype=torch.long)\n",
    "        \n",
    "        edge_coordinates_data = self.edge_coordinates[edge_idxs]\n",
    "\n",
    "        if len(edge_coordinates_data) > 0:\n",
    "            edge_coordinates_np = np.array(edge_coordinates_data)\n",
    "            edge_coordinates = torch.tensor(edge_coordinates_np, dtype=torch.float64)\n",
    "        else:\n",
    "            edge_coordinates = torch.tensor([], dtype=torch.float64)\n",
    "\n",
    "        # Reverse coordinates if orientation is -1\n",
    "        edge_coordinates[edge_orientations == -1] = edge_coordinates[edge_orientations == -1][:, [1, 0]]\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices and orientations\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Pad coordinates\n",
    "        if padding_length > 0 and edge_coordinates.numel() > 0:\n",
    "            zero_padding = torch.zeros((padding_length, 2, 2), dtype=torch.float)\n",
    "            edge_coordinates = torch.cat([edge_coordinates, zero_padding], dim=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "        #history_coordinates = edge_coordinates[:self.history_len] if edge_coordinates.numel() > 0 else None\n",
    "        #future_coordinates = edge_coordinates[self.history_len:self.history_len + self.future_len] if edge_coordinates.numel() > 0 else None\n",
    "        \n",
    "        history_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "        future_edge_orientations = torch.zeros(self.get_n_edges())\n",
    "\n",
    "        for index, i in enumerate(history_indices):\n",
    "            history_edge_orientations[i] = edge_orientations[index]\n",
    "        \n",
    "        for index, i in enumerate(future_indices):\n",
    "            future_edge_orientations[i] = edge_orientations[index]\n",
    "\n",
    "        # One-hot encoding of edge indices (ensure valid indices first)\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Generate the tensor indicating nodes in history\n",
    "        \"\"\"node_in_history = torch.zeros((len(self.nodes), 1), dtype=torch.float)\n",
    "        history_edges = [self.edges[i] for i in history_indices if i >= 0]\n",
    "        history_nodes = set(node for edge in history_edges for node in edge)\n",
    "        for node in history_nodes:\n",
    "            node_in_history[node] = 1\"\"\"\n",
    "            \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            #\"history_coordinates\": history_coordinates,\n",
    "            #\"future_coordinates\": future_coordinates,\n",
    "            #\"history_one_hot_edges\": history_one_hot_edges,\n",
    "            #\"future_one_hot_edges\": future_one_hot_edges,\n",
    "            #\"history_edge_orientations\": history_edge_orientations,\n",
    "            #\"future_edge_orientations\": future_edge_orientations,\n",
    "            #\"node_in_history\": node_in_history,\n",
    "        }, self.graph# , self.edges\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def get_n_edges(self):\n",
    "        return self.graph.number_of_edges()\n",
    "    \n",
    "    \"\"\"def node_coordinates(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [#nodes, 2] containing the coordinates of each node.\n",
    "        \"\"\"\n",
    "        coords = [attr['pos'] for _, attr in self.nodes]  # List of tuples (x, y)\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float)  # Convert list to tensor\n",
    "        return coords_tensor\n",
    "    \n",
    "    def get_all_edges_tensor(self):\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape [2, num_edges] where each column represents an edge\n",
    "        and the two entries in each column represent the nodes connected by that edge.\n",
    "        \"\"\"\n",
    "        edges = list(self.graph.edges())\n",
    "        edge_tensor = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return edge_tensor\"\"\"\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    graph = [item[1] for item in batch]\n",
    "    # edges = [item[2] for item in batch]\n",
    "    # Extract elements for each sample and stack them, handling variable lengths\n",
    "    history_indices = torch.stack([item[0]['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item[0]['future_indices'] for item in batch])\n",
    "    \n",
    "    #history_one_hot_edges = torch.stack([item[0]['history_one_hot_edges'] for item in batch])\n",
    "    #future_one_hot_edges = torch.stack([item[0]['future_one_hot_edges'] for item in batch])\n",
    "\n",
    "    # Coordinates\n",
    "    #history_coordinates = [item[0]['history_coordinates'] for item in batch if item[0]['history_coordinates'] is not None]\n",
    "    #future_coordinates = [item[0]['future_coordinates'] for item in batch if item[0]['future_coordinates'] is not None]\n",
    "    \n",
    "    #history_edge_orientations = torch.stack([item[0]['history_edge_orientations'] for item in batch])\n",
    "    #future_edge_orientations = torch.stack([item[0]['future_edge_orientations'] for item in batch])\n",
    "    \n",
    "    history_edge_features = torch.stack([item[0]['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item[0]['future_edge_features'] for item in batch])\n",
    "    \n",
    "    #history_one_hot_nodes = torch.stack([item[0]['node_in_history'] for item in batch])\n",
    "    \n",
    "    # Stack coordinates if not empty\n",
    "    \"\"\"if history_coordinates:\n",
    "        history_coordinates = torch.stack(history_coordinates)\n",
    "    if future_coordinates:\n",
    "        future_coordinates = torch.stack(future_coordinates)\"\"\"\n",
    "\n",
    "    return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            #\"history_coordinates\": history_coordinates,\n",
    "            #\"future_coordinates\": future_coordinates,\n",
    "            #\"history_one_hot_edges\": history_one_hot_edges,\n",
    "            #\"future_one_hot_edges\": future_one_hot_edges,\n",
    "            #\"history_edge_orientations\": history_edge_orientations,\n",
    "            #\"future_edge_orientations\": future_edge_orientations,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "            #\"history_one_hot_nodes\": history_one_hot_nodes,\n",
    "            \"graph\": graph,\n",
    "            # \"edges\": edges,\n",
    "        }'''\n",
    "        \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, file_path, history_len, future_len, edge_features=None, device=None):\n",
    "        self.file_path = file_path\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.edge_features = edge_features\n",
    "        self.device = device\n",
    "        self.num_edge_features = 1\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            self.num_edge_features = 5\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            self.num_edge_features = 6\n",
    "        self.trajectories, self.nodes, self.edges, self.edge_coordinates = self.load_new_format(file_path, self.device)\n",
    "        \n",
    "        self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def load_new_format(file_path, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            #edges = torch.tensor(new_hf['graph']['edges'][:], dtype=torch.long, device=device)\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            #edges = [(torch.tensor(edge[0], device=device), torch.tensor(edge[1], device=device)) for edge in edges]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            \n",
    "        return paths, nodes, edges, edge_coordinates\n",
    "    \n",
    "    # @staticmethod\n",
    "    def build_graph(self):\n",
    "        graph = nx.Graph()\n",
    "        graph.add_nodes_from(self.nodes)\n",
    "        indexed_edges = [((start, end), index) for index, (start, end) in enumerate(self.edges)]\n",
    "        for (start, end), index in indexed_edges:\n",
    "            graph.add_edge(start, end, index=index, default_orientation=(start, end))\n",
    "        return graph\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "    \n",
    "        edge_idxs = trajectory['edge_idxs']\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = trajectory['edge_orientations']\n",
    "        \n",
    "        # Calculate the required padding length\n",
    "        total_len = self.history_len + self.future_len\n",
    "        padding_length = max(total_len - len(edge_idxs), 0)\n",
    "        \n",
    "        # Pad edge indices, orientations, and coordinates\n",
    "        edge_idxs = torch.nn.functional.pad(edge_idxs, (0, padding_length), value=-1)\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            edge_orientations = torch.nn.functional.pad(edge_orientations, (0, padding_length), value=0)\n",
    "        \n",
    "        # Split into history and future\n",
    "        history_indices = edge_idxs[:self.history_len]\n",
    "        future_indices = edge_idxs[self.history_len:self.history_len + self.future_len]\n",
    "\n",
    "        # Extract and generate features\n",
    "        history_edge_features, future_edge_features = self.generate_edge_features(history_indices, future_indices, self.edge_coordinates)\n",
    "\n",
    "        return {\n",
    "            \"history_indices\": history_indices,\n",
    "            \"future_indices\": future_indices,\n",
    "            \"history_edge_features\": history_edge_features,\n",
    "            \"future_edge_features\": future_edge_features,\n",
    "        }\n",
    "\n",
    "    def generate_edge_features(self, history_indices, future_indices, history_edge_orientations=None, future_edge_orientations=None):\n",
    "        # Binary on/off edges\n",
    "        valid_history_mask = history_indices >= 0\n",
    "        valid_future_mask = future_indices >= 0\n",
    "        \n",
    "        history_one_hot_edges = torch.nn.functional.one_hot(history_indices[valid_history_mask], num_classes=len(self.edges))\n",
    "        future_one_hot_edges = torch.nn.functional.one_hot(future_indices[valid_future_mask], num_classes=len(self.edges))\n",
    "        \n",
    "        # Sum across the time dimension to count occurrences of each edge\n",
    "        history_one_hot_edges = history_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        future_one_hot_edges = future_one_hot_edges.sum(dim=0)  # (num_edges,)\n",
    "        \n",
    "        # Basic History edge features = coordinates, binary encoding\n",
    "        history_edge_features = history_one_hot_edges.view(-1, 1).float()\n",
    "        future_edge_features = future_one_hot_edges.view(-1, 1).float()\n",
    "        if 'coordinates' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, torch.flatten(self.edge_coordinates, start_dim=1).float()), dim=1)\n",
    "            pass\n",
    "        if 'edge_orientations' in self.edge_features:\n",
    "            history_edge_features = torch.cat((history_edge_features, history_edge_orientations.float()), dim=1)\n",
    "            future_edge_features = torch.cat((future_edge_features, future_edge_orientations.float()), dim=1)\n",
    "        return history_edge_features, future_edge_features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    history_indices = torch.stack([item['history_indices'] for item in batch])\n",
    "    future_indices = torch.stack([item['future_indices'] for item in batch])\n",
    "    history_edge_features = torch.stack([item['history_edge_features'] for item in batch])\n",
    "    future_edge_features = torch.stack([item['future_edge_features'] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"history_indices\": history_indices,\n",
    "        \"future_indices\": future_indices,\n",
    "        \"history_edge_features\": history_edge_features,\n",
    "        \"future_edge_features\": future_edge_features,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Diffusion for discrete state spaces.\"\"\"\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_diffusion(diffusion_config, model_config, num_edges, future_len, device):\n",
    "    \"\"\"HParams -> diffusion object.\"\"\"\n",
    "    return CategoricalDiffusion(\n",
    "        betas=get_diffusion_betas(diffusion_config, device),\n",
    "        model_prediction=model_config['model_prediction'],\n",
    "        model_output=model_config['model_output'],\n",
    "        transition_mat_type=model_config['transition_mat_type'],\n",
    "        transition_bands=model_config['transition_bands'],\n",
    "        loss_type=model_config['loss_type'],\n",
    "        hybrid_coeff=model_config['hybrid_coeff'],\n",
    "        num_edges=num_edges,\n",
    "        model_name=model_config['name'],\n",
    "        future_len=future_len,\n",
    "        device=device\n",
    ")\n",
    "\n",
    "\n",
    "def get_diffusion_betas(spec, device):\n",
    "    \"\"\"Get betas from the hyperparameters.\"\"\"\n",
    "    \n",
    "    if spec['type'] == 'linear':\n",
    "        # Used by Ho et al. for DDPM, https://arxiv.org/abs/2006.11239.\n",
    "        # To be used with Gaussian diffusion models in continuous and discrete\n",
    "        # state spaces.\n",
    "        # To be used with transition_mat_type = 'gaussian'\n",
    "        return torch.linspace(spec['start'], spec['stop'], spec['num_timesteps']).to(device)\n",
    "    elif spec['type'] == 'cosine':\n",
    "        # Schedule proposed by Hoogeboom et al. https://arxiv.org/abs/2102.05379\n",
    "        # To be used with transition_mat_type = 'uniform'.\n",
    "        steps = torch.linspace(0, 1, spec['num_timesteps'] + 1, dtype=torch.float64)\n",
    "        alpha_bar = torch.cos((steps + 0.008) / 1.008 * torch.pi / 2)\n",
    "        betas = torch.minimum(1 - alpha_bar[1:] / alpha_bar[:-1], torch.tensor(0.999))\n",
    "        return betas.to(device)\n",
    "    elif spec['type'] == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        # Proposed by Sohl-Dickstein et al., https://arxiv.org/abs/1503.03585\n",
    "        # To be used with absorbing state models.\n",
    "        # ensures that the probability of decaying to the absorbing state\n",
    "        # increases linearly over time, and is 1 for t = T-1 (the final time).\n",
    "        # To be used with transition_mat_type = 'absorbing'\n",
    "        return 1. / torch.linspace(spec['num_timesteps'], 1, spec['num_timesteps']).to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(spec['type'])\n",
    "\n",
    "\n",
    "class CategoricalDiffusion:\n",
    "    \"\"\"Discrete state space diffusion process.\n",
    "\n",
    "    Time convention: noisy data is labeled x_0, ..., x_{T-1}, and original data\n",
    "    is labeled x_start (or x_{-1}). This convention differs from the papers,\n",
    "    which use x_1, ..., x_T for noisy data and x_0 for original data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, betas, model_prediction, model_output,\n",
    "               transition_mat_type, transition_bands, loss_type, hybrid_coeff,\n",
    "               num_edges, torch_dtype=torch.float32, model_name=None, future_len=None, device=None):\n",
    "\n",
    "        self.model_prediction = model_prediction  # *x_start*, xprev\n",
    "        self.model_output = model_output  # logits or *logistic_pars*\n",
    "        self.loss_type = loss_type  # kl, *hybrid*, cross_entropy_x_start\n",
    "        self.hybrid_coeff = hybrid_coeff\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "        # Data \\in {0, ..., num_edges-1}\n",
    "        self.num_classes = 2 # 0 or 1\n",
    "        self.num_edges = num_edges\n",
    "        self.future_len = future_len\n",
    "        self.class_weights = torch.tensor([self.future_len / self.num_edges, 1 - self.future_len / self.num_edges], dtype=torch.float64)\n",
    "        self.class_probs = torch.tensor([1 - self.future_len / self.num_edges, self.future_len / self.num_edges], dtype=torch.float64)\n",
    "        self.transition_bands = transition_bands\n",
    "        self.transition_mat_type = transition_mat_type\n",
    "        self.eps = 1.e-6\n",
    "\n",
    "        if not isinstance(betas, torch.Tensor):\n",
    "            raise ValueError('expected betas to be a torch tensor')\n",
    "        if not ((betas > 0).all() and (betas <= 1).all()):\n",
    "            raise ValueError('betas must be in (0, 1]')\n",
    "\n",
    "        # Computations here in float64 for accuracy\n",
    "        self.betas = betas.to(dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        self.num_timesteps, = betas.shape\n",
    "\n",
    "        # Construct transition matrices for q(x_t|x_{t-1})\n",
    "        # NOTE: t goes from {0, ..., T-1}\n",
    "        if self.transition_mat_type == 'uniform':\n",
    "            q_one_step_mats = [self._get_transition_mat(t) \n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'gaussian':\n",
    "            q_one_step_mats = [self._get_gaussian_transition_mat(t)\n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'absorbing':\n",
    "            q_one_step_mats = [self._get_absorbing_transition_mat(t)\n",
    "                            for t in range(0, self.num_timesteps)]\n",
    "        elif self.transition_mat_type == 'marginal_prior':\n",
    "            q_one_step_mats = [self._get_prior_distribution_transition_mat(t)\n",
    "                               for t in range(0, self.num_timesteps)]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"transition_mat_type must be 'gaussian', 'uniform', 'absorbing' \"\n",
    "                f\", but is {self.transition_mat_type}\"\n",
    "                )\n",
    "\n",
    "        self.q_onestep_mats = torch.stack(q_one_step_mats, axis=0).to(self.device, non_blocking=True)\n",
    "        assert self.q_onestep_mats.shape == (self.num_timesteps,\n",
    "                                            self.num_classes,\n",
    "                                            self.num_classes)\n",
    "\n",
    "        # Construct transition matrices for q(x_t|x_start)\n",
    "        q_mat_t = self.q_onestep_mats[0]\n",
    "        q_mats = [q_mat_t]\n",
    "        for t in range(1, self.num_timesteps):\n",
    "            # Q_{1...t} = Q_{1 ... t-1} Q_t = Q_1 Q_2 ... Q_t\n",
    "            q_mat_t = torch.tensordot(q_mat_t, self.q_onestep_mats[t],\n",
    "                                    dims=[[1], [0]])\n",
    "            q_mats.append(q_mat_t)\n",
    "        self.q_mats = torch.stack(q_mats, axis=0)\n",
    "        assert self.q_mats.shape == (self.num_timesteps, self.num_classes,\n",
    "                                    self.num_classes), self.q_mats.shape\n",
    "\n",
    "        # Don't precompute transition matrices for q(x_{t-1} | x_t, x_start)\n",
    "        # Can be computed from self.q_mats and self.q_one_step_mats.\n",
    "        # Only need transpose of q_onestep_mats for posterior computation.\n",
    "        self.transpose_q_onestep_mats = torch.transpose(self.q_onestep_mats, dim0=1, dim1=2)\n",
    "        del self.q_onestep_mats\n",
    "\n",
    "    def _get_full_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        Contrary to the band diagonal version, this method constructs a transition\n",
    "        matrix with uniform probability to all other states.\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "        # Create a matrix filled with beta_t/num_classes\n",
    "        mat = torch.full((self.num_classes, self.num_classes), \n",
    "                            fill_value=beta_t / float(self.num_classes),\n",
    "                            dtype=torch.float64)\n",
    "\n",
    "        # Create a diagonal matrix with values to be set on the diagonal of mat\n",
    "        diag_val = 1. - beta_t * (self.num_classes - 1.) / self.num_classes\n",
    "        diag_matrix = torch.diag(torch.full((self.num_classes,), diag_val, dtype=torch.float64))\n",
    "\n",
    "        # Set the diagonal values\n",
    "        mat.fill_diagonal_(diag_val)\n",
    "\n",
    "        return mat\n",
    "\n",
    "    def _get_transition_mat(self, t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition\n",
    "        matrix Q with\n",
    "        Q_{ij} = beta_t / num_classes       if |i-j| <= self.transition_bands\n",
    "                1 - \\sum_{l \\neq i} Q_{il} if i==j.\n",
    "                0                          else.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        if self.transition_bands is None:\n",
    "            return self._get_full_transition_mat(t)\n",
    "        # Assumes num_off_diags < num_classes\n",
    "        beta_t = self.betas[t]\n",
    "        \n",
    "        mat = torch.zeros((self.num_classes, self.num_classes),\n",
    "                        dtype=torch.float64)\n",
    "        off_diag = torch.full((self.num_classes - 1,), fill_value=beta_t / float(self.num_classes), dtype=torch.float64)\n",
    "\n",
    "        for k in range(1, self.transition_bands + 1):\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "            off_diag = off_diag[:-1]\n",
    "\n",
    "        # Add diagonal values such that rows sum to one\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag(diag)\n",
    "        \n",
    "        return mat\n",
    "\n",
    "    def _get_gaussian_transition_mat(self, t):\n",
    "        r\"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        This method constructs a transition matrix Q with\n",
    "        decaying entries as a function of how far off diagonal the entry is.\n",
    "        Normalization option 1:\n",
    "        Q_{ij} =  ~ softmax(-val^2/beta_t)   if |i-j| <= self.transition_bands\n",
    "                    1 - \\sum_{l \\neq i} Q_{il}  if i==j.\n",
    "                    0                          else.\n",
    "\n",
    "        Normalization option 2:\n",
    "        tilde{Q}_{ij} =  softmax(-val^2/beta_t)   if |i-j| <= self.transition_bands\n",
    "                            0                        else.\n",
    "\n",
    "        Q_{ij} =  tilde{Q}_{ij} / sum_l{tilde{Q}_{lj}}\n",
    "\n",
    "        Args:\n",
    "            t: timestep. integer scalar (or numpy array?)\n",
    "\n",
    "        Returns:\n",
    "            Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        transition_bands = self.transition_bands if self.transition_bands else self.num_classes - 1\n",
    "\n",
    "        beta_t = self.betas[t]\n",
    "\n",
    "        mat = torch.zeros((self.num_classes, self.num_classes),\n",
    "                        dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Make the values correspond to a similar type of gaussian as in the\n",
    "        # gaussian diffusion case for continuous state spaces.\n",
    "        values = torch.linspace(torch.tensor(0.), torch.tensor(self.num_classes-1), self.num_classes, dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        values = values * 2./ (self.num_classes - 1.)\n",
    "        values = values[:transition_bands+1]\n",
    "        values = -values * values / beta_t\n",
    "        \n",
    "        # To reverse the tensor 'values' starting from the second element\n",
    "        reversed_values = values[1:].flip(dims=[0])\n",
    "        # Concatenating the reversed values with the original values\n",
    "        values = torch.cat([reversed_values, values], dim=0)\n",
    "        values = F.softmax(values, dim=0)\n",
    "        values = values[transition_bands:]\n",
    "        \n",
    "        for k in range(1, transition_bands + 1):\n",
    "            off_diag = torch.full((self.num_classes - k,), values[k], dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "            mat += torch.diag(off_diag, k)\n",
    "            mat += torch.diag(off_diag, -k)\n",
    "\n",
    "        # Add diagonal values such that rows and columns sum to one.\n",
    "        # Technically only the ROWS need to sum to one\n",
    "        # NOTE: this normalization leads to a doubly stochastic matrix,\n",
    "        # which is necessary if we want to have a uniform stationary distribution.\n",
    "        diag = 1. - mat.sum(dim=1)\n",
    "        mat += torch.diag_embed(diag)\n",
    "\n",
    "        return mat.to(self.device, non_blocking=True)\n",
    "\n",
    "    def _get_absorbing_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "\n",
    "        Has an absorbing state for pixelvalues self.num_classes//2.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "\n",
    "        diag = torch.full((self.num_classes,), 1. - beta_t, dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "        mat = torch.diag(diag)\n",
    "\n",
    "        # Add beta_t to the num_classes/2-th column for the absorbing state\n",
    "        mat[:, self.num_classes // 2] += beta_t\n",
    "\n",
    "        return mat\n",
    "    \n",
    "    def _get_prior_distribution_transition_mat(self, t):\n",
    "        \"\"\"Computes transition matrix for q(x_t|x_{t-1}).\n",
    "        Use cosine schedule for these transition matrices.\n",
    "\n",
    "        Args:\n",
    "        t: timestep. integer scalar.\n",
    "\n",
    "        Returns:\n",
    "        Q_t: transition matrix. shape = (num_classes, num_classes).\n",
    "        \"\"\"\n",
    "        beta_t = self.betas[t]\n",
    "        mat = torch.zeros((self.num_classes, self.num_classes), dtype=torch.float64).to(self.device, non_blocking=True)\n",
    "\n",
    "        for i in range(self.num_classes):\n",
    "            for j in range(self.num_classes):\n",
    "                if i != j:\n",
    "                    mat[i, j] = beta_t * self.class_probs[j]\n",
    "                else:\n",
    "                    mat[i, j] = 1 - beta_t + beta_t * self.class_probs[j]\n",
    "        \n",
    "        return mat\n",
    "\n",
    "    def _at(self, a, t, x):\n",
    "        \"\"\"\n",
    "        Extract coefficients at specified timesteps t and conditioning data x in PyTorch.\n",
    "\n",
    "        Args:\n",
    "        a: torch.Tensor: PyTorch tensor of constants indexed by time, dtype should be pre-set.\n",
    "        t: torch.Tensor: PyTorch tensor of time indices, shape = (batch_size,).\n",
    "        x: torch.Tensor: PyTorch tensor of shape (bs, ...) of int32 or int64 type.\n",
    "            (Noisy) data. Should not be of one-hot representation, but have integer\n",
    "            values representing the class values. --> NOT A LOT NEEDS TO CHANGE, MY CLASS VALUES ARE SIMPLY 0 AND 1\n",
    "\n",
    "        Returns:\n",
    "        a[t, x]: torch.Tensor: PyTorch tensor.\n",
    "        \"\"\"\n",
    "        ### Original ###\n",
    "        # x.shape = (bs, height, width, channels)\n",
    "        # t_broadcast_shape = (bs, 1, 1, 1)\n",
    "        # a.shape = (num_timesteps, num_pixel_vals, num_pixel_vals)\n",
    "        # out.shape = (bs, height, width, channels, num_pixel_vals)\n",
    "        # out[i, j, k, l, m] = a[t[i, j, k, l], x[i, j, k, l], m]\n",
    "        \n",
    "        ### New ###\n",
    "        # x.shape = (bs, num_edges, channels=1) \n",
    "        # t_broadcast_shape = (bs, 1, 1)\n",
    "        # a.shape = (num_timesteps, num_classes, num_classes) \n",
    "        # out.shape = (bs, num_edges, channels, num_classes) \n",
    "        \n",
    "        # Convert `a` to the desired dtype if not already\n",
    "        a = a.type(self.torch_dtype)\n",
    "\n",
    "        # Prepare t for broadcasting by adding necessary singleton dimensions\n",
    "        t_broadcast = t.view(-1, *((1,) * (x.ndim - 1))).to(self.device, non_blocking=True)\n",
    "\n",
    "        # Advanced indexing in PyTorch to select elements\n",
    "        return a[t_broadcast, x.long()].to(self.device, non_blocking=True)\n",
    "\n",
    "    def _at_onehot(self, a, t, x):\n",
    "        \"\"\"Extract coefficients at specified timesteps t and conditioning data x.\n",
    "\n",
    "        Args:\n",
    "        a: torch.Tensor: PyTorch tensor of constants indexed by time, dtype should be pre-set.\n",
    "        t: torch.Tensor: PyTorch tensor of time indices, shape = (batch_size,).\n",
    "        x: torch.Tensor: PyTorch tensor of shape (bs, ...) of float32 type.\n",
    "            (Noisy) data. Should be of one-hot-type representation.\n",
    "\n",
    "        Returns:\n",
    "        out: torch.tensor: output of dot(x, a[t], axis=[[-1], [1]]).\n",
    "            shape = (bs, num_edges, channels=1, num_classes)\n",
    "        \"\"\"\n",
    "        a = a.type(self.torch_dtype)\n",
    "        \n",
    "        ### Final ###\n",
    "        # t.shape = (bs)\n",
    "        # x.shape = (bs, num_edges, num_classes)\n",
    "        # a[t].shape = (bs, num_classes, num_classes)\n",
    "        # out.shape = (bs, num_edges, num_classes)\n",
    "\n",
    "        a_t = a[t]\n",
    "        out = torch.einsum('bik,bkj->bij', x, a_t).to(self.device, non_blocking=True)\n",
    "        \n",
    "        return out.to(self.device, non_blocking=True)\n",
    "\n",
    "    def q_probs(self, x_start, t):\n",
    "        \"\"\"Compute probabilities of q(x_t | x_start).\n",
    "\n",
    "        Args:\n",
    "        x_start: torch.tensor: tensor of shape (bs, ...) of int32 or int64 type.\n",
    "            Should not be of one hot representation, but have integer values\n",
    "            representing the class values.\n",
    "        t: torch.tensor: torch tensor of shape (bs,).\n",
    "\n",
    "        Returns:\n",
    "        probs: torch.tensor: shape (bs, x_start.shape[1:],\n",
    "                                                num_classes).\n",
    "        \"\"\"\n",
    "        return self._at(self.q_mats, t, x_start)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise):\n",
    "        \"\"\"\n",
    "        Sample from q(x_t | x_start) (i.e. add noise to the data) using Gumbel softmax trick.\n",
    "\n",
    "        Args:\n",
    "        x_start: torch.tensor: original clean data, in integer form (not onehot).\n",
    "            shape = (bs, num_edges).\n",
    "        t: torch.tensor: timestep of the diffusion process, shape (bs,).\n",
    "        noise: torch.tensor: uniform noise on [0, 1) used to sample noisy data.\n",
    "            shape should match (*x_start.shape, num_classes).\n",
    "\n",
    "        Returns:\n",
    "        sample: torch.tensor: same shape as x_start. noisy data.\n",
    "        \"\"\"\n",
    "        assert noise.shape == x_start.shape + (self.num_classes,)\n",
    "        logits = torch.log(self.q_probs(x_start, t) + self.eps)\n",
    "\n",
    "        # To avoid numerical issues, clip the noise to a minimum value\n",
    "        noise = torch.clamp(noise, min=torch.finfo(noise.dtype).tiny, max=1.)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise)).to(self.device, non_blocking=True)\n",
    "        return torch.argmax(logits + gumbel_noise, dim=-1)\n",
    "    \n",
    "    def _get_logits_from_logistic_pars(self, loc, log_scale):\n",
    "        \"\"\"\n",
    "        Computes logits for an underlying logistic distribution.\n",
    "\n",
    "        Args:\n",
    "        loc: torch.tensor: location parameter of logistic distribution.\n",
    "        log_scale: torch.tensor: log scale parameter of logistic distribution.\n",
    "\n",
    "        Returns:\n",
    "        logits: torch.tensor: logits corresponding to logistic distribution\n",
    "        \"\"\"\n",
    "        loc = loc.unsqueeze(-1)\n",
    "        log_scale = log_scale.unsqueeze(-1)\n",
    "\n",
    "        # Adjust the scale such that if it's zero, the probabilities have a scale\n",
    "        # that is neither too wide nor too narrow.\n",
    "        inv_scale = torch.exp(- (log_scale - 2.))\n",
    "\n",
    "        bin_width = 2. / (self.num_classes - 1.)\n",
    "        bin_centers = torch.linspace(-1., 1., self.num_classes)\n",
    "\n",
    "        bin_centers = bin_centers.unsqueeze(0)  # Add batch dimension\n",
    "        bin_centers = bin_centers - loc\n",
    "\n",
    "        log_cdf_min = -F.softplus(-inv_scale * (bin_centers - 0.5 * bin_width))\n",
    "        log_cdf_plus = -F.softplus(-inv_scale * (bin_centers + 0.5 * bin_width))\n",
    "\n",
    "        logits = torch.log(torch.exp(log_cdf_plus) - torch.exp(log_cdf_min) + self.eps)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def q_posterior_logits(self, x_start, x_t, t, x_start_logits):\n",
    "        \"\"\"Compute logits of q(x_{t-1} | x_t, x_start) in PyTorch.\"\"\"\n",
    "        \n",
    "        if x_start_logits:\n",
    "            assert x_start.shape == x_t.shape + (self.num_classes,), (x_start.shape, x_t.shape)\n",
    "        else:\n",
    "            assert x_start.shape == x_t.shape, (x_start.shape, x_t.shape)\n",
    "            \n",
    "        fact1 = self._at(self.transpose_q_onestep_mats, t, x_t)\n",
    "        if x_start_logits:\n",
    "            fact2 = self._at_onehot(self.q_mats, t-1, F.softmax(x_start, dim=-1))\n",
    "            tzero_logits = x_start\n",
    "        else:\n",
    "            fact2 = self._at(self.q_mats, t-1, x_start)\n",
    "            tzero_logits = torch.log(F.one_hot(x_start.to(torch.int64), num_classes=self.num_classes) + self.eps)\n",
    "\n",
    "        out = torch.log(fact1 + self.eps) + torch.log(fact2 + self.eps)\n",
    "\n",
    "        t_broadcast = t.unsqueeze(1).unsqueeze(2)  # Adds new dimensions: [batch_size, 1, 1]\n",
    "        t_broadcast = t_broadcast.expand(-1, tzero_logits.size(1), tzero_logits.size(-1)).to(self.device, non_blocking=True)   # tzero_logits.size(1) = num_edges, tzero_logits.size(-1) = num_classes\n",
    "\n",
    "        return torch.where(t_broadcast == 0, tzero_logits, out) # (bs, num_edges, num_classes)\n",
    "\n",
    "    def p_logits(self, model_fn, x, t, edge_features=None, edge_index=None, condition=None):\n",
    "        \"\"\"Compute logits of p(x_{t-1} | x_t) in PyTorch.\n",
    "\n",
    "        Args:\n",
    "            model_fn (function): The model function that takes input `x` and `t` and returns the model output.\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, input_size) representing the noised input at time t.\n",
    "            t (torch.Tensor): The time tensor of shape (batch_size,) representing the time step.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing two tensors:\n",
    "                - model_logits (torch.Tensor): The logits of p(x_{t-1} | x_t) of shape (batch_size, input_size, num_classes).\n",
    "                - pred_x_start_logits (torch.Tensor): The logits of p(x_{t-1} | x_start) of shape (batch_size, input_size, num_classes).\n",
    "        \"\"\"\n",
    "        assert t.shape == (x.shape[0],)\n",
    "        model_output = model_fn(edge_features, edge_index, t, condition=condition)\n",
    "\n",
    "        if self.model_output == 'logits':\n",
    "            model_logits = model_output\n",
    "        elif self.model_output == 'logistic_pars':\n",
    "            loc, log_scale = model_output\n",
    "            model_logits = self._get_logits_from_logistic_pars(loc, log_scale)\n",
    "        else:\n",
    "            raise NotImplementedError(self.model_output)\n",
    "\n",
    "        if self.model_prediction == 'x_start':\n",
    "            pred_x_start_logits = model_logits\n",
    "            t_broadcast = t.unsqueeze(1).unsqueeze(2)  # Adds new dimensions: [batch_size, 1, 1]\n",
    "            t_broadcast = t_broadcast.expand(-1, pred_x_start_logits.size(1), pred_x_start_logits.size(-1)).to(self.device, non_blocking=True)   # pred_x_start_logits.size(1) = num_edges, pred_x_start_logits.size(-1) = num_classes\n",
    "            model_logits = torch.where(t_broadcast == 0, pred_x_start_logits,\n",
    "                                       self.q_posterior_logits(x_start=pred_x_start_logits, x_t=x, t=t, x_start_logits=True))\n",
    "            \n",
    "        elif self.model_prediction == 'xprev':\n",
    "            pred_x_start_logits = model_logits\n",
    "            raise NotImplementedError(self.model_prediction)\n",
    "        \n",
    "        assert (model_logits.shape == pred_x_start_logits.shape == x.shape + (self.num_classes,))\n",
    "        return model_logits, pred_x_start_logits    # (bs, num_eedges, 2)\n",
    "    \n",
    "    # === Sampling ===\n",
    "\n",
    "    def p_sample(self, model_fn, x, t, noise, edge_features=None, edge_index=None, condition=None):\n",
    "        \"\"\"Sample one timestep from the model p(x_{t-1} | x_t).\"\"\"\n",
    "        # Get model logits\n",
    "        model_logits, pred_x_start_logits = self.p_logits(model_fn=model_fn, x=x, t=t, edge_features=edge_features, edge_index=edge_index, condition=condition)\n",
    "        assert noise.shape == model_logits.shape, noise.shape\n",
    "\n",
    "        # No noise when t == 0\n",
    "        nonzero_mask = (t != 0).float().reshape(x.shape[0], *([1] * (len(x.shape) - 1)))\n",
    "        # For numerical precision clip the noise to a minimum value\n",
    "        noise = torch.clamp(noise, min=torch.finfo(noise.dtype).eps, max=1.)\n",
    "        gumbel_noise = -torch.log(-torch.log(noise))\n",
    "\n",
    "        sample = torch.argmax(model_logits + nonzero_mask * gumbel_noise, dim=-1)\n",
    "\n",
    "        assert sample.shape == x.shape\n",
    "        assert pred_x_start_logits.shape == model_logits.shape\n",
    "        return sample, F.softmax(pred_x_start_logits, dim=-1)\n",
    "\n",
    "    def p_sample_loop(self, model_fn, shape, num_timesteps=None, return_x_init=False, edge_features=None, edge_index=None, line_graph=None, condition=None):\n",
    "        \"\"\"Ancestral sampling.\"\"\"\n",
    "        if num_timesteps is None:\n",
    "            num_timesteps = self.num_timesteps\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if self.transition_mat_type in ['gaussian', 'uniform', 'marginal_prior']:\n",
    "            x_init = torch.randint(0, self.num_classes, size=shape, device=device)\n",
    "        elif self.transition_mat_type == 'absorbing':\n",
    "            x_init = torch.full(shape, fill_value=self.num_classes // 2, dtype=torch.int32, device=device)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid transition_mat_type {self.transition_mat_type}\")\n",
    "\n",
    "        x = x_init.clone()  # (bs, num_edges)\n",
    "        edge_attr = x_init.float()\n",
    "        #new_line_graph_x = line_graph.x.clone()\n",
    "        #new_line_graph_x[:, :, 0] = edge_attr\n",
    "        new_edge_features = edge_features.clone()\n",
    "        new_edge_features[:, :, 0] = edge_attr\n",
    "        \n",
    "        for i in range(num_timesteps):\n",
    "            t = torch.full([shape[0]], self.num_timesteps - 1 - i, dtype=torch.long, device=device)\n",
    "            noise = torch.rand(x.shape + (self.num_classes,), device=device, dtype=torch.float32)\n",
    "            x, _ = self.p_sample(model_fn=model_fn, x=x, t=t, noise=noise, edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            new_edge_features[:, :, 0] = x.float()\n",
    "\n",
    "        if return_x_init:\n",
    "            return x_init, x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "  # === Log likelihood / loss calculation ===\n",
    "        \n",
    "    def cross_entropy_x_start(self, x_start, pred_x_start_logits, class_weights):\n",
    "        \"\"\"Calculate binary weighted cross entropy between x_start and predicted x_start logits.\n",
    "\n",
    "        Args:\n",
    "            x_start (torch.Tensor): original clean data, expected binary labels (0 or 1), shape (bs, num_edges)\n",
    "            pred_x_start_logits (torch.Tensor): logits as predicted by the model\n",
    "            class_weights (torch.Tensor): tensor with weights for class 0 and class 1\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: scalar tensor representing the mean binary weighted cross entropy loss.\n",
    "        \"\"\"\n",
    "        # Calculate binary cross-entropy with logits\n",
    "        x_start = x_start.long().to(self.device, non_blocking=True)\n",
    "        pred_x_start_logits = pred_x_start_logits.permute(0, 2, 1).float() # (bs, num_edges, num_classes) -> (bs, num_classes, num_edges)\n",
    "        ce = F.cross_entropy(pred_x_start_logits, x_start, weight=class_weights.float().to(self.device, non_blocking=True), reduction='mean')\n",
    "\n",
    "        return ce\n",
    "\n",
    "    def training_losses(self, model_fn, condition=None, *, x_start, edge_features, edge_index, line_graph=None):\n",
    "        \"\"\"Training loss calculation.\"\"\"\n",
    "        # Add noise to data\n",
    "        noise = torch.rand(x_start.shape + (self.num_classes,), dtype=torch.float32)\n",
    "        t = torch.randint(0, self.num_timesteps, (x_start.shape[0],))\n",
    "\n",
    "        # t starts at zero. so x_0 is the first noisy datapoint, not the datapoint itself.\n",
    "        x_t = self.q_sample(x_start=x_start, t=t, noise=noise)  # (bs, num_edges)\n",
    "        \n",
    "        edge_attr_t = x_t.float()\n",
    "        # new_line_graph_x = line_graph.x.clone()\n",
    "        new_edge_features = edge_features.clone()\n",
    "        for i in range(edge_attr_t.shape[0]):\n",
    "            # new_line_graph_x[i, :, 0] = edge_attr_t[i]  # Update the edge attributes in the line graph with the noised trajectory x_t\n",
    "            new_edge_features[i, :, 0] = edge_attr_t[i]\n",
    "\n",
    "\n",
    "        # Calculate the loss\n",
    "        if self.loss_type == 'kl':\n",
    "            losses, pred_x_start_logits = self.vb_terms_bpd(model_fn=model_fn, x_start=x_start, x_t=x_t, t=t,\n",
    "                                                               edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            \n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(2)    # (bs, num_edges, channels, classes) -> (bs, num_edges, classes)\n",
    "            # NOTE: Currently only works for batch size of 1\n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(0)    # (bs, num_edges, classes) -> (num_edges, classes)\n",
    "            pred = pred_x_start_logits.argmax(dim=1)                # (num_edges, classes) -> (num_edges,)\n",
    "            \n",
    "            return losses, pred\n",
    "            \n",
    "        elif self.loss_type == 'cross_entropy_x_start':\n",
    "            \n",
    "            _, pred_x_start_logits = self.p_logits(model_fn, x=x_t, t=t, edge_features=new_edge_features, edge_index=edge_index, condition=condition)\n",
    "            losses = self.cross_entropy_x_start(x_start=x_start, pred_x_start_logits=pred_x_start_logits, class_weights=self.class_weights)\n",
    "            \n",
    "            pred_x_start_logits = pred_x_start_logits.squeeze(2)    # (bs, num_edges, channels, classes) -> (bs, num_edges, classes)\n",
    "            \n",
    "            if (self.model_name == 'edge_encoder') | (self.model_name == 'edge_encoder_residual'):\n",
    "                # NOTE: Currently only works for batch size of 1\n",
    "                pred_x_start_logits = pred_x_start_logits.squeeze(0)    # (bs, num_edges, classes) -> (num_edges, classes)\n",
    "                pred = pred_x_start_logits.argmax(dim=1)    # (num_edges, classes) -> (num_edges,)\n",
    "            elif self.model_name == 'edge_encoder_mlp':\n",
    "                pred = pred_x_start_logits.argmax(dim=2)\n",
    "            \n",
    "            return losses, pred\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score\n",
    "from torch_geometric.utils import from_networkx\n",
    "#from dataset.trajctory_dataset import TrajectoryDataset, collate_fn\n",
    "#from .d3pm_diffusion import make_diffusion\n",
    "#from .d3pm_edge_encoder import Edge_Encoder\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import wandb\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "class Graph_Diffusion_Model(nn.Module):\n",
    "    def __init__(self, data_config, diffusion_config, model_config, train_config, test_config, wandb_config, model):\n",
    "        super(Graph_Diffusion_Model, self).__init__()\n",
    "        \n",
    "        # Data\n",
    "        self.data_config = data_config\n",
    "        self.train_data_path = self.data_config['train_data_path']\n",
    "        self.val_data_path = self.data_config['val_data_path']\n",
    "        self.history_len = self.data_config['history_len']\n",
    "        self.future_len = self.data_config['future_len']\n",
    "        self.num_classes = self.data_config['num_classes']\n",
    "        self.edge_features = self.data_config['edge_features']\n",
    "        \n",
    "        # Diffusion\n",
    "        self.diffusion_config = diffusion_config\n",
    "        self.num_timesteps = self.diffusion_config['num_timesteps']\n",
    "        \n",
    "        # Model\n",
    "        self.model_config = model_config\n",
    "        self.model = model # Edge_Encoder\n",
    "        self.hidden_channels = self.model_config['hidden_channels']\n",
    "        self.time_embedding_dim = self.model_config['time_embedding_dim']\n",
    "        self.condition_dim = self.model_config['condition_dim']\n",
    "        self.num_layers = self.model_config['num_layers']\n",
    "        \n",
    "        # Training\n",
    "        self.train_config = train_config\n",
    "        self.lr = self.train_config['lr']\n",
    "        self.lr_decay_parameter = self.train_config['lr_decay']\n",
    "        self.learning_rate_warmup_steps = self.train_config['learning_rate_warmup_steps']\n",
    "        self.num_epochs = self.train_config['num_epochs']\n",
    "        self.gradient_accumulation = self.train_config['gradient_accumulation']\n",
    "        self.gradient_accumulation_steps = self.train_config['gradient_accumulation_steps']\n",
    "        self.batch_size = self.train_config['batch_size'] if not self.gradient_accumulation else self.train_config['batch_size'] * self.gradient_accumulation_steps\n",
    "        \n",
    "        # Testing\n",
    "        self.test_config = test_config\n",
    "        self.test_batch_size = self.test_config['batch_size']\n",
    "        self.model_path = self.test_config['model_path']\n",
    "        self.eval_every_steps = self.test_config['eval_every_steps']\n",
    "        \n",
    "        # WandB\n",
    "        self.wandb_config = wandb_config\n",
    "        wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"fork\"),\n",
    "            project=self.wandb_config['project'],\n",
    "            entity=self.wandb_config['entity'],\n",
    "            notes=self.wandb_config['notes'],\n",
    "            job_type=self.wandb_config['job_type'],\n",
    "            config={**self.data_config, **self.diffusion_config, **self.model_config, **self.train_config}\n",
    "        )\n",
    "        self.exp_name = self.wandb_config['exp_name']\n",
    "        wandb.run.name = self.exp_name\n",
    "\n",
    "        # Logging\n",
    "        self.dataset = self.data_config['dataset']\n",
    "        self.model_dir = os.path.join(\"experiments\", self.exp_name)\n",
    "        os.makedirs(self.model_dir,exist_ok=True)\n",
    "        log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))\n",
    "        log_name = f\"{self.dataset}_{log_name}\"\n",
    "        \n",
    "        self.log = logging.getLogger()\n",
    "        self.log.setLevel(logging.INFO)\n",
    "        log_dir = os.path.join(self.model_dir, log_name)\n",
    "        file_handler = logging.FileHandler(log_dir)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        self.log.addHandler(file_handler)\n",
    "        \n",
    "        self.log_loss_every_steps = self.train_config['log_loss_every_steps']        \n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Build Components\n",
    "        self._build_train_dataloader()\n",
    "        self._build_test_dataloader()\n",
    "        self._build_model()\n",
    "        self._build_optimizer()\n",
    "        \n",
    "        # Move model to GPU\n",
    "        \n",
    "        self.model.to(self.device, non_blocking=True)\n",
    "        print(\"device\", self.device)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the diffusion-based trajectory prediction model.\n",
    "\n",
    "        This function performs the training of the diffusion-based trajectory prediction model. It iterates over the specified number of epochs and updates the model's parameters based on the training data. The training process includes forward propagation, loss calculation, gradient computation, and parameter updates.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        dif = make_diffusion(self.diffusion_config, self.model_config, num_edges=self.num_edges, future_len=self.future_len, device=self.device)\n",
    "        def model_fn(x, edge_index, t, condition=None):\n",
    "            if self.model_config['name'] == 'edge_encoder':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                return self.model.forward(x, t=t, condition=condition, mode='future')\n",
    "                \n",
    "        for epoch in tqdm(range(self.num_epochs)):\n",
    "            current_lr = self.scheduler.get_last_lr()[0]\n",
    "            wandb.log({\"epoch\": epoch, \"learning_rate\": current_lr})\n",
    "            \n",
    "            total_loss = 0\n",
    "            ground_truth_fut = []\n",
    "            pred_fut = []\n",
    "            #with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                #with record_function(\"model_training\"):\n",
    "            if self.gradient_accumulation:\n",
    "                for data in self.train_data_loader:\n",
    "                    history_edge_features = data[\"history_edge_features\"]\n",
    "                    future_edge_indices_one_hot = data[\"future_edge_features\"][:, :, 0]\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    for i in range(min(self.gradient_accumulation_steps, history_edge_features.size(0))):\n",
    "                        # Calculate history condition c\n",
    "                        \n",
    "                        if self.model_config['name'] == 'edge_encoder':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, mode='history')\n",
    "                        elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, mode='history')\n",
    "                        elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                            c = self.model.forward(x=history_edge_features[i].unsqueeze(0), mode='history')\n",
    "                        else:\n",
    "                            raise NotImplementedError(self.model_config['name'])\n",
    "                        \n",
    "                        x_start = future_edge_indices_one_hot[i].unsqueeze(0)   # (1, num_edges)\n",
    "                        # Get loss and predictions\n",
    "                        loss, preds = dif.training_losses(model_fn, c, x_start=x_start, edge_features=history_edge_features[i].unsqueeze(0), edge_index=self.edge_index, line_graph=None)   # preds are of shape (num_edges,)\n",
    "                        \n",
    "                        total_loss += loss / self.gradient_accumulation_steps\n",
    "                        (loss / self.gradient_accumulation_steps).backward() # Gradient accumulation\n",
    "                        \n",
    "                        ground_truth_fut.append(x_start.detach())\n",
    "                        pred_fut.append(preds.detach())\n",
    "                        \n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "            else:\n",
    "                for data in self.train_data_loader:\n",
    "                    history_edge_features = data[\"history_edge_features\"]\n",
    "                    future_edge_indices_one_hot = data[\"future_edge_features\"][:, :, 0]\n",
    "                    \n",
    "                    batch_size = future_edge_indices_one_hot.size(0)\n",
    "                    if self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                        if batch_size == self.batch_size:\n",
    "                            future_edge_indices_one_hot = future_edge_indices_one_hot.view(self.batch_size, self.num_edges)\n",
    "                        else:\n",
    "                            future_edge_indices_one_hot = future_edge_indices_one_hot.view(batch_size, self.num_edges)\n",
    "                    \n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Calculate history condition c\n",
    "                    if self.model_config['name'] == 'edge_encoder':\n",
    "                        c = self.model.forward(x=history_edge_features, edge_index=self.edge_index, mode='history')\n",
    "                    elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                        c = self.model.forward(x=history_edge_features, edge_index=self.edge_index, mode='history')\n",
    "                    elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                        c = self.model.forward(x=history_edge_features, mode='history')\n",
    "                    else:\n",
    "                        raise NotImplementedError(self.model_config['name'])\n",
    "                    \n",
    "                    x_start = future_edge_indices_one_hot\n",
    "                    # Get loss and predictions\n",
    "                    loss, preds = dif.training_losses(model_fn, c, x_start=x_start, edge_features=history_edge_features, edge_index=self.edge_index, line_graph=None)\n",
    "                                        \n",
    "                    total_loss += loss\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                    ground_truth_fut.append(x_start.detach())\n",
    "                    pred_fut.append(preds.detach())\n",
    "            \n",
    "            self.scheduler.step()\n",
    "                    \n",
    "            avg_loss = total_loss / len(self.train_data_loader)\n",
    "            f1_score = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1_epoch = f1_score(torch.flatten(torch.cat(pred_fut)).detach().to('cpu'), torch.flatten(torch.cat(ground_truth_fut)).detach().to('cpu'))\n",
    "            if epoch % self.log_loss_every_steps == 0:\n",
    "                wandb.log({\"epoch\": epoch, \"average_loss\": avg_loss.item()})\n",
    "                wandb.log({\"epoch\": epoch, \"average_F1_score\": f1_epoch.item()})\n",
    "                self.log.info(f\"Epoch {epoch} Average Loss: {avg_loss.item()}\")\n",
    "                print(\"Epoch:\", epoch+1)\n",
    "                print(\"Loss:\", avg_loss.item())\n",
    "                print(\"F1:\", f1_epoch.item())\n",
    "                \n",
    "            if (epoch + 1) % self.eval_every_steps == 0:\n",
    "                print(\"Evaluating on test set...\")\n",
    "                sample_list, ground_truth_hist, ground_truth_fut = self.get_samples(task='predict')\n",
    "                fut_ratio, f1, avg_sample_length = self.eval(sample_list, ground_truth_hist, ground_truth_fut)\n",
    "                print(\"Samples\", sample_list)\n",
    "                print(\"Ground truth\", ground_truth_fut)\n",
    "                print(\"Test F1 Score\", f1.item())\n",
    "                wandb.log({\"Test F1 Score\": f1.item()})\n",
    "                wandb.log({\"Test Future ratio\": fut_ratio})\n",
    "                wandb.log({\"Average test sample length\": avg_sample_length})\n",
    "                        \n",
    "            if self.train_config['save_model'] and (epoch + 1) % self.train_config['save_model_every_steps'] == 0:\n",
    "                self.save_model()\n",
    "            \n",
    "    def get_samples(self, load_model=False, model_path=None, task='predict', number_samples=1, save=False):\n",
    "        \"\"\"\n",
    "        Retrieves samples from the model.\n",
    "\n",
    "        Args:\n",
    "            load_model (bool, optional): Whether to load a pre-trained model. Defaults to False.\n",
    "            model_path (str, optional): The path to the pre-trained model. Required if `load_model` is True.\n",
    "            task (str, optional): The task to perform. Defaults to 'predict'. Other possible value: 'generate' to generate realistic trajectories\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing three lists:\n",
    "                - sample_list (list): A list of samples generated by the model.\n",
    "                - ground_truth_hist (list): A list of ground truth history edge indices.\n",
    "                - ground_truth_fut (list): A list of ground truth future trajectory indices.\n",
    "        \"\"\"\n",
    "        \n",
    "        if load_model:\n",
    "            if model_path is None:\n",
    "                raise ValueError(\"Model path must be provided to load model.\")\n",
    "            self.load_model(model_path)\n",
    "        \n",
    "        if self.test_config['number_samples'] is not None:\n",
    "            number_samples = self.test_config['number_samples']\n",
    "        \n",
    "        def model_fn(x, edge_index, t, condition=None):\n",
    "            if self.model_config['name'] == 'edge_encoder':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                return self.model.forward(x, edge_index, t, condition, mode='future')\n",
    "            elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                return self.model.forward(x=x, t=t, condition=condition, mode='future')\n",
    "        \n",
    "        sample_list = []\n",
    "        ground_truth_hist = []\n",
    "        ground_truth_fut = []\n",
    "        \n",
    "        if task == 'predict':\n",
    "            for data in tqdm(self.test_dataloader):\n",
    "                history_edge_features = data[\"history_edge_features\"]\n",
    "\n",
    "                history_edge_indices = data[\"history_indices\"]\n",
    "\n",
    "                future_trajectory_indices = data[\"future_indices\"]\n",
    "                # with torch.no_grad():\n",
    "                if self.model_config['name'] == 'edge_encoder':\n",
    "                    c = self.model.forward(x=self.line_graph.x, edge_index=self.line_graph.edge_index, mode='history')\n",
    "                elif self.model_config['name'] == 'edge_encoder_residual':\n",
    "                    c = self.model.forward(x=self.line_graph.x, edge_index=self.line_graph.edge_index, mode='history')\n",
    "                elif self.model_config['name'] == 'edge_encoder_mlp':\n",
    "                    c = self.model.forward(x=history_edge_features, mode='history')\n",
    "            \n",
    "                if number_samples > 1:\n",
    "                    new_seed = torch.seed() + torch.randint(0, 100000, (1,)).item()\n",
    "                    torch.manual_seed(new_seed)\n",
    "                    sample_sublist = []\n",
    "                    for _ in range(number_samples):\n",
    "                        samples = make_diffusion(self.diffusion_config, self.model_config, \n",
    "                                                num_edges=self.num_edges, future_len=self.future_len).p_sample_loop(model_fn=model_fn,\n",
    "                                                                                        shape=(self.test_batch_size, self.num_edges),\n",
    "                                                                                        edge_features=history_edge_features,\n",
    "                                                                                        edge_index=self.edge_index,\n",
    "                                                                                        line_graph=None,\n",
    "                                                                                        condition=c)\n",
    "                        samples = torch.where(samples == 1)[1]\n",
    "                        sample_sublist.append(samples.detach())\n",
    "                    sample_list.append(sample_sublist)\n",
    "                elif number_samples == 1:\n",
    "                    samples = make_diffusion(self.diffusion_config, self.model_config, \n",
    "                                            num_edges=self.num_edges, future_len=self.future_len, device=self.device).p_sample_loop(model_fn=model_fn,\n",
    "                                                                                    shape=(self.test_batch_size, self.num_edges), \n",
    "                                                                                    edge_features=history_edge_features,\n",
    "                                                                                    edge_index=self.edge_index,\n",
    "                                                                                    line_graph=None,\n",
    "                                                                                    condition=c)\n",
    "                    samples = torch.where(samples == 1)[1]\n",
    "                    sample_list.append(samples.detach())\n",
    "                else:\n",
    "                    raise ValueError(\"Number of samples must be greater than 0.\")\n",
    "                ground_truth_hist.append(history_edge_indices.detach())\n",
    "                ground_truth_fut.append(future_trajectory_indices.detach())\n",
    "            \n",
    "            if number_samples == 1:\n",
    "                fut_ratio, f1, avg_sample_length = self.eval(sample_list, ground_truth_hist, ground_truth_fut)\n",
    "                wandb.log({\"F1 Score\": f1.item()})\n",
    "                wandb.log({\"Future ratio\": fut_ratio})\n",
    "                wandb.log({\"Average sample length\": avg_sample_length})\n",
    "            \n",
    "            if save:\n",
    "                torch.save(sample_list, os.path.join(self.model_dir, f'{self.exp_name}_samples.pth'))\n",
    "                torch.save(ground_truth_hist, os.path.join(self.model_dir, f'{self.exp_name}_ground_truth_hist.pth'))\n",
    "                torch.save(ground_truth_fut, os.path.join(self.model_dir, f'{self.exp_name}_ground_truth_fut.pth'))\n",
    "                print(f\"Samples saved at {os.path.join(self.model_dir, f'{self.exp_name}_samples.pth')}!\")\n",
    "            else:\n",
    "                return sample_list, ground_truth_hist, ground_truth_fut\n",
    "        \n",
    "        elif task == 'generate':\n",
    "            # Generate realistic trajectories without condition\n",
    "            # Edge encoder model needs to be able to funciton with no edge_attr and no condition\n",
    "            # Add generate mode to p_logits, p_sample, and p_sample_loop\n",
    "            return\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(task)\n",
    "    \n",
    "    def visualize_sample_density(self, samples, ground_truth_hist, ground_truth_fut, number_plots=5, number_samples=10):\n",
    "        \"\"\"\n",
    "        Visualize the density of the samples generated by the model.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        samples, ground_truth_hist, ground_truth_fut = self.get_samples(load_model=True, model_path=self.test_config['model_path'], number_samples=number_samples)\n",
    "        save_dir = f'{os.path.join(self.model_dir, f'{self.exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.nodes)\n",
    "        all_edges = {tuple(self.edges[idx]) for idx in range(len(self.edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "\n",
    "        pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "        for i in range(min(number_plots, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i][0]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i][0]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'\n",
    "                if plot_num == 2:\n",
    "                    edge_counts = np.zeros(len(all_edges))\n",
    "                    for sample in samples[i]:\n",
    "                        for edge in sample:\n",
    "                            edge_counts[edge] += 1\n",
    "                    max_count = np.max(edge_counts)\n",
    "                    edge_widths = edge_counts / max_count\n",
    "                    \n",
    "                    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                    nx.draw_networkx_edges(G, pos, edgelist=all_edges, edge_color='red', width=edge_widths*5, alpha=edge_widths/np.max(edge_widths))\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "                else:\n",
    "                    subgraph_edges = {tuple(self.edges[idx]) for idx in edge_indices if idx < len(self.edges)}\n",
    "                    nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                    nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            # Save plot\n",
    "            plt.savefig(os.path.join(save_dir, f'sample_{i+1}.png'))\n",
    "            plt.close()  # Close the figure to free memory\n",
    "    \n",
    "    def visualize_predictions(self, samples, ground_truth_hist, ground_truth_fut, number_plots=5):\n",
    "        \"\"\"\n",
    "        Visualize the predictions of the model along with ground truth data.\n",
    "\n",
    "        :param samples: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        :param number_plots: Number of samples to visualize.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        save_dir = f'{os.path.join(self.model_dir, f'{self.exp_name}', 'plots')}'\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(self.nodes)\n",
    "        all_edges = {tuple(self.edges[idx]) for idx in range(len(self.edges))}\n",
    "        G.add_edges_from(all_edges)\n",
    "        \n",
    "        pos = nx.get_node_attributes(G, 'pos')  # Retrieve node positions stored in node attributes\n",
    "\n",
    "        for i in range(min(number_plots, len(samples))):\n",
    "            plt.figure(figsize=(18, 8))            \n",
    "\n",
    "            for plot_num, (title, edge_indices) in enumerate([\n",
    "                ('Ground Truth History', ground_truth_hist[i][0]),\n",
    "                ('Ground Truth Future', ground_truth_fut[i][0]),\n",
    "                ('Predicted Future', samples[i])\n",
    "            ]):\n",
    "                plt.subplot(1, 3, plot_num + 1)\n",
    "                plt.title(title)\n",
    "                subgraph_edges = {tuple(self.edges[idx]) for idx in edge_indices if idx < len(self.edges)}\n",
    "\n",
    "                # Draw all edges as muted gray\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=all_edges, width=0.5, alpha=0.3, edge_color='gray')\n",
    "\n",
    "                # Draw subgraph edges with specified color\n",
    "                edge_color = 'gray' if plot_num == 0 else 'green' if plot_num == 1 else 'red'\n",
    "                node_color = 'skyblue'# if plot_num == 0 else 'lightgreen' if plot_num == 1 else 'orange'\n",
    "                nx.draw_networkx_nodes(G, pos, node_color=node_color, node_size=500)\n",
    "                nx.draw_networkx_edges(G, pos, edgelist=subgraph_edges, width=3, alpha=1.0, edge_color=edge_color)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=15)\n",
    "            # Save plot\n",
    "            plt.savefig(os.path.join(save_dir, f'sample_{i+1}.png'))\n",
    "            plt.close()  # Close the figure to free memory\n",
    "    \n",
    "    def eval(self, sample_list, ground_truth_hist, ground_truth_fut):\n",
    "        \"\"\"\n",
    "        Evaluate the model's performance.\n",
    "\n",
    "        :param sample_list: A list of predicted edge indices.\n",
    "        :param ground_truth_hist: A list of actual history edge indices.\n",
    "        :param ground_truth_fut: A list of actual future edge indices.\n",
    "        \"\"\"\n",
    "        def calculate_fut_ratio(sample_list, ground_truth_fut):\n",
    "            \"\"\"\n",
    "            Calculates the ratio of samples in `sample_list` that have at least one or two edges in common with the ground truth future trajectory.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "                ground_truth_fut (list): A list of ground truth future trajectories.\n",
    "\n",
    "            Returns:\n",
    "                tuple: A tuple containing the ratios of samples that have at least one or two edges in common with the ground truth future trajectory.\n",
    "            \"\"\"\n",
    "            count_1 = 0\n",
    "            count_2 = 0\n",
    "            total = len(sample_list)\n",
    "\n",
    "            for i, sample in enumerate(sample_list):\n",
    "                edges_count = sum(1 for edge in ground_truth_fut[i][0] if edge in sample)\n",
    "                if edges_count >= 1:\n",
    "                    count_1 += 1\n",
    "                if edges_count >= 2:\n",
    "                    count_2 += 1\n",
    "\n",
    "            ratio_1 = count_1 / total\n",
    "            ratio_2 = count_2 / total\n",
    "            return ratio_1, ratio_2\n",
    "        \n",
    "        def calculate_sample_f1(sample_list, ground_truth_fut):\n",
    "            \"\"\"\n",
    "            Calculates the F1 score for a given list of samples and ground truth futures.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "                ground_truth_fut (list): A list of ground truth futures.\n",
    "\n",
    "            Returns:\n",
    "                float: The F1 score.\n",
    "\n",
    "            \"\"\"\n",
    "            one_hot_samples = [torch.zeros(self.num_edges) for _ in range(len(sample_list))]\n",
    "            one_hot_futures = [torch.zeros(self.num_edges) for _ in range(len(ground_truth_fut))]\n",
    "            for i, one_hot_sample in enumerate(one_hot_samples):\n",
    "                for edge_index, edge in enumerate(self.edges):\n",
    "                    if edge_index in sample_list[i]:\n",
    "                        one_hot_sample[edge_index] = 1\n",
    "            for i, one_hot_fut in enumerate(one_hot_futures):\n",
    "                for edge_index, edge in enumerate(self.edges):\n",
    "                    if edge_index in ground_truth_fut[i]:\n",
    "                        one_hot_fut[edge_index] = 1\n",
    "            metric = F1Score(task='binary', average='macro', num_classes=2)\n",
    "            f1 = metric(torch.cat(one_hot_samples), torch.cat(one_hot_futures))\n",
    "\n",
    "            return f1\n",
    "        \n",
    "        def calculate_avg_sample_length(sample_list):\n",
    "            \"\"\"\n",
    "            Calculate the average sample length.\n",
    "\n",
    "            Args:\n",
    "                sample_list (list): A list of samples.\n",
    "\n",
    "            Returns:\n",
    "                float: The average sample length.\n",
    "            \"\"\"\n",
    "            return sum(len(sample) for sample in sample_list) / len(sample_list)\n",
    "        \n",
    "        fut_ratio = calculate_fut_ratio(sample_list, ground_truth_fut)\n",
    "        f1 = calculate_sample_f1(sample_list, ground_truth_fut)\n",
    "        avg_sample_length = calculate_avg_sample_length(sample_list)\n",
    "        \n",
    "        return fut_ratio, f1, avg_sample_length\n",
    "    \n",
    "    def save_model(self):\n",
    "        save_path = os.path.join(self.model_dir, \n",
    "                                 self.exp_name + '_' + self.model_config['name'] + '_' +  self.model_config['transition_mat_type'] + '_' +  self.diffusion_config['type'] + \n",
    "                                 f'_hidden_dim_{self.hidden_channels}_time_dim_{str(self.time_embedding_dim)}_condition_dim_{self.condition_dim}_layers_{self.num_layers}.pth')\n",
    "        torch.save(self.model.state_dict(), save_path)\n",
    "        self.log.info(f\"Model saved at {save_path}!\")\n",
    "        print(f\"Model saved at {save_path}\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.log.info(\"Model loaded!\")\n",
    "    \n",
    "    def _build_optimizer(self):\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.learning_rate_warmup_steps:\n",
    "                return 1.0\n",
    "            else:\n",
    "                decay_lr = self.lr_decay_parameter ** (epoch - self.learning_rate_warmup_steps)\n",
    "                return max(decay_lr, 2e-5 / self.lr)\n",
    "            \n",
    "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "        print(\"> Optimizer and Scheduler built!\")\n",
    "        \n",
    "        \"\"\"print(\"Parameters to optimize:\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name)\"\"\"\n",
    "        \n",
    "    def _build_train_dataloader(self):\n",
    "        print(\"Loading Training Dataset...\")\n",
    "        self.train_dataset = TrajectoryDataset(self.train_data_path, self.history_len, self.future_len, self.edge_features, device=self.device)\n",
    "        self.G = self.train_dataset.build_graph()\n",
    "        self.nodes = self.G.nodes\n",
    "        self.edges = self.G.edges(data=True)\n",
    "        self.num_edges = self.G.number_of_edges()\n",
    "        self.indexed_edges = self.train_dataset.edges\n",
    "        self.num_edge_features = self.train_dataset.num_edge_features\n",
    "        \n",
    "        # Build the line graph and corresponding edge index\n",
    "        self.edge_index = self._build_edge_index()\n",
    "                \n",
    "        self.train_data_loader = DataLoader(self.train_dataset, \n",
    "                                            batch_size=self.batch_size, \n",
    "                                            shuffle=True, \n",
    "                                            collate_fn=collate_fn, \n",
    "                                            num_workers=0,\n",
    "                                            pin_memory=False)\n",
    "                        \n",
    "        print(\"> Training Dataset loaded!\\n\")\n",
    "        \n",
    "    def _build_edge_index(self):\n",
    "        print(\"Building edge index for line graph...\")\n",
    "        edge_index = torch.tensor([[e[0], e[1]] for e in self.edges], dtype=torch.long).t().contiguous()\n",
    "        edge_to_index = {tuple(e[:2]): e[2]['index'] for e in self.edges}\n",
    "        line_graph_edges = []\n",
    "        edge_list = edge_index.t().tolist()\n",
    "        for i, (u1, v1) in tqdm(enumerate(edge_list), total=len(edge_list), desc=\"Processing edges\"):\n",
    "            for j, (u2, v2) in enumerate(edge_list):\n",
    "                if i != j and (u1 == u2 or u1 == v2 or v1 == u2 or v1 == v2):\n",
    "                    line_graph_edges.append((edge_to_index[(u1, v1)], edge_to_index[(u2, v2)]))\n",
    "\n",
    "        # Create the edge index for the line graph\n",
    "        edge_index = torch.tensor(line_graph_edges, dtype=torch.long).t().contiguous()\n",
    "        print(\"> Edge index built!\\n\")\n",
    "        \n",
    "        return edge_index.to(self.device, non_blocking=True)\n",
    "    \n",
    "    def _build_test_dataloader(self):\n",
    "        self.test_dataset = TrajectoryDataset(self.val_data_path, self.history_len, self.future_len, self.edge_features, device=self.device)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        print(\"> Test Dataset loaded!\")\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.model = self.model(self.model_config, self.history_len, self.future_len, self.num_classes,\n",
    "                                num_edges=self.num_edges, hidden_channels=self.hidden_channels, num_edge_features=self.num_edge_features, num_timesteps=self.num_timesteps)\n",
    "        print(\"> Model built!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim, max_time=1000., device=None):\n",
    "    \"\"\"\n",
    "    Build sinusoidal embeddings (from Fairseq).\n",
    "\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "\n",
    "    Args:\n",
    "        timesteps: torch.Tensor: generate embedding vectors at these timesteps\n",
    "        embedding_dim: int: dimension of the embeddings to generate\n",
    "        max_time: float: largest time input\n",
    "\n",
    "    Returns:\n",
    "        embedding vectors with shape `(len(timesteps), embedding_dim)`\n",
    "    \"\"\"\n",
    "    timesteps = timesteps.to(device)\n",
    "    assert timesteps.dim() == 1  # Ensure timesteps is a 1D tensor\n",
    "\n",
    "    # Scale timesteps by the maximum time\n",
    "    timesteps = timesteps.float() * (1000. / max_time)\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = torch.log(torch.tensor(10000.0, device=device)) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32, device=device) * -emb)\n",
    "    emb = timesteps[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "\n",
    "    if embedding_dim % 2 == 1:  # Add zero-padding if embedding dimension is odd\n",
    "        zero_pad = torch.zeros((timesteps.shape[0], 1), dtype=torch.float32)\n",
    "        emb = torch.cat([emb, zero_pad], dim=1)\n",
    "\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return emb.to(device)\n",
    "\n",
    "class Edge_Encoder_MLP(nn.Module):\n",
    "    def __init__(self, model_config, history_len, future_len, num_classes, num_edges, hidden_channels, num_edge_features, num_timesteps):\n",
    "        super(Edge_Encoder_MLP, self).__init__()\n",
    "        # Config\n",
    "        self.config = model_config\n",
    "        \n",
    "        # Data\n",
    "        self.num_edges = num_edges\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.history_len = history_len\n",
    "        self.future_len = future_len\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Time embedding\n",
    "        self.max_time = num_timesteps\n",
    "        self.time_embedding_dim = self.config['time_embedding_dim']\n",
    "        self.time_linear0 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "        self.time_linear1 = nn.Linear(self.time_embedding_dim, self.time_embedding_dim)\n",
    "    \n",
    "        # Model\n",
    "        # GNN layers\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = self.config['num_layers']\n",
    "        self.lin_layers = nn.ModuleList()\n",
    "        self.lin_layers.append(nn.Linear(self.num_edge_features, self.hidden_channels))\n",
    "        for _ in range(1, self.num_layers):\n",
    "            self.lin_layers.append(nn.Linear(self.hidden_channels, self.hidden_channels))\n",
    "        \n",
    "        # Output layers for each task\n",
    "        self.condition_dim = self.config['condition_dim']\n",
    "        self.history_encoder = nn.Linear(self.hidden_channels, self.condition_dim)  # To encode history to c\n",
    "        self.future_decoder = nn.Linear(self.hidden_channels + self.condition_dim + self.time_embedding_dim,\n",
    "                                        self.hidden_channels)  # To predict future edges\n",
    "        self.adjust_to_class_shape = nn.Linear(self.hidden_channels, self.num_classes)\n",
    "\n",
    "    def forward(self, x, t=None, condition=None, mode=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        Args:\n",
    "            x: torch.Tensor: input tensor: noised future trajectory indices / history trajectory indices\n",
    "            t: torch.Tensor: timestep tensor\n",
    "        \"\"\"    \n",
    "        \n",
    "        # GNN forward pass\n",
    "        \n",
    "        # Edge Embedding        \n",
    "        for layer in self.lin_layers:\n",
    "            x = F.relu(layer(x))\n",
    "\n",
    "        if mode == 'history':\n",
    "            c = self.history_encoder(x) # (bs, num_edges, condition_dim)\n",
    "            return c\n",
    "        \n",
    "        elif mode == 'future':\n",
    "            # Time embedding\n",
    "            t_emb = get_timestep_embedding(t, embedding_dim=self.time_embedding_dim, max_time=self.max_time, device=x.device)\n",
    "            t_emb = self.time_linear0(t_emb)\n",
    "            t_emb = F.silu(t_emb)  # SiLU activation, equivalent to Swish\n",
    "            t_emb = self.time_linear1(t_emb)\n",
    "            t_emb = F.silu(t_emb)   # (bs, time_embedding_dim)\n",
    "            t_emb = t_emb.unsqueeze(1).repeat(1, x.size(1), 1) # (bs, num_edges, time_embedding_dim)\n",
    "            \n",
    "            #Concatenation\n",
    "            x = torch.cat((x, t_emb), dim=2) # Concatenate with time embedding\n",
    "            x = torch.cat((x, condition), dim=2) # Concatenate with condition c, (bs, num_edges, hidden_channels + condition_dim + time_embedding_dim)\n",
    "            \n",
    "            logits = self.future_decoder(x) # (bs, num_edges, hidden_channels)\n",
    "            logits = self.adjust_to_class_shape(logits) # (bs, num_edges, num_classes=2)\n",
    "\n",
    "            return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_samples.pth\", map_location=torch.device('cpu'))\n",
    "hist = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_ground_truth_hist.pth\", map_location=torch.device('cpu'))\n",
    "fut = torch.load(\"/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/tdrive/tdrive_ground_truth_fut.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoeschmit99\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/models/wandb/run-20240627_072506-8xre9aof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/8xre9aof' target=\"_blank\">colorful-butterfly-1462</a></strong> to <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/8xre9aof' target=\"_blank\">https://wandb.ai/joeschmit99/trajectory_prediction_using_denoising_diffusion_models/runs/8xre9aof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2680938/1910163089.py:243: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
      "100%|██████████| 6135/6135 [00:05<00:00, 1057.48it/s]\n",
      "/tmp/ipykernel_2680938/1910163089.py:233: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.edge_coordinates = torch.tensor(self.edge_coordinates, dtype=torch.float64, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building edge index for line graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing edges: 100%|██████████| 16784/16784 [00:26<00:00, 622.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Edge index built!\n",
      "\n",
      "> Training Dataset loaded!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [00:00<00:00, 1074.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Test Dataset loaded!\n",
      "> Model built!\n",
      "> Optimizer and Scheduler built!\n",
      "device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#encoder_model = Edge_Encoder\n",
    "encoder_model = Edge_Encoder_MLP\n",
    "#encoder_model = Edge_Encoder_Residual\n",
    "\n",
    "\n",
    "    \n",
    "data_config = {\"dataset\": \"synthetic_20_traj\",\n",
    "    \"train_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_train.h5',\n",
    "    \"val_data_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_val.h5',\n",
    "    \"history_len\": 5,\n",
    "    \"future_len\": 2,\n",
    "    \"num_classes\": 2,\n",
    "    \"edge_features\": ['one_hot_edges', 'coordinates']\n",
    "    }\n",
    "\n",
    "diffusion_config = {\"type\": 'linear', # Options: 'linear', 'cosine', 'jsd'\n",
    "    \"start\": 0.9,  # 1e-4 gauss, 0.02 uniform\n",
    "    \"stop\": 1.0,  # 0.02 gauss, 1. uniform\n",
    "    \"num_timesteps\": 200}\n",
    "\n",
    "model_config = {\"name\": \"edge_encoder_mlp\",\n",
    "    \"hidden_channels\": 32,\n",
    "    \"time_embedding_dim\": 16,\n",
    "    \"condition_dim\": 16,\n",
    "    \"out_ch\": 1,\n",
    "    \"num_heads\": 1,\n",
    "    \"num_layers\": 2,\n",
    "    \"theta\": 1.0, # controls strength of conv layers in residual model\n",
    "    \"dropout\": 0.1,\n",
    "    \"model_output\": \"logits\",\n",
    "    \"model_prediction\": \"x_start\",  # Options: 'x_start','xprev'\n",
    "    \"transition_mat_type\": 'marginal_prior',  # Options: 'gaussian','uniform','absorbing', 'marginal_prior'\n",
    "    \"transition_bands\": 1,\n",
    "    \"loss_type\": \"cross_entropy_x_start\",  # Options: kl, cross_entropy_x_start, hybrid\n",
    "    \"hybrid_coeff\": 0.001,  # Only used for hybrid loss type.\n",
    "    \"class_weights\": [0.05, 0.95] # = future_len/num_edges and (num_edges - future_len)/num_edges\n",
    "    }\n",
    "\n",
    "train_config = {\"batch_size\": 32,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"lr\": 0.01,\n",
    "    \"gradient_accumulation\": False,\n",
    "    \"gradient_accumulation_steps\": 128,\n",
    "    \"num_epochs\": 500,\n",
    "    \"learning_rate_warmup_steps\": 2000, # previously 10000\n",
    "    \"lr_decay\": 0.9999, # previously 0.9999\n",
    "    \"log_loss_every_steps\": 1,\n",
    "    \"save_model\": True,\n",
    "    \"save_model_every_steps\": 50}\n",
    "\n",
    "test_config = {\"batch_size\": 1, # currently only 1 works\n",
    "    \"model_path\": '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/experiments/synthetic_d3pm_residual_fixed/synthetic_d3pm_residual_fixed_hidden_dim_32_time_dim_16_condition_dim_16_layers_2_weights_0.1.pth',\n",
    "    \"number_samples\": 1,\n",
    "    \"eval_every_steps\": 1000\n",
    "  }\n",
    "\n",
    "wandb_config = {\"exp_name\": \"tdrive_d3pm_test\",\n",
    "    \"project\": \"trajectory_prediction_using_denoising_diffusion_models\",\n",
    "    \"entity\": \"joeschmit99\",\n",
    "    \"job_type\": \"eval\",\n",
    "    \"notes\": \"\",\n",
    "    \"tags\": [\"tdrive\", \"edge_encoder\"]} \n",
    "\n",
    "model = Graph_Diffusion_Model(data_config, diffusion_config, model_config, train_config, test_config, wandb_config, model=encoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.7722222222222223, 0.7333333333333333), tensor(0.0005), 5720.158333333334)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fut_ratio, f1, avg_sample_length\n",
    "model.eval(sample_list=samples, ground_truth_hist=hist, ground_truth_fut=fut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
