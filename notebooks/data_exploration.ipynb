{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/src/MA_Diffusion_base_trajectory_prediction')\n",
    "\n",
    "from utils.data_utils import TDRIVE, GEOLIFE, load_data, calculate_bbox_and_filter, \\\n",
    "    plot_coordinates, plot_paths, load_new_format, find_cycles, split_cycle_in_paths, \\\n",
    "    plot_histograms_before_after_split, \\\n",
    "    get_edge_used_by_trajectories, modify_and_save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHICH = TDRIVE\n",
    "# Paths to cleaned and filtered data\n",
    "GEOLIFE_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/geolife.h5'\n",
    "TDRIVE_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive.h5'\n",
    "MUNICH_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/munich.h5'\n",
    "# pNEUMA dataset is not in the correct format yet\n",
    "PNEUMA_PATH = '/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/pNEUMA.h5'\n",
    "# merged_path = GEOLIFE_PATH if WHICH == GEOLIFE else TDRIVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_format(file_path, edge_features, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            # Normalize the coordinates to (0, 1) if any of the coordinates is larger than 1\n",
    "            if node_coordinates.max() > 1:\n",
    "                max_values = node_coordinates.max(0)[0]\n",
    "                min_values = node_coordinates.min(0)[0]\n",
    "                node_coordinates[:, 0] = (node_coordinates[:, 0] - min_values[0]) / (max_values[0] - min_values[0])\n",
    "                node_coordinates[:, 1] = (node_coordinates[:, 1] - min_values[1]) / (max_values[1] - min_values[1])\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            if 'road_type' in edge_features:\n",
    "                if 'highway' in new_hf['graph']['edge_features'].keys():\n",
    "                    road_type = new_hf['graph']['edge_features']['highway'][()]\n",
    "                    road_type = [byte_string.decode('utf-8')[1:-1] for byte_string in road_type]\n",
    "                    road_type_clean = []\n",
    "                    for string in road_type:\n",
    "                        # Split the string by comma, remove duplicates, and join back\n",
    "                        cleaned_string = string.split(',')[0]\n",
    "                        road_type_clean.append(cleaned_string)\n",
    "                    unique_labels = list(set(road_type_clean))\n",
    "                    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "                    # Convert string labels to integer labels using the mapping\n",
    "                    integer_encoded = [label_to_index[label] for label in road_type_clean]\n",
    "\n",
    "                    # Step 4: One-hot encode the numerical labels (optional)\n",
    "                    integer_encoded_tensor = torch.tensor(integer_encoded, dtype=torch.long, device=device)\n",
    "                    num_classes = len(unique_labels)\n",
    "                    onehot_encoded_road_type = torch.nn.functional.one_hot(integer_encoded_tensor, num_classes=num_classes)\n",
    "                    return paths, nodes, edges, edge_coordinates, onehot_encoded_road_type\n",
    "            else:\n",
    "                return paths, nodes, edges, edge_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48748/1402280330.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
      "100%|██████████| 7218/7218 [00:06<00:00, 1054.71it/s]\n"
     ]
    }
   ],
   "source": [
    "paths, node_coord, edges, edge_coordinates, roads = load_new_format(TDRIVE_PATH, ['coord', 'road_type'], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16784, 9])\n"
     ]
    }
   ],
   "source": [
    "print(roads.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to include\n",
    "### T-Drive\n",
    "highway,\n",
    "lanes (mostly 0),\n",
    "length (mostly nan),\n",
    "maxspeed (mostly nan)\n",
    "### pNEUMA\n",
    "highway,\n",
    "lanes,\n",
    "length,\n",
    "maxspeed,\n",
    "tunnel (maybe)\n",
    "### Geolife\n",
    "highway,\n",
    "lanes,\n",
    "length,\n",
    "maxspeed\n",
    "### Munich\n",
    "None\\\n",
    "\n",
    "# --> Only use highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coordinates': tensor([[0.7536, 0.5798],\n",
      "        [0.7536, 0.5799],\n",
      "        [0.7536, 0.5800],\n",
      "        [0.7536, 0.5798],\n",
      "        [0.7535, 0.5800],\n",
      "        [0.7349, 0.5560],\n",
      "        [0.7642, 0.4845],\n",
      "        [0.7624, 0.3738],\n",
      "        [0.7354, 0.3724],\n",
      "        [0.7335, 0.3581],\n",
      "        [0.7338, 0.3530],\n",
      "        [0.7338, 0.3540],\n",
      "        [0.7339, 0.3562],\n",
      "        [0.7339, 0.3573]], dtype=torch.float64), 'edge_idxs': tensor([15116, 15115, 15117,   470,   471, 16749,   462,   466, 11607, 11606,\n",
      "        11486, 11122,  5276,  3908,  2138,  2137,  4117,  4116,  8745,  9139,\n",
      "         9138,  1169,  1170,  1541,  1540,  5274,    65,    67,   550,  3004,\n",
      "         3001,  8710,  8709, 14955,  9668]), 'edge_orientations': tensor([-1,  1,  1, -1,  1,  1, -1,  1, -1,  1, -1, -1, -1, -1, -1,  1, -1,  1,\n",
      "         1, -1,  1, -1,  1, -1,  1, -1, -1,  1,  1, -1,  1, -1,  1, -1, -1])}\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, node_coordinates, edges = load_new_format(TDRIVE_PATH)\n",
    "# edge_coordinates = node_coordinates[edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Edges\n",
    "num_edges = len(edges)\n",
    "print(\"Number of Edges:\", num_edges)\n",
    "\n",
    "# Number of Nodes\n",
    "num_nodes = len(node_coordinates)\n",
    "print(\"Number of Nodes:\", num_nodes)\n",
    "\n",
    "# Distribution of Path Lengths\n",
    "path_lengths = [len(path['edge_idxs']) for path in paths]\n",
    "\n",
    "avg_length = round(sum(path_lengths) / len(path_lengths), 2)\n",
    "print(f\"Average length: {avg_length} edges\")\n",
    "\n",
    "path = np.random.randint(len(paths))\n",
    "print(f'Exemplary Path: {path}')\n",
    "print(paths[path])\n",
    "print('\\nCoordinates:')\n",
    "#print(paths[path]['coordinates'])\n",
    "print('\\nTimestamps:')\n",
    "#print(paths[path]['timestamps'])\n",
    "print('\\nTaxi Index:')\n",
    "#print(paths[path]['taxi_idx'])\n",
    "print('\\nEdge Indexes:')\n",
    "print(paths[path]['edge_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_paths(paths, node_coordinates, edges, num_paths_to_plot=4, random=False, start_id=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles = find_cycles(paths)\n",
    "print(\"Number of paths with cycles:\", np.sum(cycles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Paths with Cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_with_cycles = list(path for path, mask in zip(paths, cycles) if mask)\n",
    "plot_paths(paths_with_cycles, node_coordinates, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split paths with cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_paths = split_cycle_in_paths(paths)\n",
    "print(\n",
    "    f\"Number of paths before split: {len(paths)}, after split: {len(split_paths)}, ratio: {len(split_paths) / len(paths)}\")\n",
    "print(\"Number of paths with cycles after split:\", np.sum(find_cycles(split_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Splitted Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_paths(split_paths, node_coordinates, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Short Paths (<5 edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges_before_split = np.array([len(path['edge_idxs']) for path in paths])\n",
    "num_edges_after_split = np.array([len(path['edge_idxs']) for path in split_paths])\n",
    "print(\n",
    "    f\"Number of paths smaller than 5 before split: {np.sum(num_edges_before_split < 5)} out of {len(num_edges_before_split)}, ratio: {np.sum(num_edges_before_split < 5) / len(num_edges_before_split)}\")\n",
    "print(\n",
    "    f\"Number of paths smaller than 5 after split: {np.sum(num_edges_after_split < 5)} out of {len(num_edges_after_split)}, ratio: {np.sum(num_edges_after_split < 5) / len(num_edges_after_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_length_cutoff_value = 5\n",
    "longer_paths = [path for path in split_paths if len(path['edge_idxs']) >= path_length_cutoff_value]\n",
    "print(\n",
    "    f\"Number of paths longer than {path_length_cutoff_value}: {len(longer_paths)} out of {len(split_paths)}, ratio: {len(longer_paths) / len(split_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "def load_new_format(file_path, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            # Normalize the coordinates to (0, 1) if any of the coordinates is larger than 1\n",
    "            if node_coordinates.max() > 1:\n",
    "                max_values = node_coordinates.max(0)[0]\n",
    "                min_values = node_coordinates.min(0)[0]\n",
    "                node_coordinates[:, 0] = (node_coordinates[:, 0] - min_values[0]) / (max_values[0] - min_values[0])\n",
    "                node_coordinates[:, 1] = (node_coordinates[:, 1] - min_values[1]) / (max_values[1] - min_values[1])\n",
    "            #edges = torch.tensor(new_hf['graph']['edges'][:], dtype=torch.long, device=device)\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            #edges = [(torch.tensor(edge[0], device=device), torch.tensor(edge[1], device=device)) for edge in edges]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "\n",
    "            '''nodes = [(i, {'pos': tuple(pos)}) for i, pos in enumerate(node_coordinates)]\n",
    "            edges = [tuple(edge) for edge in edges]'''\n",
    "\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                # path = {attr: path_group[attr][()] for attr in path_group.keys()}\n",
    "                paths.append(path)\n",
    "            \n",
    "        return paths, nodes, edges, edge_coordinates\n",
    "    \n",
    "# paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_train.h5', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_train.h5', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/tdrive_train.h5', 'cpu')\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lens)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Path lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/geolife_train.h5', 'cpu')\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lens)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Path lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/pneuma_train.h5', 'cpu')\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lens)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Path lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/munich_train.h5', 'cpu')\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lens)\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Path lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/munich_train.h5', 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_coordinates.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_new_format(file_path, edge_features, device):\n",
    "        paths = []\n",
    "        with h5py.File(file_path, 'r') as new_hf:\n",
    "            node_coordinates = torch.tensor(new_hf['graph']['node_coordinates'][:], dtype=torch.float, device=device)\n",
    "            # Normalize the coordinates to (0, 1) if any of the coordinates is larger than 1\n",
    "            if node_coordinates.max() > 1:\n",
    "                max_values = node_coordinates.max(0)[0]\n",
    "                min_values = node_coordinates.min(0)[0]\n",
    "                node_coordinates[:, 0] = (node_coordinates[:, 0] - min_values[0]) / (max_values[0] - min_values[0])\n",
    "                node_coordinates[:, 1] = (node_coordinates[:, 1] - min_values[1]) / (max_values[1] - min_values[1])\n",
    "            edges = new_hf['graph']['edges'][:]\n",
    "            edge_coordinates = node_coordinates[edges]\n",
    "            nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
    "            edges = [tuple(edge) for edge in edges]\n",
    "            for i in tqdm(new_hf['trajectories'].keys()):\n",
    "                path_group = new_hf['trajectories'][i]\n",
    "                path = {attr: torch.tensor(path_group[attr][()], device=device) for attr in path_group.keys() if attr in ['coordinates', 'edge_idxs', 'edge_orientations']}\n",
    "                paths.append(path)\n",
    "            if 'road_type' in edge_features:\n",
    "                onehot_encoded_road_type = new_hf['graph']['road_type'][:]\n",
    "                return paths, nodes, edges, edge_coordinates, onehot_encoded_road_type\n",
    "            else:\n",
    "                return paths, nodes, edges, edge_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3635535/1226096777.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = [(i, {'pos': torch.tensor(pos, device=device)}) for i, pos in enumerate(node_coordinates)]\n",
      "100%|██████████| 1472/1472 [00:01<00:00, 956.02it/s]\n"
     ]
    }
   ],
   "source": [
    "paths, nodes, edges, edge_coordinates = load_new_format('/ceph/hdd/students/schmitj/MA_Diffusion_based_trajectory_prediction/data/geolife_val.h5', [], 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munich Train\n",
      "24.094827586206897\n"
     ]
    }
   ],
   "source": [
    "print(\"Munich Train\")\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munich Val\n",
      "25.18881118881119\n"
     ]
    }
   ],
   "source": [
    "print(\"Munich Val\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Munich Test\n",
      "24.29268292682927\n"
     ]
    }
   ],
   "source": [
    "print(\"Munich Test\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tdrive train\n",
      "23.970171149144253\n"
     ]
    }
   ],
   "source": [
    "print(\"Tdrive train\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tdrive Val\n",
      "24.305555555555557\n"
     ]
    }
   ],
   "source": [
    "print(\"Tdrive Val\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tdrive Test\n",
      "24.199170124481327\n"
     ]
    }
   ],
   "source": [
    "print(\"Tdrive Test\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pneuma Train\n",
      "11.47207140296061\n"
     ]
    }
   ],
   "source": [
    "print(\"Pneuma Train\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pneuma Val\n",
      "11.061397779229262\n"
     ]
    }
   ],
   "source": [
    "print(\"Pneuma Val\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pneuma Test\n",
      "10.489171835890739\n"
     ]
    }
   ],
   "source": [
    "print(\"Pneuma Test\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geolife Train\n",
      "11.996086105675147\n"
     ]
    }
   ],
   "source": [
    "print(\"Geolife Train\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geolife Val\n",
      "14.472826086956522\n"
     ]
    }
   ],
   "source": [
    "print(\"Geolife Val\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geolife Test\n",
      "11.43569731930777\n"
     ]
    }
   ],
   "source": [
    "print(\"Geolife Test\")\n",
    "lens = [paths[i]['edge_idxs'].size()[0] for i in range(len(paths))]\n",
    "print(sum(lens)/len(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
